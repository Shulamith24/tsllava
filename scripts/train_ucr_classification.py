#!/usr/bin/env python3
# SPDX-FileCopyrightText: 2025 Stanford University, ETH Zurich, and the project authors (see CONTRIBUTORS.md)
# SPDX-FileCopyrightText: 2025 This source file is part of the OpenTSLM open-source project.
#
# SPDX-License-Identifier: MIT

"""
M1: UCRå•æ•°æ®é›†åˆ†ç±»è®­ç»ƒ

éªŒè¯OpenTSLMSPæ¶æ„åœ¨å•ä¸ªUCRæ•°æ®é›†ä¸Šçš„æœ‰ç›‘ç£åˆ†ç±»èƒ½åŠ›ã€‚
ä½¿ç”¨LLaVAèŒƒå¼ï¼ˆSoft Promptï¼‰è¿›è¡ŒæŒ‡ä»¤å¼åˆ†ç±»ã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/train_ucr_classification.py \
        --dataset ECG5000 \
        --encoder_type tslanet \
        --encoder_pretrained pretrained/tslanet_ucr98.pt \
        --epochs 30 \
        --batch_size 4 \
        --use_lora

è®­ç»ƒé…ç½®ï¼š
- LoRA: r=16, alpha=32 (å¯é€‰)
- Encoder LR: 2e-4
- Projector LR: 1e-4
- LoRA LR: 1e-4
"""

import os
import sys
import json
import argparse
import datetime
from pathlib import Path
from typing import List, Dict, Any

import torch
import torch.distributed as dist
from torch.optim import AdamW
from torch.nn.utils import clip_grad_norm_
from torch.utils.data import DataLoader
from tqdm.auto import tqdm
from transformers import get_linear_schedule_with_warmup

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from opentslm.model.llm.OpenTSLMSP import OpenTSLMSP
from opentslm.time_series_datasets.ucr.UCRClassificationDataset import UCRClassificationDataset
from opentslm.time_series_datasets.util import extend_time_series_to_match_patch_size_and_aggregate
from opentslm.model_config import PATCH_SIZE


def parse_args():
    parser = argparse.ArgumentParser(description="M1: UCRå•æ•°æ®é›†åˆ†ç±»è®­ç»ƒ")
    
    # æ•°æ®ç›¸å…³
    parser.add_argument(
        "--dataset",
        type=str,
        default="ECG5000",
        help="UCRæ•°æ®é›†åç§°",
    )
    parser.add_argument(
        "--data_path",
        type=str,
        default="./data",
        help="UCRæ•°æ®æ ¹ç›®å½•",
    )
    parser.add_argument(
        "--val_ratio",
        type=float,
        default=0.1,
        help="éªŒè¯é›†æ¯”ä¾‹ï¼ˆä»è®­ç»ƒé›†åˆ’åˆ†ï¼‰",
    )
    
    # æ¨¡å‹ç›¸å…³
    parser.add_argument(
        "--encoder_type",
        type=str,
        default="tslanet",
        choices=["transformer_cnn", "tslanet"],
        help="ç¼–ç å™¨ç±»å‹",
    )
    parser.add_argument(
        "--encoder_pretrained",
        type=str,
        default=None,
        help="TSLANeté¢„è®­ç»ƒæƒé‡è·¯å¾„",
    )
    parser.add_argument(
        "--llm_id",
        type=str,
        default="meta-llama/Llama-3.2-1B",
        help="LLMæ¨¡å‹ID",
    )
    
    # LoRAç›¸å…³
    parser.add_argument("--use_lora", action="store_true", help="æ˜¯å¦ä½¿ç”¨LoRA")
    parser.add_argument("--lora_r", type=int, default=16, help="LoRA rank")
    parser.add_argument("--lora_alpha", type=int, default=32, help="LoRA alpha")
    
    # è®­ç»ƒç›¸å…³
    parser.add_argument("--epochs", type=int, default=30, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=4, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--lr_encoder", type=float, default=2e-4, help="ç¼–ç å™¨å­¦ä¹ ç‡")
    parser.add_argument("--lr_projector", type=float, default=1e-4, help="æŠ•å½±å±‚å­¦ä¹ ç‡")
    parser.add_argument("--lr_lora", type=float, default=1e-4, help="LoRAå­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=1e-2, help="æƒé‡è¡°å‡")
    parser.add_argument("--grad_clip", type=float, default=1.0, help="æ¢¯åº¦è£å‰ª")
    parser.add_argument("--warmup_ratio", type=float, default=0.03, help="é¢„çƒ­æ¯”ä¾‹")
    
    # ä¿å­˜ç›¸å…³
    parser.add_argument(
        "--save_dir",
        type=str,
        default="results/m1_classification",
        help="ç»“æœä¿å­˜ç›®å½•",
    )
    
    # å…¶ä»–
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--device", type=str, default="cuda", help="è®¾å¤‡")
    parser.add_argument("--eval_every", type=int, default=5, help="æ¯Nè½®è¯„ä¼°ä¸€æ¬¡")
    parser.add_argument("--early_stop", type=int, default=10, help="æ—©åœè€å¿ƒå€¼")
    parser.add_argument("--max_new_tokens", type=int, default=10, help="ç”Ÿæˆæœ€å¤§tokenæ•°")
    
    return parser.parse_args()


def set_seed(seed: int):
    """è®¾ç½®éšæœºç§å­"""
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def calculate_accuracy(predictions: List[str], labels: List[str]) -> float:
    """
    è®¡ç®—åˆ†ç±»å‡†ç¡®ç‡
    
    å¯¹ç”Ÿæˆæ–‡æœ¬è¿›è¡Œåå¤„ç†ï¼Œæå–é¢„æµ‹æ ‡ç­¾å¹¶ä¸çœŸå®æ ‡ç­¾æ¯”è¾ƒ
    """
    correct = 0
    for pred, label in zip(predictions, labels):
        # æ¸…ç†é¢„æµ‹æ–‡æœ¬ï¼Œæå–æœ€åä¸€ä¸ªå­—æ¯
        pred_clean = pred.strip()
        
        # å°è¯•å¤šç§æ–¹å¼æå–æ ‡ç­¾
        pred_label = None
        
        # 1. å¦‚æœé¢„æµ‹å°±æ˜¯å•ä¸ªå­—æ¯
        if len(pred_clean) == 1 and pred_clean.isalpha():
            pred_label = pred_clean.upper()
        # 2. å–æœ€åä¸€ä¸ªå•è¯çš„ç¬¬ä¸€ä¸ªå­—æ¯
        elif pred_clean:
            words = pred_clean.split()
            if words:
                last_word = words[-1].strip(".,!?:;")
                if last_word and last_word[0].isalpha():
                    pred_label = last_word[0].upper()
        
        # æ¯”è¾ƒ
        label_clean = label.strip().upper()
        if pred_label == label_clean:
            correct += 1
    
    return correct / len(predictions) if predictions else 0.0


def create_data_loaders(args, eos_token: str):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = UCRClassificationDataset(
        split="train",
        EOS_TOKEN=eos_token,
        dataset_name=args.dataset,
        raw_data_path=args.data_path,
        val_ratio=args.val_ratio,
    )
    
    val_dataset = UCRClassificationDataset(
        split="validation",
        EOS_TOKEN=eos_token,
        dataset_name=args.dataset,
        raw_data_path=args.data_path,
        val_ratio=args.val_ratio,
    )
    
    test_dataset = UCRClassificationDataset(
        split="test",
        EOS_TOKEN=eos_token,
        dataset_name=args.dataset,
        raw_data_path=args.data_path,
        val_ratio=args.val_ratio,
    )
    
    # Collateå‡½æ•°
    def collate_fn(batch):
        return extend_time_series_to_match_patch_size_and_aggregate(
            batch, patch_size=PATCH_SIZE
        )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        collate_fn=collate_fn,
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=1,
        shuffle=False,
        collate_fn=collate_fn,
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=1,
        shuffle=False,
        collate_fn=collate_fn,
    )
    
    return train_loader, val_loader, test_loader


def train_one_epoch(
    model: OpenTSLMSP,
    train_loader: DataLoader,
    optimizer,
    scheduler,
    grad_clip: float,
    device: str,
    epoch: int,
    num_epochs: int,
) -> float:
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0.0
    num_batches = 0
    
    pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{num_epochs}")
    for batch in pbar:
        # è®¡ç®—æŸå¤±
        loss = model.compute_loss(batch)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        clip_grad_norm_(model.parameters(), max_norm=grad_clip)
        
        optimizer.step()
        scheduler.step()
        
        total_loss += loss.item()
        num_batches += 1
        
        pbar.set_postfix({
            "loss": f"{loss.item():.4f}",
            "lr": f"{scheduler.get_last_lr()[0]:.2e}"
        })
    
    return total_loss / max(num_batches, 1)


@torch.no_grad()
def evaluate(
    model: OpenTSLMSP,
    data_loader: DataLoader,
    max_new_tokens: int,
    desc: str = "Evaluating",
) -> Dict[str, Any]:
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    
    all_predictions = []
    all_labels = []
    total_loss = 0.0
    num_batches = 0
    
    for batch in tqdm(data_loader, desc=desc):
        # è®¡ç®—æŸå¤±
        loss = model.compute_loss(batch)
        total_loss += loss.item()
        num_batches += 1
        
        # ç”Ÿæˆé¢„æµ‹
        predictions = model.generate(batch, max_new_tokens=max_new_tokens)
        
        # æ”¶é›†ç»“æœ
        for sample, pred in zip(batch, predictions):
            all_predictions.append(pred)
            all_labels.append(sample["answer"].replace(model.get_eos_token(), "").strip())
    
    # è®¡ç®—æŒ‡æ ‡
    avg_loss = total_loss / max(num_batches, 1)
    accuracy = calculate_accuracy(all_predictions, all_labels)
    
    return {
        "loss": avg_loss,
        "accuracy": accuracy,
        "predictions": all_predictions,
        "labels": all_labels,
    }


def save_checkpoint(
    model: OpenTSLMSP,
    optimizer,
    scheduler,
    epoch: int,
    val_loss: float,
    val_acc: float,
    save_path: str,
    args,
):
    """ä¿å­˜checkpoint"""
    checkpoint = {
        "encoder_state": model.encoder.state_dict(),
        "projector_state": model.projector.state_dict(),
        "optimizer_state": optimizer.state_dict(),
        "scheduler_state": scheduler.state_dict(),
        "epoch": epoch,
        "val_loss": val_loss,
        "val_acc": val_acc,
        "args": vars(args),
    }
    
    # ä¿å­˜LoRAæƒé‡
    model.save_lora_state_to_checkpoint(checkpoint)
    
    torch.save(checkpoint, save_path)
    print(f"ğŸ’¾ Saved checkpoint to: {save_path}")


def main():
    args = parse_args()
    
    print("=" * 60)
    print("M1: UCRå•æ•°æ®é›†åˆ†ç±»è®­ç»ƒ")
    print("=" * 60)
    print(f"æ—¶é—´: {datetime.datetime.now()}")
    print(f"æ•°æ®é›†: {args.dataset}")
    print(f"ç¼–ç å™¨: {args.encoder_type}")
    print(f"LoRA: {args.use_lora}")
    print("=" * 60)
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    
    # è®¾ç½®è®¾å¤‡
    if args.device == "cuda" and not torch.cuda.is_available():
        print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        args.device = "cpu"
    device = args.device
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = os.path.join(args.save_dir, args.dataset)
    os.makedirs(save_dir, exist_ok=True)
    
    # ä¿å­˜é…ç½®
    with open(os.path.join(save_dir, "config.json"), "w") as f:
        json.dump(vars(args), f, indent=2)
    
    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ”§ åˆ›å»ºæ¨¡å‹...")
    tslanet_config = {
        "patch_size": 4,  # TSLANetä½¿ç”¨patch_size=8
    }
    
    model = OpenTSLMSP(
        llm_id=args.llm_id,
        device=device,
        encoder_type=args.encoder_type,
        encoder_pretrained_path=args.encoder_pretrained,
        tslanet_config=tslanet_config if args.encoder_type == "tslanet" else None,
    )
    
    # å¯ç”¨LoRA
    if args.use_lora:
        print("ğŸ“ å¯ç”¨LoRA...")
        model.enable_lora(lora_r=args.lora_r, lora_alpha=args.lora_alpha)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("\nğŸ“‚ åŠ è½½æ•°æ®...")
    eos_token = model.get_eos_token()
    train_loader, val_loader, test_loader = create_data_loaders(args, eos_token)
    
    print(f"   Train batches: {len(train_loader)}")
    print(f"   Val batches: {len(val_loader)}")
    print(f"   Test batches: {len(test_loader)}")
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    print("\nâš™ï¸ åˆ›å»ºä¼˜åŒ–å™¨...")
    param_groups = [
        {"params": model.encoder.parameters(), "lr": args.lr_encoder},
        {"params": model.projector.parameters(), "lr": args.lr_projector},
    ]
    
    if args.use_lora:
        lora_params = model.get_lora_parameters()
        if lora_params:
            param_groups.append({"params": lora_params, "lr": args.lr_lora})
    
    optimizer = AdamW(param_groups, weight_decay=args.weight_decay)
    
    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    total_steps = args.epochs * len(train_loader)
    warmup_steps = int(args.warmup_ratio * total_steps)
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_steps,
    )
    
    print(f"   Total steps: {total_steps}")
    print(f"   Warmup steps: {warmup_steps}")
    
    # è®­ç»ƒå¾ªç¯
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    best_val_acc = 0.0
    patience_counter = 0
    loss_history = []
    
    for epoch in range(1, args.epochs + 1):
        # è®­ç»ƒ
        train_loss = train_one_epoch(
            model, train_loader, optimizer, scheduler,
            args.grad_clip, device, epoch, args.epochs
        )
        
        # å®šæœŸè¯„ä¼°
        if epoch % args.eval_every == 0 or epoch == args.epochs:
            print(f"\nğŸ“Š Epoch {epoch} è¯„ä¼°...")
            
            # éªŒè¯é›†è¯„ä¼°
            val_results = evaluate(model, val_loader, args.max_new_tokens, "Validating")
            val_loss = val_results["loss"]
            val_acc = val_results["accuracy"]
            
            print(f"   Train Loss: {train_loss:.4f}")
            print(f"   Val Loss: {val_loss:.4f}")
            print(f"   Val Accuracy: {val_acc:.4f}")
            
            # æ˜¾ç¤ºä¸€äº›é¢„æµ‹æ ·æœ¬
            print("   Sample predictions:")
            for i in range(min(3, len(val_results["predictions"]))):
                pred = val_results["predictions"][i]
                label = val_results["labels"][i]
                # åªæ˜¾ç¤ºç”Ÿæˆçš„æœ€åéƒ¨åˆ†
                pred_short = pred[-50:] if len(pred) > 50 else pred
                print(f"     Pred: '{pred_short}' | Label: '{label}'")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                patience_counter = 0
                save_checkpoint(
                    model, optimizer, scheduler, epoch,
                    val_loss, val_acc,
                    os.path.join(save_dir, "best_model.pt"),
                    args
                )
            else:
                patience_counter += 1
                print(f"   (æ— æ”¹è¿›, patience: {patience_counter}/{args.early_stop})")
            
            # è®°å½•å†å²
            loss_history.append({
                "epoch": epoch,
                "train_loss": train_loss,
                "val_loss": val_loss,
                "val_acc": val_acc,
            })
            
            # ä¿å­˜å†å²
            with open(os.path.join(save_dir, "loss_history.json"), "w") as f:
                json.dump(loss_history, f, indent=2)
        else:
            print(f"Epoch {epoch}: Train Loss = {train_loss:.4f}")
        
        # æ—©åœ
        if patience_counter >= args.early_stop:
            print(f"\nâ¹ï¸ æ—©åœ! éªŒè¯å‡†ç¡®ç‡ {args.early_stop} è½®æœªæ”¹è¿›")
            break
    
    # æœ€ç»ˆæµ‹è¯•
    print("\n" + "=" * 60)
    print("ğŸ“‹ æœ€ç»ˆæµ‹è¯•è¯„ä¼°...")
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    best_ckpt = torch.load(os.path.join(save_dir, "best_model.pt"), map_location=device, weights_only=False)
    model.encoder.load_state_dict(best_ckpt["encoder_state"])
    model.projector.load_state_dict(best_ckpt["projector_state"])
    model.load_lora_state_from_checkpoint(best_ckpt, allow_missing=True)
    
    test_results = evaluate(model, test_loader, args.max_new_tokens, "Testing")
    
    print(f"\nâœ… æµ‹è¯•ç»“æœ:")
    print(f"   Test Loss: {test_results['loss']:.4f}")
    print(f"   Test Accuracy: {test_results['accuracy']:.4f}")
    
    # ä¿å­˜æµ‹è¯•ç»“æœ
    final_results = {
        "dataset": args.dataset,
        "best_val_acc": best_val_acc,
        "test_loss": test_results["loss"],
        "test_accuracy": test_results["accuracy"],
        "epochs_trained": epoch,
    }
    
    with open(os.path.join(save_dir, "final_results.json"), "w") as f:
        json.dump(final_results, f, indent=2)
    
    # ä¿å­˜æµ‹è¯•é¢„æµ‹
    with open(os.path.join(save_dir, "test_predictions.json"), "w") as f:
        json.dump({
            "predictions": test_results["predictions"],
            "labels": test_results["labels"],
        }, f, indent=2)
    
    print("=" * 60)
    print(f"ç»“æœä¿å­˜åˆ°: {save_dir}")
    print("=" * 60)


if __name__ == "__main__":
    main()
