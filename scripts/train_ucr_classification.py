#!/usr/bin/env python3
# SPDX-FileCopyrightText: 2025 Stanford University, ETH Zurich, and the project authors (see CONTRIBUTORS.md)
# SPDX-FileCopyrightText: 2025 This source file is part of the OpenTSLM open-source project.
#
# SPDX-License-Identifier: MIT

"""
M1: UCRå•æ•°æ®é›†åˆ†ç±»è®­ç»ƒ

éªŒè¯OpenTSLMSPæ¶æ„åœ¨å•ä¸ªUCRæ•°æ®é›†ä¸Šçš„æœ‰ç›‘ç£åˆ†ç±»èƒ½åŠ›ã€‚
ä½¿ç”¨LLaVAèŒƒå¼ï¼ˆSoft Promptï¼‰è¿›è¡ŒæŒ‡ä»¤å¼åˆ†ç±»ã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/train_ucr_classification.py \
        --dataset ECG5000 \
        --encoder_type tslanet \
        --encoder_pretrained pretrained/tslanet_ucr98.pt \
        --epochs 30 \
        --batch_size 4 \
        --use_lora

è®­ç»ƒé…ç½®ï¼š
- LoRA: r=16, alpha=32 (å¯é€‰)
- Encoder LR: 2e-4
- Projector LR: 1e-4
- LoRA LR: 1e-4
"""

import os
import sys
import json
import argparse
import datetime
from pathlib import Path
from typing import List, Dict, Any

import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.optim import AdamW
from torch.nn.utils import clip_grad_norm_
from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler
from tqdm.auto import tqdm
from transformers import get_linear_schedule_with_warmup

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from opentslm.model.llm.OpenTSLMSP import OpenTSLMSP
from opentslm.time_series_datasets.ucr.UCRClassificationDataset import UCRClassificationDataset
from opentslm.time_series_datasets.util import extend_time_series_to_match_patch_size_and_aggregate
from opentslm.model_config import PATCH_SIZE


def parse_args():
    parser = argparse.ArgumentParser(description="M1: UCRå•æ•°æ®é›†åˆ†ç±»è®­ç»ƒ")

    #å¿…é¡»æŒ‡å®š
    parser.add_argument("--use_lora", action="store_true", help="æ˜¯å¦ä½¿ç”¨LoRA")
    parser.add_argument("--gradient_checkpointing", action="store_true", help="å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
    parser.add_argument("--freeze_encoder", action="store_true", help="å†»ç»“ç¼–ç å™¨å‚æ•°")

    # æ•°æ®ç›¸å…³
    parser.add_argument("--dataset", type=str, default="CricketZ", help="UCRæ•°æ®é›†åç§°")
    parser.add_argument("--data_path", type=str, default="./data", help="UCRæ•°æ®æ ¹ç›®å½•")
    
    # æ¨¡å‹ç›¸å…³
    parser.add_argument("--encoder_type", type=str, default="tslanet", choices=["transformer_cnn", "tslanet"], help="ç¼–ç å™¨ç±»å‹")
    parser.add_argument("--encoder_pretrained", type=str, default=None, help="TSLANeté¢„è®­ç»ƒæƒé‡è·¯å¾„")
    parser.add_argument("--llm_id", type=str, default="meta-llama/Llama-3.2-1B", help="LLMæ¨¡å‹ID")
    
    # LoRAç›¸å…³
    # parser.add_argument("--use_lora", action="store_true", help="æ˜¯å¦ä½¿ç”¨LoRA")
    parser.add_argument("--lora_r", type=int, default=16, help="LoRA rank")
    parser.add_argument("--lora_alpha", type=int, default=32, help="LoRA alpha")
    
    # è®­ç»ƒç›¸å…³
    parser.add_argument("--epochs", type=int, default=30, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=32, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--lr_encoder", type=float, default=2e-4, help="ç¼–ç å™¨å­¦ä¹ ç‡")
    parser.add_argument("--lr_projector", type=float, default=1e-4, help="æŠ•å½±å±‚å­¦ä¹ ç‡")
    parser.add_argument("--lr_lora", type=float, default=1e-4, help="LoRAå­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=1e-2, help="æƒé‡è¡°å‡")
    parser.add_argument("--grad_clip", type=float, default=1.0, help="æ¢¯åº¦è£å‰ª")
    parser.add_argument("--warmup_ratio", type=float, default=0.03, help="é¢„çƒ­æ¯”ä¾‹")
    
    # ä¿å­˜ç›¸å…³
    parser.add_argument("--save_dir", type=str, default="results/m1_classification", help="ç»“æœä¿å­˜ç›®å½•")
    
    # DDPå’Œæ¢¯åº¦ç›¸å…³
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    # parser.add_argument("--gradient_checkpointing", action="store_true", help="å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
    # parser.add_argument("--freeze_encoder", action="store_true", help="å†»ç»“ç¼–ç å™¨å‚æ•°")
    
    # å…¶ä»–
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--device", type=str, default="cuda", help="è®¾å¤‡")
    parser.add_argument("--eval_every", type=int, default=5, help="æ¯Nè½®è¯„ä¼°ä¸€æ¬¡")
    parser.add_argument("--early_stop", type=int, default=10, help="æ—©åœè€å¿ƒå€¼")
    parser.add_argument("--max_new_tokens", type=int, default=10, help="ç”Ÿæˆæœ€å¤§tokenæ•°")
    parser.add_argument("--eval_batch_size", type=int, default=8, help="è¯„ä¼°æ‰¹æ¬¡å¤§å°")
    
    return parser.parse_args()


def setup_distributed():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒï¼ˆç”¨äºtorchrunï¼‰"""
    if "LOCAL_RANK" in os.environ:
        local_rank = int(os.environ["LOCAL_RANK"])
        world_size = int(os.environ.get("WORLD_SIZE", 1))
        rank = int(os.environ.get("RANK", 0))
        
        torch.cuda.set_device(local_rank)
        dist.init_process_group(backend="nccl", init_method="env://")
        
        return local_rank, world_size, rank
    return 0, 1, 0


def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ"""
    if dist.is_initialized():
        dist.destroy_process_group()


def get_model(model):
    """è·å–åº•å±‚æ¨¡å‹ï¼ˆå…¼å®¹DDPåŒ…è£…ï¼‰"""
    return model.module if hasattr(model, "module") else model


def set_seed(seed: int):
    """è®¾ç½®éšæœºç§å­"""
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def calculate_accuracy(predictions: List[str], labels: List[str]) -> float:
    """
    è®¡ç®—åˆ†ç±»å‡†ç¡®ç‡
    
    å¯¹ç”Ÿæˆæ–‡æœ¬è¿›è¡Œåå¤„ç†ï¼Œæå–é¢„æµ‹æ ‡ç­¾å¹¶ä¸çœŸå®æ ‡ç­¾æ¯”è¾ƒ
    """
    correct = 0
    for pred, label in zip(predictions, labels):
        # æ¸…ç†é¢„æµ‹æ–‡æœ¬ï¼Œæå–æœ€åä¸€ä¸ªå­—æ¯
        pred_clean = pred.strip()
        
        # å°è¯•å¤šç§æ–¹å¼æå–æ ‡ç­¾
        pred_label = None
        
        # 1. å¦‚æœé¢„æµ‹å°±æ˜¯å•ä¸ªå­—æ¯
        if len(pred_clean) == 1 and pred_clean.isalpha():
            pred_label = pred_clean.upper()
        # 2. å–æœ€åä¸€ä¸ªå•è¯çš„ç¬¬ä¸€ä¸ªå­—æ¯
        elif pred_clean:
            words = pred_clean.split()
            if words:
                last_word = words[-1].strip(".,!?:;")
                if last_word and last_word[0].isalpha():
                    pred_label = last_word[0].upper()
        
        # æ¯”è¾ƒ
        label_clean = label.strip().upper()
        if pred_label == label_clean:
            correct += 1
    
    return correct / len(predictions) if predictions else 0.0


def create_data_loaders(args, eos_token: str, world_size: int = 1, rank: int = 0):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = UCRClassificationDataset(
        split="train",
        EOS_TOKEN=eos_token,
        dataset_name=args.dataset,
        raw_data_path=args.data_path,
    )
    
    val_dataset = UCRClassificationDataset(
        split="validation",
        EOS_TOKEN=eos_token,
        dataset_name=args.dataset,
        raw_data_path=args.data_path,
    )
    
    test_dataset = UCRClassificationDataset(
        split="test",
        EOS_TOKEN=eos_token,
        dataset_name=args.dataset,
        raw_data_path=args.data_path,
    )
    
    # Collateå‡½æ•°
    def collate_fn(batch):
        return extend_time_series_to_match_patch_size_and_aggregate(
            batch, patch_size=PATCH_SIZE
        )
    
    # åˆ†å¸ƒå¼é‡‡æ ·å™¨ï¼ˆä»…è®­ç»ƒé›†ï¼‰
    train_sampler = None
    if world_size > 1:
        train_sampler = DistributedSampler(
            train_dataset, num_replicas=world_size, rank=rank, shuffle=True
        )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=(train_sampler is None),
        sampler=train_sampler,
        collate_fn=collate_fn,
    )
    
    # è¯„ä¼°ç”¨DataLoaderï¼ˆæ”¯æŒæ‰¹é‡è¯„ä¼°ï¼‰
    eval_batch_size = getattr(args, 'eval_batch_size', 8)
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=eval_batch_size,
        shuffle=False,
        collate_fn=collate_fn,
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=eval_batch_size,
        shuffle=False,
        collate_fn=collate_fn,
    )
    
    return train_loader, val_loader, test_loader, train_sampler


def train_one_epoch(
    model,
    train_loader: DataLoader,
    optimizer,
    scheduler,
    grad_clip: float,
    epoch: int,
    num_epochs: int,
    gradient_accumulation_steps: int = 1,
    rank: int = 0,
) -> float:
    """è®­ç»ƒä¸€ä¸ªepochï¼ˆæ”¯æŒæ¢¯åº¦ç´¯ç§¯å’ŒDDPï¼‰"""
    model.train()
    total_loss = 0.0
    num_batches = 0
    optimizer.zero_grad()
    
    pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{num_epochs}", disable=(rank != 0))
    for step, batch in enumerate(pbar):
        # è®¡ç®—æŸå¤±ï¼ˆç¼©æ”¾ç”¨äºæ¢¯åº¦ç´¯ç§¯ï¼‰
        # ä½¿ç”¨model(batch)è°ƒç”¨forwardæ–¹æ³•ï¼ŒDDPæ¢¯åº¦åŒæ­¥åœ¨backward()æ—¶è‡ªåŠ¨è¿›è¡Œ
        loss = model(batch)
        loss = loss / gradient_accumulation_steps
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        # æ¢¯åº¦ç´¯ç§¯å®Œæˆåæ›´æ–°
        if (step + 1) % gradient_accumulation_steps == 0:
            clip_grad_norm_(model.parameters(), max_norm=grad_clip)
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()
        
        total_loss += loss.item() * gradient_accumulation_steps
        num_batches += 1
        
        if rank == 0:
            pbar.set_postfix({
                "loss": f"{loss.item() * gradient_accumulation_steps:.4f}",
                "lr": f"{scheduler.get_last_lr()[0]:.2e}"
            })
    
    # å¤„ç†æœ€åä¸è¶³accumulation_stepsçš„batch
    if num_batches % gradient_accumulation_steps != 0:
        clip_grad_norm_(model.parameters(), max_norm=grad_clip)
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()
    
    return total_loss / max(num_batches, 1)


@torch.no_grad()
def evaluate(
    model,
    data_loader: DataLoader,
    max_new_tokens: int,
    desc: str = "Evaluating",
    rank: int = 0,
) -> Dict[str, Any]:
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    underlying_model = get_model(model)
    
    all_predictions = []
    all_labels = []
    total_loss = 0.0
    num_batches = 0
    
    for batch in tqdm(data_loader, desc=desc, disable=(rank != 0)):
        # è¯„ä¼°æ—¶ä¸éœ€è¦æ¢¯åº¦åŒæ­¥ï¼Œæ‰€ä»¥å¯ä»¥ç›´æ¥è°ƒç”¨åº•å±‚æ¨¡å‹
        loss = underlying_model.compute_loss(batch)
        total_loss += loss.item()
        num_batches += 1
        
        # ç”Ÿæˆé¢„æµ‹
        predictions = underlying_model.generate(batch, max_new_tokens=max_new_tokens)
        
        # æ”¶é›†ç»“æœ
        for sample, pred in zip(batch, predictions):
            all_predictions.append(pred)
            all_labels.append(sample["answer"].replace(underlying_model.get_eos_token(), "").strip())
    
    # è®¡ç®—æŒ‡æ ‡
    avg_loss = total_loss / max(num_batches, 1)
    accuracy = calculate_accuracy(all_predictions, all_labels)
    
    return {
        "loss": avg_loss,
        "accuracy": accuracy,
        "predictions": all_predictions,
        "labels": all_labels,
    }


def save_checkpoint(
    model,
    optimizer,
    scheduler,
    epoch: int,
    val_loss: float,
    val_acc: float,
    save_path: str,
    args,
    rank: int = 0,
):
    """ä¿å­˜checkpointï¼ˆä»…rank=0æ‰§è¡Œï¼‰"""
    if rank != 0:
        return
    
    underlying_model = get_model(model)
    checkpoint = {
        "encoder_state": underlying_model.encoder.state_dict(),
        "projector_state": underlying_model.projector.state_dict(),
        "optimizer_state": optimizer.state_dict(),
        "scheduler_state": scheduler.state_dict(),
        "epoch": epoch,
        "val_loss": val_loss,
        "val_acc": val_acc,
        "args": vars(args),
    }
    
    # ä¿å­˜LoRAæƒé‡
    underlying_model.save_lora_state_to_checkpoint(checkpoint)
    
    torch.save(checkpoint, save_path)
    print(f"ğŸ’¾ Saved checkpoint to: {save_path}")


def main():
    args = parse_args()
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
    local_rank, world_size, rank = setup_distributed()
    
    # ä»…rank=0æ‰“å°ä¿¡æ¯
    if rank == 0:
        print("=" * 60)
        print("M1: UCRå•æ•°æ®é›†åˆ†ç±»è®­ç»ƒ")
        print("=" * 60)
        print(f"æ—¶é—´: {datetime.datetime.now()}")
        print(f"æ•°æ®é›†: {args.dataset}")
        print(f"ç¼–ç å™¨: {args.encoder_type}")
        print(f"LoRA: {args.use_lora}")
        print(f"DDP: world_size={world_size}")
        print(f"æ¢¯åº¦ç´¯ç§¯: {args.gradient_accumulation_steps}")
        print(f"æ¢¯åº¦æ£€æŸ¥ç‚¹: {args.gradient_checkpointing}")
        print("=" * 60)
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed + rank)  # æ¯ä¸ªrankä½¿ç”¨ä¸åŒçš„éšæœºç§å­
    
    # è®¾ç½®è®¾å¤‡
    if world_size > 1:
        device = f"cuda:{local_rank}"
    elif args.device == "cuda" and torch.cuda.is_available():
        device = "cuda"
    else:
        if rank == 0:
            print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        device = "cpu"
    
    # åˆ›å»ºä¿å­˜ç›®å½•ï¼ˆä»…rank=0ï¼‰
    save_dir = os.path.join(args.save_dir, args.dataset)
    if rank == 0:
        os.makedirs(save_dir, exist_ok=True)
        # ä¿å­˜é…ç½®
        with open(os.path.join(save_dir, "config.json"), "w") as f:
            json.dump(vars(args), f, indent=2)
    
    # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
    if world_size > 1:
        dist.barrier()
    
    # åˆ›å»ºæ¨¡å‹
    if rank == 0:
        print("\nğŸ”§ åˆ›å»ºæ¨¡å‹...")
    tslanet_config = {
        "patch_size": 4,
    }
    
    model = OpenTSLMSP(
        llm_id=args.llm_id,
        device=device,
        encoder_type=args.encoder_type,
        encoder_pretrained_path=args.encoder_pretrained,
        tslanet_config=tslanet_config if args.encoder_type == "tslanet" else None,
    )
    
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
    if args.gradient_checkpointing:
        model.enable_gradient_checkpointing()
    
    # å†»ç»“ç¼–ç å™¨
    if args.freeze_encoder:
        for param in model.encoder.parameters():
            param.requires_grad = False
        if rank == 0:
            print("ğŸ§Š ç¼–ç å™¨å‚æ•°å·²å†»ç»“")
    
    # å¯ç”¨LoRA
    if args.use_lora:
        if rank == 0:
            print("ğŸ“ å¯ç”¨LoRA...")
        model.enable_lora(lora_r=args.lora_r, lora_alpha=args.lora_alpha)
    
    # DDPåŒ…è£…
    if world_size > 1:
        model = DDP(model, device_ids=[local_rank])
        if rank == 0:
            print(f"âœ… æ¨¡å‹å·²ç”¨DDPåŒ…è£… (world_size={world_size})")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    if rank == 0:
        print("\nğŸ“‚ åŠ è½½æ•°æ®...")
    eos_token = get_model(model).get_eos_token()
    train_loader, val_loader, test_loader, train_sampler = create_data_loaders(
        args, eos_token, world_size, rank
    )
    
    if rank == 0:
        print(f"   Train batches: {len(train_loader)}")
        print(f"   Val batches: {len(val_loader)}")
        print(f"   Test batches: {len(test_loader)}")
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    if rank == 0:
        print("\nâš™ï¸ åˆ›å»ºä¼˜åŒ–å™¨...")
    underlying_model = get_model(model)
    
    # æ ¹æ®æ˜¯å¦å†»ç»“ç¼–ç å™¨å†³å®šå‚æ•°ç»„
    param_groups = []
    if not args.freeze_encoder:
        param_groups.append({"params": underlying_model.encoder.parameters(), "lr": args.lr_encoder})
    param_groups.append({"params": underlying_model.projector.parameters(), "lr": args.lr_projector})
    
    if args.use_lora:
        lora_params = underlying_model.get_lora_parameters()
        if lora_params:
            param_groups.append({"params": lora_params, "lr": args.lr_lora})
    
    optimizer = AdamW(param_groups, weight_decay=args.weight_decay)
    
    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆè€ƒè™‘æ¢¯åº¦ç´¯ç§¯ï¼‰
    effective_batch_size = args.batch_size * args.gradient_accumulation_steps * world_size
    steps_per_epoch = len(train_loader) // args.gradient_accumulation_steps
    total_steps = args.epochs * steps_per_epoch
    warmup_steps = int(args.warmup_ratio * total_steps)
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_steps,
    )
    
    if rank == 0:
        print(f"   Effective batch size: {effective_batch_size}")
        print(f"   Total steps: {total_steps}")
        print(f"   Warmup steps: {warmup_steps}")
    
    # è®­ç»ƒå¾ªç¯
    if rank == 0:
        print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    best_val_acc = 0.0
    patience_counter = 0
    loss_history = []
    epoch = 0  # åˆå§‹åŒ–é˜²æ­¢unbound
    
    try:
        for epoch in range(1, args.epochs + 1):
            # è®¾ç½®samplerçš„epochï¼ˆDDPå¿…éœ€ï¼‰
            if train_sampler is not None:
                train_sampler.set_epoch(epoch)
            
            # è®­ç»ƒ
            train_loss = train_one_epoch(
                model, train_loader, optimizer, scheduler,
                args.grad_clip, epoch, args.epochs,
                args.gradient_accumulation_steps, rank
            )
            
            # å®šæœŸè¯„ä¼°
            if epoch % args.eval_every == 0 or epoch == args.epochs:
                if rank == 0:
                    print(f"\nğŸ“Š Epoch {epoch} è¯„ä¼°...")
                
                # éªŒè¯é›†è¯„ä¼°
                val_results = evaluate(model, val_loader, args.max_new_tokens, "Validating", rank)
                val_loss = val_results["loss"]
                val_acc = val_results["accuracy"]
                
                if rank == 0:
                    print(f"   Train Loss: {train_loss:.4f}")
                    print(f"   Val Loss: {val_loss:.4f}")
                    print(f"   Val Accuracy: {val_acc:.4f}")
                    
                    # æ˜¾ç¤ºä¸€äº›é¢„æµ‹æ ·æœ¬
                    print("   Sample predictions:")
                    for i in range(min(3, len(val_results["predictions"]))):
                        pred = val_results["predictions"][i]
                        label = val_results["labels"][i]
                        pred_short = pred[-50:] if len(pred) > 50 else pred
                        print(f"     Pred: '{pred_short}' | Label: '{label}'")
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_acc > best_val_acc:
                    best_val_acc = val_acc
                    patience_counter = 0
                    save_checkpoint(
                        model, optimizer, scheduler, epoch,
                        val_loss, val_acc,
                        os.path.join(save_dir, "best_model.pt"),
                        args, rank
                    )
                else:
                    patience_counter += 1
                    if rank == 0:
                        print(f"   (æ— æ”¹è¿›, patience: {patience_counter}/{args.early_stop})")
                
                # è®°å½•å†å²ï¼ˆä»…rank=0ï¼‰
                if rank == 0:
                    loss_history.append({
                        "epoch": epoch,
                        "train_loss": train_loss,
                        "val_loss": val_loss,
                        "val_acc": val_acc,
                    })
                    with open(os.path.join(save_dir, "loss_history.json"), "w") as f:
                        json.dump(loss_history, f, indent=2)
            else:
                if rank == 0:
                    print(f"Epoch {epoch}: Train Loss = {train_loss:.4f}")
            
            # æ—©åœ
            if patience_counter >= args.early_stop:
                if rank == 0:
                    print(f"\nâ¹ï¸ æ—©åœ! éªŒè¯å‡†ç¡®ç‡ {args.early_stop} è½®æœªæ”¹è¿›")
                break
        
        # æœ€ç»ˆæµ‹è¯•ï¼ˆä»…rank=0ï¼‰
        if rank == 0:
            print("\n" + "=" * 60)
            print("ğŸ“‹ æœ€ç»ˆæµ‹è¯•è¯„ä¼°...")
            
            # åŠ è½½æœ€ä½³æ¨¡å‹
            best_ckpt = torch.load(os.path.join(save_dir, "best_model.pt"), map_location=device, weights_only=False)
            underlying_model.encoder.load_state_dict(best_ckpt["encoder_state"])
            underlying_model.projector.load_state_dict(best_ckpt["projector_state"])
            underlying_model.load_lora_state_from_checkpoint(best_ckpt, allow_missing=True)
            
            test_results = evaluate(model, test_loader, args.max_new_tokens, "Testing", rank)
            
            print(f"\nâœ… æµ‹è¯•ç»“æœ:")
            print(f"   Test Loss: {test_results['loss']:.4f}")
            print(f"   Test Accuracy: {test_results['accuracy']:.4f}")
            
            # ä¿å­˜æµ‹è¯•ç»“æœ
            final_results = {
                "dataset": args.dataset,
                "best_val_acc": best_val_acc,
                "test_loss": test_results["loss"],
                "test_accuracy": test_results["accuracy"],
                "epochs_trained": epoch,
            }
            
            with open(os.path.join(save_dir, "final_results.json"), "w") as f:
                json.dump(final_results, f, indent=2)
            
            # ä¿å­˜æµ‹è¯•é¢„æµ‹
            with open(os.path.join(save_dir, "test_predictions.json"), "w") as f:
                json.dump({
                    "predictions": test_results["predictions"],
                    "labels": test_results["labels"],
                }, f, indent=2)
            
            print("=" * 60)
            print(f"ç»“æœä¿å­˜åˆ°: {save_dir}")
            print("=" * 60)
    
    finally:
        # æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
        cleanup_distributed()


if __name__ == "__main__":
    main()

