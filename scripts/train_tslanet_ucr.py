#!/usr/bin/env python3
# SPDX-FileCopyrightText: 2025 Stanford University, ETH Zurich, and the project authors (see CONTRIBUTORS.md)
# SPDX-FileCopyrightText: 2025 This source file is part of the OpenTSLM open-source project.
#
# SPDX-License-Identifier: MIT

"""
TSLANetå•æ•°æ®é›†åˆ†ç±»è®­ç»ƒè„šæœ¬

ç”¨äºåœ¨UCRæ•°æ®é›†ä¸Šè®­ç»ƒTSLANetåˆ†ç±»å™¨ï¼Œè®­ç»ƒå¥½çš„encoderç”¨äºICLåˆ†ç±»çš„ç›¸ä¼¼æ ·æœ¬æ£€ç´¢ã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/train_tslanet_ucr.py \\
        --dataset ECG5000 \\
        --epochs 100 \\
        --batch_size 16 \\
        --lr 1e-3

è®­ç»ƒæµç¨‹ï¼š
1. åŠ è½½UCRæ•°æ®é›†
2. ä½¿ç”¨TSLANetEncoder + åˆ†ç±»å¤´è¿›è¡Œè®­ç»ƒ
3. ä¿å­˜encoder checkpointç”¨äºåç»­æ£€ç´¢
"""

import os
import sys
import json
import argparse
import datetime
from pathlib import Path
from typing import List, Dict, Any

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import AdamW
from torch.utils.data import DataLoader, Dataset
from tqdm.auto import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from opentslm.model.encoder.TSLANetEncoder import TSLANetEncoder
from opentslm.time_series_datasets.ucr.ucr_loader import (
    load_ucr_dataset, 
    ensure_ucr_data,
    UCRDataset,
    collate_fn
)


def parse_args():
    parser = argparse.ArgumentParser(description="TSLANetå•æ•°æ®é›†åˆ†ç±»è®­ç»ƒ")

    # æ•°æ®ç›¸å…³
    parser.add_argument("--dataset", type=str, default="ECG5000", help="UCRæ•°æ®é›†åç§°")
    parser.add_argument("--data_path", type=str, default="./data", help="UCRæ•°æ®æ ¹ç›®å½•")
    
    # æ¨¡å‹ç›¸å…³
    parser.add_argument("--emb_dim", type=int, default=128, help="åµŒå…¥ç»´åº¦")
    parser.add_argument("--depth", type=int, default=2, help="TSLANetå±‚æ•°")
    parser.add_argument("--patch_size", type=int, default=8, help="Patchå¤§å°")
    parser.add_argument("--dropout", type=float, default=0.15, help="Dropoutæ¯”ä¾‹")
    
    # è®­ç»ƒç›¸å…³
    parser.add_argument("--epochs", type=int, default=100, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=16, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--lr", type=float, default=1e-3, help="å­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=1e-4, help="æƒé‡è¡°å‡")
    parser.add_argument("--label_smoothing", type=float, default=0.1, help="æ ‡ç­¾å¹³æ»‘")
    
    # é¢„è®­ç»ƒé˜¶æ®µï¼ˆå¯é€‰ï¼‰
    parser.add_argument("--pretrain", action="store_true", help="æ˜¯å¦è¿›è¡Œæ©ç é¢„è®­ç»ƒ")
    parser.add_argument("--pretrain_epochs", type=int, default=50, help="é¢„è®­ç»ƒè½®æ•°")
    parser.add_argument("--mask_ratio", type=float, default=0.4, help="æ©ç æ¯”ä¾‹")
    
    # ä¿å­˜ç›¸å…³
    parser.add_argument("--save_dir", type=str, default="results/tslanet_ucr", help="ç»“æœä¿å­˜ç›®å½•")
    
    # å…¶ä»–
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--device", type=str, default="cuda", help="è®¾å¤‡")
    parser.add_argument("--val_ratio", type=float, default=0.1, help="éªŒè¯é›†æ¯”ä¾‹(ä»è®­ç»ƒé›†åˆ’åˆ†)")
    
    return parser.parse_args()


def set_seed(seed: int):
    """è®¾ç½®éšæœºç§å­"""
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


class TSLANetClassifier(nn.Module):
    """TSLANetåˆ†ç±»å™¨ = TSLANetEncoder + åˆ†ç±»å¤´"""
    
    def __init__(
        self,
        encoder: TSLANetEncoder,
        num_classes: int,
        dropout: float = 0.1
    ):
        super().__init__()
        self.encoder = encoder
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(encoder.emb_dim, num_classes)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [B, L] æ—¶é—´åºåˆ—
        Returns:
            [B, num_classes] logits
        """
        # ç¼–ç 
        features = self.encoder(x)  # [B, N, emb_dim]
        # å…¨å±€å¹³å‡æ± åŒ–
        pooled = features.mean(dim=1)  # [B, emb_dim]
        # åˆ†ç±»
        pooled = self.dropout(pooled)
        logits = self.classifier(pooled)  # [B, num_classes]
        return logits
    
    def get_embedding(self, x: torch.Tensor) -> torch.Tensor:
        """è·å–å…¨å±€embeddingç”¨äºæ£€ç´¢"""
        return self.encoder.get_embedding(x)


def create_data_loaders(args):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    ensure_ucr_data()
    
    # åŠ è½½æ•°æ®
    train_df, test_df = load_ucr_dataset(args.dataset, raw_data_path=args.data_path)
    
    # è·å–æ•°æ®é›†ä¿¡æ¯
    all_labels = sorted(train_df["label"].unique().tolist())
    num_classes = len(all_labels)
    seq_len = train_df.shape[1] - 1  # å‡å»labelåˆ—
    
    # æ ‡ç­¾é‡æ˜ å°„åˆ°0-indexed
    label_to_idx = {label: idx for idx, label in enumerate(all_labels)}
    train_df["label"] = train_df["label"].map(label_to_idx)
    test_df["label"] = test_df["label"].map(label_to_idx)
    
    # ä»è®­ç»ƒé›†åˆ’åˆ†éªŒè¯é›†
    train_df = train_df.sample(frac=1, random_state=args.seed).reset_index(drop=True)
    val_size = int(len(train_df) * args.val_ratio)
    
    if val_size > 0:
        val_df = train_df.iloc[:val_size]
        train_df = train_df.iloc[val_size:]
    else:
        val_df = test_df.copy()  # å¦‚æœè®­ç»ƒé›†å¤ªå°ï¼Œç”¨æµ‹è¯•é›†ä½œä¸ºéªŒè¯é›†
    
    print(f"ğŸ“Š Dataset: {args.dataset}")
    print(f"   Classes: {num_classes}")
    print(f"   Sequence length: {seq_len}")
    print(f"   Train samples: {len(train_df)}")
    print(f"   Val samples: {len(val_df)}")
    print(f"   Test samples: {len(test_df)}")
    
    # åˆ›å»ºDataset
    train_dataset = UCRDataset(train_df)
    val_dataset = UCRDataset(val_df)
    test_dataset = UCRDataset(test_df)
    
    # åˆ›å»ºDataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=min(args.batch_size, len(train_dataset)),
        shuffle=True,
        collate_fn=collate_fn,
        drop_last=len(train_dataset) > args.batch_size
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        collate_fn=collate_fn
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        collate_fn=collate_fn
    )
    
    return train_loader, val_loader, test_loader, num_classes, seq_len, label_to_idx


def pretrain_one_epoch(
    model: TSLANetClassifier,
    train_loader: DataLoader,
    optimizer,
    mask_ratio: float,
    epoch: int,
    num_epochs: int,
    device: str
) -> float:
    """é¢„è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0.0
    num_batches = 0
    
    pbar = tqdm(train_loader, desc=f"Pretrain Epoch {epoch}/{num_epochs}")
    for batch in pbar:
        features, _ = batch
        features = features.to(device)  # [B, L]
        
        # æ©ç é¢„è®­ç»ƒ
        preds, target, mask = model.encoder.pretrain_forward(features, mask_ratio=mask_ratio)
        
        # è®¡ç®—æ©ç ä½ç½®çš„MSEæŸå¤±
        loss = (preds - target) ** 2
        loss = loss.mean(dim=-1)  # [B, N]
        loss = (loss * mask.float()).sum() / mask.float().sum()
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        num_batches += 1
        pbar.set_postfix({"loss": f"{loss.item():.4f}"})
    
    return total_loss / max(num_batches, 1)


def train_one_epoch(
    model: TSLANetClassifier,
    train_loader: DataLoader,
    optimizer,
    criterion,
    epoch: int,
    num_epochs: int,
    device: str
) -> float:
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0.0
    num_batches = 0
    
    pbar = tqdm(train_loader, desc=f"Train Epoch {epoch}/{num_epochs}")
    for batch in pbar:
        features, labels = batch
        features = features.to(device)  # [B, L]
        labels = labels.to(device)  # [B]
        
        # å‰å‘ä¼ æ’­
        logits = model(features)  # [B, num_classes]
        loss = criterion(logits, labels)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        num_batches += 1
        
        # è®¡ç®—å‡†ç¡®ç‡
        preds = logits.argmax(dim=-1)
        acc = (preds == labels).float().mean().item()
        pbar.set_postfix({"loss": f"{loss.item():.4f}", "acc": f"{acc:.4f}"})
    
    return total_loss / max(num_batches, 1)


@torch.no_grad()
def evaluate(
    model: TSLANetClassifier,
    data_loader: DataLoader,
    criterion,
    device: str,
    desc: str = "Evaluating"
) -> Dict[str, float]:
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    total_loss = 0.0
    num_batches = 0
    all_preds = []
    all_labels = []
    
    for batch in tqdm(data_loader, desc=desc):
        features, labels = batch
        features = features.to(device)
        labels = labels.to(device)
        
        logits = model(features)
        loss = criterion(logits, labels)
        
        total_loss += loss.item()
        num_batches += 1
        
        preds = logits.argmax(dim=-1)
        all_preds.extend(preds.cpu().tolist())
        all_labels.extend(labels.cpu().tolist())
    
    # è®¡ç®—æŒ‡æ ‡
    all_preds = np.array(all_preds)
    all_labels = np.array(all_labels)
    accuracy = (all_preds == all_labels).mean()
    
    return {
        "loss": total_loss / max(num_batches, 1),
        "accuracy": accuracy
    }


def main():
    args = parse_args()
    
    print("=" * 60)
    print("TSLANetå•æ•°æ®é›†åˆ†ç±»è®­ç»ƒ")
    print("=" * 60)
    print(f"æ—¶é—´: {datetime.datetime.now()}")
    print(f"æ•°æ®é›†: {args.dataset}")
    print(f"é¢„è®­ç»ƒ: {args.pretrain}")
    print("=" * 60)
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    
    # è®¾ç½®è®¾å¤‡
    if args.device == "cuda" and torch.cuda.is_available():
        device = "cuda"
    else:
        print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        device = "cpu"
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = os.path.join(args.save_dir, args.dataset)
    os.makedirs(save_dir, exist_ok=True)
    
    # ä¿å­˜é…ç½®
    with open(os.path.join(save_dir, "config.json"), "w") as f:
        json.dump(vars(args), f, indent=2)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("\nğŸ“‚ åŠ è½½æ•°æ®...")
    train_loader, val_loader, test_loader, num_classes, seq_len, label_to_idx = create_data_loaders(args)
    
    # è®¡ç®—éœ€è¦çš„patchæ•°é‡å’Œåºåˆ—é•¿åº¦
    padded_seq_len = seq_len
    if seq_len % args.patch_size != 0:
        padded_seq_len = seq_len + (args.patch_size - seq_len % args.patch_size)
    
    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ”§ åˆ›å»ºæ¨¡å‹...")
    encoder = TSLANetEncoder(
        output_dim=args.emb_dim,
        dropout=args.dropout,
        patch_size=args.patch_size,
        emb_dim=args.emb_dim,
        depth=args.depth,
        max_seq_len=max(padded_seq_len, 512)  # ç¡®ä¿è¶³å¤Ÿé•¿
    )
    
    model = TSLANetClassifier(
        encoder=encoder,
        num_classes=num_classes,
        dropout=args.dropout
    ).to(device)
    
    print(f"   Encoder params: {sum(p.numel() for p in encoder.parameters()):,}")
    print(f"   Total params: {sum(p.numel() for p in model.parameters()):,}")
    
    # åˆ›å»ºä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    criterion = nn.CrossEntropyLoss(label_smoothing=args.label_smoothing)
    
    # é¢„è®­ç»ƒé˜¶æ®µï¼ˆå¯é€‰ï¼‰
    if args.pretrain:
        print("\nğŸ”„ å¼€å§‹é¢„è®­ç»ƒé˜¶æ®µ...")
        pretrain_optimizer = AdamW(model.encoder.parameters(), lr=args.lr, weight_decay=args.weight_decay)
        
        for epoch in range(1, args.pretrain_epochs + 1):
            pretrain_loss = pretrain_one_epoch(
                model, train_loader, pretrain_optimizer,
                args.mask_ratio, epoch, args.pretrain_epochs, device
            )
            print(f"Pretrain Epoch {epoch}: Loss = {pretrain_loss:.4f}")
        
        print("âœ… é¢„è®­ç»ƒå®Œæˆ")
    
    # è®­ç»ƒé˜¶æ®µ
    print("\nğŸš€ å¼€å§‹åˆ†ç±»è®­ç»ƒ...")
    best_val_acc = 0.0
    patience = 20
    patience_counter = 0
    loss_history = []
    
    for epoch in range(1, args.epochs + 1):
        # è®­ç»ƒ
        train_loss = train_one_epoch(
            model, train_loader, optimizer, criterion,
            epoch, args.epochs, device
        )
        
        # éªŒè¯
        val_results = evaluate(model, val_loader, criterion, device, "Validating")
        
        print(f"Epoch {epoch}: Train Loss = {train_loss:.4f}, "
              f"Val Loss = {val_results['loss']:.4f}, Val Acc = {val_results['accuracy']:.4f}")
        
        # è®°å½•å†å²
        loss_history.append({
            "epoch": epoch,
            "train_loss": train_loss,
            "val_loss": val_results["loss"],
            "val_acc": val_results["accuracy"]
        })
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_results["accuracy"] > best_val_acc:
            best_val_acc = val_results["accuracy"]
            patience_counter = 0
            
            # ä¿å­˜checkpoint
            checkpoint = {
                "encoder_state": model.encoder.state_dict(),
                "classifier_state": model.classifier.state_dict(),
                "epoch": epoch,
                "val_acc": best_val_acc,
                "num_classes": num_classes,
                "seq_len": seq_len,
                "label_to_idx": label_to_idx,
                "config": vars(args)
            }
            torch.save(checkpoint, os.path.join(save_dir, "best_model.pt"))
            print(f"ğŸ’¾ Saved best model (val_acc={best_val_acc:.4f})")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"\nâ¹ï¸ æ—©åœ! éªŒè¯å‡†ç¡®ç‡ {patience} è½®æœªæ”¹è¿›")
                break
    
    # ä¿å­˜è®­ç»ƒå†å²
    with open(os.path.join(save_dir, "loss_history.json"), "w") as f:
        json.dump(loss_history, f, indent=2)
    
    # æœ€ç»ˆæµ‹è¯•
    print("\n" + "=" * 60)
    print("ğŸ“‹ æœ€ç»ˆæµ‹è¯•è¯„ä¼°...")
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    best_ckpt = torch.load(os.path.join(save_dir, "best_model.pt"), map_location=device, weights_only=False)
    model.encoder.load_state_dict(best_ckpt["encoder_state"])
    model.classifier.load_state_dict(best_ckpt["classifier_state"])
    
    test_results = evaluate(model, test_loader, criterion, device, "Testing")
    
    print(f"\nâœ… æµ‹è¯•ç»“æœ:")
    print(f"   Test Loss: {test_results['loss']:.4f}")
    print(f"   Test Accuracy: {test_results['accuracy']:.4f}")
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    final_results = {
        "dataset": args.dataset,
        "best_val_acc": best_val_acc,
        "test_loss": test_results["loss"],
        "test_accuracy": test_results["accuracy"],
        "epochs_trained": epoch,
        "num_classes": num_classes,
        "seq_len": seq_len
    }
    
    with open(os.path.join(save_dir, "final_results.json"), "w") as f:
        json.dump(final_results, f, indent=2)
    
    print("=" * 60)
    print(f"ç»“æœä¿å­˜åˆ°: {save_dir}")
    print("=" * 60)


if __name__ == "__main__":
    main()
