#!/usr/bin/env python3
# SPDX-FileCopyrightText: 2025 Stanford University, ETH Zurich, and the project authors (see CONTRIBUTORS.md)
# SPDX-FileCopyrightText: 2025 This source file is part of the OpenTSLM open-source project.
#
# SPDX-License-Identifier: MIT

"""
UCR ICLåˆ†ç±»æµ‹è¯•è„šæœ¬

åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡ï¼Œåœ¨ä»»æ„UCRæ•°æ®é›†çš„æµ‹è¯•é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/test_ucr_icl_classification.py \\
        --dataset ECG5000 \\
        --pretrained_model OpenTSLM/llama-3.2-1b-m4-sp \\
        --tslanet_checkpoint results/tslanet_ucr/ECG5000/best_model.pt \\
        --icl_checkpoint results/icl_classification/ECG5000/best_model.pt \\
        --k_shot 1

å¯é€‰ï¼šè·¨åŸŸæµ‹è¯•ï¼ˆç”¨Aæ•°æ®é›†çš„TSLANetæ£€ç´¢å™¨åœ¨Bæ•°æ®é›†ä¸Šæµ‹è¯•ï¼‰ï¼š
    python scripts/test_ucr_icl_classification.py \\
        --dataset Wafer \\
        --tslanet_checkpoint results/tslanet_ucr/ECG5000/best_model.pt \\
        --icl_checkpoint results/icl_classification/ECG5000/best_model.pt \\
        --pretrained_model OpenTSLM/llama-3.2-1b-m4-sp \\
        --k_shot 1
"""

import os
import sys
import json
import argparse
import datetime
from pathlib import Path
from typing import List, Dict, Any

import torch
from torch.utils.data import DataLoader
from tqdm.auto import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from opentslm.model.llm.OpenTSLM import OpenTSLM
from opentslm.model.llm.OpenTSLMSP import OpenTSLMSP
from opentslm.model.encoder.TSLANetEncoder import TSLANetEncoder
from opentslm.retrieval.TSLANetRetriever import TSLANetRetriever
from opentslm.time_series_datasets.ucr.UCRICLClassificationDataset import (
    UCRICLClassificationDataset,
    create_icl_collate_fn
)
from opentslm.time_series_datasets.ucr.ucr_loader import load_ucr_dataset, ensure_ucr_data
from opentslm.time_series_datasets.ucr.UCRClassificationDataset import index_to_excel_label
from opentslm.model_config import PATCH_SIZE, ENCODER_OUTPUT_DIM


def parse_args():
    parser = argparse.ArgumentParser(description="UCR ICLåˆ†ç±»æµ‹è¯•")

    # æ•°æ®ç›¸å…³
    parser.add_argument("--dataset", type=str, required=True, help="è¦æµ‹è¯•çš„UCRæ•°æ®é›†åç§°")
    parser.add_argument("--data_path", type=str, default="./data", help="UCRæ•°æ®æ ¹ç›®å½•")
    
    # æ¨¡å‹ç›¸å…³ - OpenTSLM
    parser.add_argument("--pretrained_model", type=str, default=None, 
                        help="é¢„è®­ç»ƒæ¨¡å‹ID (HuggingFace repo_id)")
    parser.add_argument("--icl_checkpoint", type=str, default=None,
                        help="ICLè®­ç»ƒåçš„checkpointè·¯å¾„ (å¯é€‰ï¼Œç”¨äºåŠ è½½fine-tunedæƒé‡)")
    parser.add_argument("--encoder_type", type=str, default="tslanet",
                        choices=["transformer_cnn", "tslanet"],
                        help="ç¼–ç å™¨ç±»å‹")
    parser.add_argument("--llm_id", type=str, default="meta-llama/Llama-3.2-1B",
                        help="LLMæ¨¡å‹ID")
    
    # æ¨¡å‹ç›¸å…³ - TSLANetæ£€ç´¢å™¨
    parser.add_argument("--tslanet_checkpoint", type=str, required=True,
                        help="TSLANetåˆ†ç±»å™¨checkpointè·¯å¾„ (ç”¨äºæ£€ç´¢)")
    
    # ICLç›¸å…³
    parser.add_argument("--k_shot", type=int, default=1, 
                        help="æ¯ä¸ªç±»åˆ«çš„æ”¯æŒæ ·æœ¬æ•°")
    parser.add_argument("--top_m", type=int, default=10,
                        help="æ¯ä¸ªç±»åˆ«æ£€ç´¢çš„å€™é€‰æ•°é‡")
    
    # LoRAç›¸å…³
    parser.add_argument("--no_lora", action="store_true", help="ç¦ç”¨LoRA")
    parser.add_argument("--lora_r", type=int, default=16, help="LoRA rank")
    parser.add_argument("--lora_alpha", type=int, default=32, help="LoRA alpha")
    
    # æµ‹è¯•ç›¸å…³
    parser.add_argument("--batch_size", type=int, default=8, help="æµ‹è¯•æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--max_new_tokens", type=int, default=10, help="ç”Ÿæˆæœ€å¤§tokenæ•°")
    
    # ä¿å­˜ç›¸å…³
    parser.add_argument("--save_dir", type=str, default="results/icl_test", help="ç»“æœä¿å­˜ç›®å½•")
    parser.add_argument("--save_predictions", action="store_true", help="ä¿å­˜è¯¦ç»†é¢„æµ‹ç»“æœ")
    
    # å…¶ä»–
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--device", type=str, default="cuda", help="è®¾å¤‡")
    
    return parser.parse_args()


def set_seed(seed: int):
    """è®¾ç½®éšæœºç§å­"""
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def calculate_accuracy(predictions: List[str], labels: List[str]) -> float:
    """è®¡ç®—åˆ†ç±»å‡†ç¡®ç‡"""
    correct = 0
    for pred, label in zip(predictions, labels):
        pred_clean = pred.strip().upper()
        label_clean = label.strip().upper()
        
        # æå–é¢„æµ‹æ ‡ç­¾
        pred_label = None
        if len(pred_clean) == 1 and pred_clean.isalpha():
            pred_label = pred_clean
        elif len(pred_clean) == 2 and pred_clean.isalpha():
            pred_label = pred_clean  # AA, ABç­‰
        elif pred_clean:
            # å–æœ€åä¸€ä¸ªè¯
            words = pred_clean.split()
            if words:
                last_word = words[-1].strip(".,!?:;")
                if len(last_word) <= 2 and last_word.isalpha():
                    pred_label = last_word.upper()
        
        if pred_label == label_clean:
            correct += 1
    
    return correct / len(predictions) if predictions else 0.0


def load_tslanet_for_retrieval(checkpoint_path: str, device: str):
    """åŠ è½½TSLANetç”¨äºæ£€ç´¢"""
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    config = checkpoint.get("config", {})
    encoder_state = checkpoint["encoder_state"]
    patch_size = config.get("patch_size", 8)
    
    # è·å–max_seq_len: ä¼˜å…ˆä½¿ç”¨ä¿å­˜çš„å€¼ï¼Œå¦åˆ™ä»pos_embedæ¨æ–­
    if "max_seq_len" in checkpoint:
        max_seq_len = checkpoint["max_seq_len"]
    else:
        # ä»pos_embedå½¢çŠ¶æ¨æ–­ (å…¼å®¹æ—§ç‰ˆæœ¬checkpoint)
        pos_embed_shape = encoder_state["pos_embed"].shape
        num_patches = pos_embed_shape[1]
        stride = patch_size // 2
        max_seq_len = (num_patches - 1) * stride + patch_size
    
    # åˆ›å»ºencoder
    encoder = TSLANetEncoder(
        output_dim=config.get("emb_dim", 128),
        dropout=config.get("dropout", 0.15),
        patch_size=patch_size,
        emb_dim=config.get("emb_dim", 128),
        depth=config.get("depth", 2),
        max_seq_len=max_seq_len
    )
    
    # åŠ è½½æƒé‡
    encoder.load_state_dict(encoder_state)
    encoder = encoder.to(device)
    encoder.eval()
    
    print(f"âœ… åŠ è½½TSLANetæ£€ç´¢å™¨: {checkpoint_path}")
    print(f"   åºåˆ—é•¿åº¦: {checkpoint.get('seq_len', 'unknown')}")
    print(f"   ç±»åˆ«æ•°: {checkpoint.get('num_classes', 'unknown')}")
    print(f"   max_seq_len: {max_seq_len}")
    
    return encoder, checkpoint


def create_test_dataset(args, retriever, eos_token: str):
    """åˆ›å»ºæµ‹è¯•Dataset"""
    ensure_ucr_data()
    
    # åŠ è½½æ•°æ®
    train_df, test_df = load_ucr_dataset(args.dataset, raw_data_path=args.data_path)
    
    # è·å–ç±»åˆ«ä¿¡æ¯
    all_labels = sorted(train_df["label"].unique().tolist())
    label_to_idx = {label: idx for idx, label in enumerate(all_labels)}
    
    # æå–æ—¶é—´åºåˆ—å’Œæ ‡ç­¾
    feature_cols = [col for col in train_df.columns if col != "label"]
    
    def df_to_tensors(df):
        ts = torch.tensor(df[feature_cols].values, dtype=torch.float32)
        labels = torch.tensor([label_to_idx[l] for l in df["label"]], dtype=torch.long)
        return ts, labels
    
    train_ts, train_labels = df_to_tensors(train_df)
    test_ts, test_labels = df_to_tensors(test_df)
    
    print(f"ğŸ“Š Dataset: {args.dataset}")
    print(f"   Classes: {len(all_labels)}")
    print(f"   Train samples (ç”¨äºæ£€ç´¢): {len(train_ts)}")
    print(f"   Test samples: {len(test_ts)}")
    
    # æ„å»ºæ£€ç´¢ç´¢å¼• (ç”¨è®­ç»ƒé›†)
    print("\nğŸ”§ æ„å»ºæ£€ç´¢ç´¢å¼•...")
    retriever.build_index(train_ts, train_labels)
    
    # åˆ›å»ºæµ‹è¯•Dataset
    test_dataset = UCRICLClassificationDataset(
        time_series=test_ts,
        labels=test_labels,
        retriever=retriever,
        dataset_name=args.dataset,
        k_shot=args.k_shot,
        top_m=args.top_m,
        eos_token=eos_token,
        split="test",
        exclude_query=False
    )
    
    return test_dataset, len(all_labels)


@torch.no_grad()
def evaluate(
    model,
    data_loader: DataLoader,
    max_new_tokens: int,
    desc: str = "Testing",
) -> Dict[str, Any]:
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    
    all_predictions = []
    all_labels = []
    all_details = []
    total_loss = 0.0
    num_batches = 0
    
    for batch in tqdm(data_loader, desc=desc):
        # è®¡ç®—loss
        loss = model.compute_loss(batch)
        total_loss += loss.item()
        num_batches += 1
        
        # ç”Ÿæˆé¢„æµ‹
        predictions = model.generate(batch, max_new_tokens=max_new_tokens)
        
        for sample, pred in zip(batch, predictions):
            label = sample.get("letter_label", "")
            all_predictions.append(pred)
            all_labels.append(label)
            
            # ä¿å­˜è¯¦ç»†ä¿¡æ¯
            all_details.append({
                "prediction": pred,
                "label": label,
                "query_idx": sample.get("query_idx", -1),
                "support_labels": sample.get("support_labels", []),
            })
    
    avg_loss = total_loss / max(num_batches, 1)
    accuracy = calculate_accuracy(all_predictions, all_labels)
    
    return {
        "loss": avg_loss,
        "accuracy": accuracy,
        "predictions": all_predictions,
        "labels": all_labels,
        "details": all_details,
    }


def main():
    args = parse_args()
    
    print("=" * 60)
    print("UCR ICLåˆ†ç±»æµ‹è¯•")
    print("=" * 60)
    print(f"æ—¶é—´: {datetime.datetime.now()}")
    print(f"æ•°æ®é›†: {args.dataset}")
    print(f"K-shot: {args.k_shot}")
    print(f"Top-M: {args.top_m}")
    print("=" * 60)
    
    set_seed(args.seed)
    
    # è®¾ç½®è®¾å¤‡
    if args.device == "cuda" and torch.cuda.is_available():
        device = "cuda"
    else:
        print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        device = "cpu"
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = os.path.join(args.save_dir, args.dataset)
    os.makedirs(save_dir, exist_ok=True)
    
    # ä¿å­˜é…ç½®
    with open(os.path.join(save_dir, "test_config.json"), "w") as f:
        json.dump(vars(args), f, indent=2)
    
    # åŠ è½½TSLANetç”¨äºæ£€ç´¢
    print("\nğŸ”§ åŠ è½½TSLANetæ£€ç´¢å™¨...")
    tslanet_encoder, tslanet_ckpt = load_tslanet_for_retrieval(args.tslanet_checkpoint, device)
    retriever = TSLANetRetriever(tslanet_encoder, device=device)
    
    # åŠ è½½OpenTSLMæ¨¡å‹
    print("\nğŸ”§ åŠ è½½OpenTSLMæ¨¡å‹...")
    
    use_lora = not args.no_lora
    
    if args.pretrained_model:
        model = OpenTSLM.load_pretrained(
            repo_id=args.pretrained_model,
            device=device,
            enable_lora=use_lora,
        )
        if use_lora and (args.lora_r != 16 or args.lora_alpha != 32):
            model.disable_lora()
            model.enable_lora(lora_r=args.lora_r, lora_alpha=args.lora_alpha)
    else:
        raise ValueError("å¿…é¡»æŒ‡å®š --pretrained_model")
    
    # åŠ è½½ICLè®­ç»ƒåçš„æƒé‡ï¼ˆå¦‚æœæä¾›ï¼‰
    if args.icl_checkpoint:
        print(f"\nğŸ“‚ åŠ è½½ICL checkpoint: {args.icl_checkpoint}")
        icl_ckpt = torch.load(args.icl_checkpoint, map_location=device, weights_only=False)
        
        # åŠ è½½encoderå’Œprojectoræƒé‡
        model.encoder.load_state_dict(icl_ckpt["encoder_state"])
        model.projector.load_state_dict(icl_ckpt["projector_state"])
        
        # åŠ è½½LoRAæƒé‡
        if use_lora and "lora_state" in icl_ckpt:
            model.load_lora_state_from_checkpoint(icl_ckpt, allow_missing=True)
        
        print(f"   Epoch: {icl_ckpt.get('epoch', 'unknown')}")
        print(f"   Test Acc (è®­ç»ƒæ—¶): {icl_ckpt.get('test_acc', 'unknown')}")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
    print("\nğŸ“‚ åˆ›å»ºæµ‹è¯•æ•°æ®é›†...")
    eos_token = model.get_eos_token()
    test_dataset, num_classes = create_test_dataset(args, retriever, eos_token)
    
    # åˆ›å»ºDataLoader
    collate_fn = create_icl_collate_fn(patch_size=PATCH_SIZE)
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        collate_fn=collate_fn,
    )
    
    print(f"   Test batches: {len(test_loader)}")
    
    # æµ‹è¯•
    print("\nğŸš€ å¼€å§‹æµ‹è¯•...")
    test_results = evaluate(model, test_loader, args.max_new_tokens)
    
    # æ‰“å°ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“‹ æµ‹è¯•ç»“æœ")
    print("=" * 60)
    print(f"   Dataset: {args.dataset}")
    print(f"   Test Loss: {test_results['loss']:.4f}")
    print(f"   Test Accuracy: {test_results['accuracy']:.4f}")
    print(f"   Total Samples: {len(test_results['predictions'])}")
    
    # æ˜¾ç¤ºæ ·æœ¬é¢„æµ‹
    print("\n   Sample predictions:")
    for i in range(min(5, len(test_results["predictions"]))):
        pred = test_results["predictions"][i]
        label = test_results["labels"][i]
        pred_short = pred[:40] if len(pred) > 40 else pred
        status = "âœ“" if pred.strip().upper() == label.strip().upper() else "âœ—"
        print(f"     [{status}] Pred: '{pred_short}' | Label: '{label}'")
    
    # ä¿å­˜ç»“æœ
    final_results = {
        "dataset": args.dataset,
        "k_shot": args.k_shot,
        "top_m": args.top_m,
        "test_loss": test_results["loss"],
        "test_accuracy": test_results["accuracy"],
        "num_samples": len(test_results["predictions"]),
        "num_classes": num_classes,
        "tslanet_checkpoint": args.tslanet_checkpoint,
        "icl_checkpoint": args.icl_checkpoint,
        "timestamp": str(datetime.datetime.now()),
    }
    
    with open(os.path.join(save_dir, "test_results.json"), "w") as f:
        json.dump(final_results, f, indent=2)
    
    # ä¿å­˜è¯¦ç»†é¢„æµ‹ï¼ˆå¯é€‰ï¼‰
    if args.save_predictions:
        with open(os.path.join(save_dir, "predictions.json"), "w") as f:
            json.dump(test_results["details"], f, indent=2)
        print(f"\nğŸ’¾ è¯¦ç»†é¢„æµ‹å·²ä¿å­˜åˆ°: {os.path.join(save_dir, 'predictions.json')}")
    
    print("=" * 60)
    print(f"ç»“æœä¿å­˜åˆ°: {save_dir}")
    print("=" * 60)
    
    return test_results["accuracy"]


if __name__ == "__main__":
    main()
