#!/usr/bin/env python3
# SPDX-FileCopyrightText: 2025 Stanford University, ETH Zurich, and the project authors (see CONTRIBUTORS.md)
# SPDX-FileCopyrightText: 2025 This source file is part of the OpenTSLM open-source project.
#
# SPDX-License-Identifier: MIT

"""
UCRå•æ•°æ®é›†Prototypeåˆ†ç±»è®­ç»ƒè„šæœ¬

ä½¿ç”¨Prototypeåˆ†ç±»å¤´æ›¿ä»£ç”Ÿæˆå¼è§£ç ï¼Œæ¶æ„:
[Learnable Prompt] + [TS_tokens] + [CLS] â†’ LLM â†’ CLSéšå‘é‡ â†’ Prototypeå¤´ â†’ logits

ä¸¤é˜¶æ®µè®­ç»ƒ:
- Stage 0: å†»ç»“backboneï¼Œåªè®­ç»ƒ prompt + cls + prototypes + temperature
- Stage 1: è§£å†» encoder + projector + LoRAï¼Œè”åˆè®­ç»ƒ

ä½¿ç”¨æ–¹æ³•:
    # Stage 0 (å¿«é€Ÿæ”¶æ•›)
    python scripts/train_ucr_prototype_single.py \
        --pretrained_model OpenTSLM/llama-3.2-1b-m4-sp \
        --dataset ECG200 \
        --stage 0 \
        --epochs 10

    # Stage 1 (è”åˆè®­ç»ƒ)
    python scripts/train_ucr_prototype_single.py \
        --pretrained_model OpenTSLM/llama-3.2-1b-m4-sp \
        --dataset ECG200 \
        --stage 1 \
        --epochs 30 \
        --resume_from results/prototype_ucr/ECG200/stage0_best.pt
"""

import os
import sys
import json
import argparse
import datetime
from pathlib import Path
from typing import List, Dict, Any

import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.optim import AdamW
from torch.nn.utils import clip_grad_norm_
from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler
from tqdm.auto import tqdm
from transformers import get_linear_schedule_with_warmup

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from opentslm.model.llm.OpenTSLM import OpenTSLM
from opentslm.model.llm.OpenTSLMPrototype import OpenTSLMPrototype
from opentslm.time_series_datasets.ucr.UCRClassificationDataset import UCRClassificationDataset
from opentslm.time_series_datasets.util import extend_time_series_to_match_patch_size_and_aggregate
from opentslm.model_config import PATCH_SIZE, ENCODER_OUTPUT_DIM


def parse_args():
    parser = argparse.ArgumentParser(description="UCRå•æ•°æ®é›†Prototypeåˆ†ç±»è®­ç»ƒ")
    
    # æ•°æ®ç›¸å…³
    parser.add_argument("--dataset", type=str, default="ECG200", help="UCRæ•°æ®é›†åç§°")
    parser.add_argument("--data_path", type=str, default="./data", help="UCRæ•°æ®æ ¹ç›®å½•")
    
    # æ¨¡å‹ç›¸å…³
    parser.add_argument("--pretrained_model", type=str, default=None,
                        help="é¢„è®­ç»ƒæ¨¡å‹ID (HuggingFace repo_id)")
    parser.add_argument("--local_checkpoint", type=str, default=None,
                        help="æœ¬åœ°checkpointè·¯å¾„")
    parser.add_argument("--encoder_type", type=str, default="transformer_cnn",
                        choices=["transformer_cnn", "tslanet"])
    parser.add_argument("--llm_id", type=str, default="meta-llama/Llama-3.2-1B")
    parser.add_argument("--tslanet_patch_size", type=int, default=8)
    
    # Prototypeç›¸å…³
    parser.add_argument("--prompt_len", type=int, default=10, help="å¯å­¦ä¹ Prompté•¿åº¦")
    parser.add_argument("--init_temperature", type=float, default=1.0, help="æ¸©åº¦åˆå§‹å€¼")
    
    # LoRAç›¸å…³
    parser.add_argument("--no_lora", action="store_true", help="ç¦ç”¨LoRA")
    parser.add_argument("--lora_r", type=int, default=16, help="LoRA rank")
    parser.add_argument("--lora_alpha", type=int, default=32, help="LoRA alpha")
    
    # è®­ç»ƒé˜¶æ®µ
    parser.add_argument("--stage", type=int, default=0, choices=[0, 1],
                        help="è®­ç»ƒé˜¶æ®µ: 0=åªè®­ç»ƒå¤´éƒ¨, 1=è”åˆè®­ç»ƒ")
    parser.add_argument("--resume_from", type=str, default=None,
                        help="ä»Stage0 checkpointåŠ è½½æ¨¡å‹æƒé‡ï¼ˆä¸æ¢å¤è®­ç»ƒçŠ¶æ€ï¼Œç”¨äºStage1åŠ è½½Stage0ï¼‰")
    parser.add_argument("--continue_training", type=str, default=None,
                        help="ä»checkpointå®Œå…¨æ¢å¤è®­ç»ƒï¼ˆåŒ…æ‹¬epoch/optimizer/schedulerï¼‰")
    
    # è®­ç»ƒè¶…å‚
    parser.add_argument("--epochs", type=int, default=30)
    parser.add_argument("--batch_size", type=int, default=32)
    parser.add_argument("--lr_prompt", type=float, default=1e-3, help="Prompt/CLSå­¦ä¹ ç‡")
    parser.add_argument("--lr_head", type=float, default=1e-3, help="Prototypeå¤´å­¦ä¹ ç‡")
    parser.add_argument("--lr_encoder", type=float, default=2e-4, help="Encoderå­¦ä¹ ç‡")
    parser.add_argument("--lr_projector", type=float, default=1e-4, help="Projectorå­¦ä¹ ç‡")
    parser.add_argument("--lr_lora", type=float, default=1e-4, help="LoRAå­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=1e-2)
    parser.add_argument("--grad_clip", type=float, default=1.0)
    parser.add_argument("--warmup_ratio", type=float, default=0.1)
    
    # ä¿å­˜ç›¸å…³
    parser.add_argument("--save_dir", type=str, default="results/prototype_ucr")
    
    # å…¶ä»–
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--device", type=str, default="cuda")
    parser.add_argument("--eval_every", type=int, default=5)
    parser.add_argument("--early_stop", type=int, default=10)
    parser.add_argument("--eval_batch_size", type=int, default=32)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1)
    parser.add_argument("--gradient_checkpointing", action="store_true")
    parser.add_argument("--overfit_test", action="store_true", help="ä½¿ç”¨å°å­é›†æµ‹è¯•overfit")
    
    return parser.parse_args()


def setup_distributed():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ"""
    if "LOCAL_RANK" in os.environ:
        local_rank = int(os.environ["LOCAL_RANK"])
        world_size = int(os.environ.get("WORLD_SIZE", 1))
        rank = int(os.environ.get("RANK", 0))
        torch.cuda.set_device(local_rank)
        dist.init_process_group(backend="nccl", init_method="env://")
        return local_rank, world_size, rank
    return 0, 1, 0


def cleanup_distributed():
    if dist.is_initialized():
        dist.destroy_process_group()


def get_model(model):
    return model.module if hasattr(model, "module") else model


def set_seed(seed: int):
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


class PrototypeDataset(torch.utils.data.Dataset):
    """
    é€‚é…Prototypeæ¨¡å‹çš„æ•°æ®é›†åŒ…è£…å™¨
    
    å°†UCRClassificationDatasetçš„æ ·æœ¬è½¬æ¢ä¸ºPrototypeæ¨¡å‹éœ€è¦çš„æ ¼å¼
    """
    def __init__(self, ucr_dataset: UCRClassificationDataset):
        self.ucr_dataset = ucr_dataset
        # æ„å»ºç±»åˆ«tokenåˆ°ç´¢å¼•çš„æ˜ å°„
        self._build_label_mapping()
    
    def _build_label_mapping(self):
        """æ„å»ºæ ‡ç­¾æ˜ å°„"""
        label_mapping = UCRClassificationDataset.get_label_mapping()
        self.token_to_index = {token: i for i, token in enumerate(sorted(label_mapping.values()))}
    
    def __len__(self):
        return len(self.ucr_dataset)
    
    def __getitem__(self, idx):
        sample = self.ucr_dataset[idx]
        
        # å°†ç±»åˆ«tokenè½¬æ¢ä¸ºæ•´æ•°ç´¢å¼•
        class_token = sample.get("class_token", sample["answer"].replace(self.ucr_dataset.EOS_TOKEN, "").strip())
        label_index = self.token_to_index.get(class_token, 0)
        
        return {
            "time_series": sample["time_series"],
            "label_index": label_index,
            "_sample_idx": idx,
        }


def create_data_loaders(args, eos_token: str, world_size: int = 1, rank: int = 0):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    # åˆ›å»ºåŸå§‹æ•°æ®é›†
    train_dataset_raw = UCRClassificationDataset(
        split="train",
        EOS_TOKEN=eos_token,
        dataset_name=args.dataset,
        raw_data_path=args.data_path,
    )
    
    val_dataset_raw = UCRClassificationDataset(
        split="validation",
        EOS_TOKEN=eos_token,
        dataset_name=args.dataset,
        raw_data_path=args.data_path,
    )
    
    test_dataset_raw = UCRClassificationDataset(
        split="test",
        EOS_TOKEN=eos_token,
        dataset_name=args.dataset,
        raw_data_path=args.data_path,
    )
    
    # åŒ…è£…ä¸ºPrototypeæ ¼å¼
    train_dataset = PrototypeDataset(train_dataset_raw)
    val_dataset = PrototypeDataset(val_dataset_raw)
    test_dataset = PrototypeDataset(test_dataset_raw)
    
    # Overfitæµ‹è¯•ï¼šåªä½¿ç”¨å‰10ä¸ªæ ·æœ¬
    if args.overfit_test:
        from torch.utils.data import Subset
        train_dataset = Subset(train_dataset, range(min(10, len(train_dataset))))
        val_dataset = Subset(val_dataset, range(min(10, len(val_dataset))))
        test_dataset = Subset(test_dataset, range(min(10, len(test_dataset))))
        if rank == 0:
            print(f"âš ï¸ Overfit test: ä½¿ç”¨å‰10ä¸ªæ ·æœ¬")
    
    def collate_fn(batch):
        """æ•´ç†æ‰¹æ¬¡æ•°æ®"""
        # å¤„ç†æ—¶é—´åºåˆ—padding
        processed = extend_time_series_to_match_patch_size_and_aggregate(
            batch, patch_size=PATCH_SIZE
        )
        return processed
    
    # é‡‡æ ·å™¨
    train_sampler = None
    if world_size > 1:
        train_sampler = DistributedSampler(
            train_dataset, num_replicas=world_size, rank=rank, shuffle=True
        )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=(train_sampler is None),
        sampler=train_sampler,
        collate_fn=collate_fn,
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.eval_batch_size,
        shuffle=False,
        collate_fn=collate_fn,
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=args.eval_batch_size,
        shuffle=False,
        collate_fn=collate_fn,
    )
    
    return train_loader, val_loader, test_loader, train_sampler


def train_one_epoch(
    model,
    train_loader: DataLoader,
    optimizer,
    scheduler,
    grad_clip: float,
    epoch: int,
    num_epochs: int,
    gradient_accumulation_steps: int = 1,
    rank: int = 0,
) -> float:
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0.0
    num_batches = 0
    optimizer.zero_grad()
    
    pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{num_epochs}", disable=(rank != 0))
    for step, batch in enumerate(pbar):
        loss = model(batch)
        loss = loss / gradient_accumulation_steps
        
        loss.backward()
        
        if (step + 1) % gradient_accumulation_steps == 0:
            clip_grad_norm_(model.parameters(), max_norm=grad_clip)
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()
        
        total_loss += loss.item() * gradient_accumulation_steps
        num_batches += 1
        
        if rank == 0:
            pbar.set_postfix({
                "loss": f"{loss.item() * gradient_accumulation_steps:.4f}",
                "lr": f"{scheduler.get_last_lr()[0]:.2e}"
            })
    
    # å¤„ç†æœ€åä¸è¶³accumulation_stepsçš„batch
    if num_batches % gradient_accumulation_steps != 0:
        clip_grad_norm_(model.parameters(), max_norm=grad_clip)
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()
    
    return total_loss / max(num_batches, 1)


@torch.no_grad()
def evaluate(
    model,
    data_loader: DataLoader,
    desc: str = "Evaluating",
    rank: int = 0,
) -> Dict[str, Any]:
    """è¯„ä¼°æ¨¡å‹"""
    underlying_model = get_model(model)
    underlying_model.eval()
    
    total_loss = 0.0
    total_correct = 0
    total_samples = 0
    all_predictions = []
    all_labels = []
    
    for batch in tqdm(data_loader, desc=desc, disable=(rank != 0)):
        loss, logits = underlying_model.forward_prototype(batch)
        predictions = logits.argmax(dim=-1)
        labels = torch.tensor([s["label_index"] for s in batch], device=logits.device)
        
        total_loss += loss.item()
        total_correct += (predictions == labels).sum().item()
        total_samples += len(batch)
        
        all_predictions.extend(predictions.cpu().tolist())
        all_labels.extend(labels.cpu().tolist())
    
    accuracy = total_correct / total_samples if total_samples > 0 else 0.0
    avg_loss = total_loss / len(data_loader) if len(data_loader) > 0 else 0.0
    
    return {
        "loss": avg_loss,
        "accuracy": accuracy,
        "predictions": all_predictions,
        "labels": all_labels,
    }


def save_checkpoint(
    model,
    optimizer,
    scheduler,
    epoch: int,
    val_loss: float,
    val_acc: float,
    save_path: str,
    args,
    rank: int = 0,
):
    """ä¿å­˜checkpoint"""
    if rank != 0:
        return
    
    underlying_model = get_model(model)
    checkpoint = {
        "encoder_state": underlying_model.encoder.state_dict(),
        "projector_state": underlying_model.projector.state_dict(),
        "prompt_embeds": underlying_model.prompt_embeds.data,
        "cls_embed": underlying_model.cls_embed.data,
        "cls_projector_state": underlying_model.cls_projector.state_dict(),
        "cls_head_state": underlying_model.cls_head.state_dict(),
        "optimizer_state": optimizer.state_dict(),
        "scheduler_state": scheduler.state_dict(),
        "epoch": epoch,
        "val_loss": val_loss,
        "val_acc": val_acc,
        "args": vars(args),
    }
    
    underlying_model.save_lora_state_to_checkpoint(checkpoint)
    torch.save(checkpoint, save_path)
    print(f"ğŸ’¾ Saved checkpoint to: {save_path}")


def main():
    args = parse_args()
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼
    local_rank, world_size, rank = setup_distributed()
    
    if rank == 0:
        print("=" * 60)
        print("UCRå•æ•°æ®é›†Prototypeåˆ†ç±»è®­ç»ƒ")
        print("=" * 60)
        print(f"æ—¶é—´: {datetime.datetime.now()}")
        print(f"æ•°æ®é›†: {args.dataset}")
        print(f"Stage: {args.stage}")
        print(f"Prompté•¿åº¦: {args.prompt_len}")
        print(f"LoRA: {not args.no_lora}")
        print("=" * 60)
    
    set_seed(args.seed + rank)
    
    # è®¾å¤‡
    if world_size > 1:
        device = f"cuda:{local_rank}"
    elif args.device == "cuda" and torch.cuda.is_available():
        device = "cuda"
    else:
        device = "cpu"
        if rank == 0:
            print("âš ï¸ ä½¿ç”¨CPU")
    
    # ä¿å­˜ç›®å½•
    save_dir = os.path.join(args.save_dir, args.dataset)
    if rank == 0:
        os.makedirs(save_dir, exist_ok=True)
        with open(os.path.join(save_dir, f"config_stage{args.stage}.json"), "w") as f:
            json.dump(vars(args), f, indent=2)
    
    if world_size > 1:
        dist.barrier()
    
    # ä¸´æ—¶åˆ›å»ºæ•°æ®é›†è·å–ç±»åˆ«æ•°
    if rank == 0:
        print("\nğŸ“‚ åŠ è½½æ•°æ®è·å–ç±»åˆ«æ•°...")
    temp_dataset = UCRClassificationDataset(
        split="train",
        EOS_TOKEN="<eos>",
        dataset_name=args.dataset,
        raw_data_path=args.data_path,
    )
    num_classes = UCRClassificationDataset.get_num_classes()
    if rank == 0:
        print(f"   ç±»åˆ«æ•°: {num_classes}")
    
    # åˆ›å»ºæ¨¡å‹
    if rank == 0:
        print("\nğŸ”§ åˆ›å»ºæ¨¡å‹...")
    
    use_lora = not args.no_lora
    
    if args.local_checkpoint:
        # ä»æœ¬åœ°checkpointåˆ›å»º
        tslanet_config = {"patch_size": args.tslanet_patch_size, "output_dim": ENCODER_OUTPUT_DIM}
        model = OpenTSLMPrototype(
            llm_id=args.llm_id,
            device=device,
            encoder_type=args.encoder_type,
            tslanet_config=tslanet_config if args.encoder_type == "tslanet" else None,
            prompt_len=args.prompt_len,
            num_classes=num_classes,
            init_temperature=args.init_temperature,
        )
        
        checkpoint = torch.load(args.local_checkpoint, map_location=device, weights_only=False)
        model.encoder.load_state_dict(checkpoint["encoder_state"])
        model.projector.load_state_dict(checkpoint["projector_state"])
        if rank == 0:
            print(f"âœ… åŠ è½½encoder/projector from {args.local_checkpoint}")
        
        if use_lora:
            model.enable_lora(lora_r=args.lora_r, lora_alpha=args.lora_alpha)
            model.load_lora_state_from_checkpoint(checkpoint, allow_missing=True)
    
    elif args.pretrained_model:
        # ä»HuggingFaceåˆ›å»º - éœ€è¦å…ˆåˆ›å»ºåŸºç¡€æ¨¡å‹å†è½¬æ¢
        if rank == 0:
            print(f"ğŸ“‚ ä»HuggingFaceåŠ è½½: {args.pretrained_model}")
        
        # å…ˆåŠ è½½OpenTSLMè·å–æƒé‡
        base_model = OpenTSLM.load_pretrained(
            repo_id=args.pretrained_model,
            device=device,
            enable_lora=False,  # å…ˆä¸å¯ç”¨LoRA
        )
        
        # åˆ›å»ºPrototypeæ¨¡å‹
        model = OpenTSLMPrototype(
            llm_id=base_model.llm.config._name_or_path if hasattr(base_model.llm.config, '_name_or_path') else "meta-llama/Llama-3.2-1B",
            device=device,
            encoder_type="transformer_cnn",
            prompt_len=args.prompt_len,
            num_classes=num_classes,
            init_temperature=args.init_temperature,
        )
        
        # å¤åˆ¶æƒé‡
        model.encoder.load_state_dict(base_model.encoder.state_dict())
        model.projector.load_state_dict(base_model.projector.state_dict())
        
        if use_lora:
            model.enable_lora(lora_r=args.lora_r, lora_alpha=args.lora_alpha)
        
        del base_model
        torch.cuda.empty_cache()
    
    else:
        raise ValueError("å¿…é¡»æŒ‡å®š --pretrained_model æˆ– --local_checkpoint")
    
    # ä»resumeåŠ è½½ï¼ˆå¦‚æœæœ‰ï¼‰
    if args.resume_from:
        if rank == 0:
            print(f"ğŸ“‚ ä»{args.resume_from}æ¢å¤...")
        ckpt = torch.load(args.resume_from, map_location=device, weights_only=False)
        model.prompt_embeds.data = ckpt["prompt_embeds"].to(device)
        model.cls_embed.data = ckpt["cls_embed"].to(device)
        if "cls_projector_state" in ckpt:
            model.cls_projector.load_state_dict(ckpt["cls_projector_state"])
        model.cls_head.load_state_dict(ckpt["cls_head_state"])
        if rank == 0:
            print(f"âœ… æ¢å¤prompt/cls/projector/head")
    
    # é…ç½®è®­ç»ƒé˜¶æ®µ
    if args.stage == 0:
        model.freeze_backbone()
    else:
        model.unfreeze_for_stage1(unfreeze_encoder=True)
    
    # æ¢¯åº¦æ£€æŸ¥ç‚¹
    if args.gradient_checkpointing:
        model.enable_gradient_checkpointing()
    
    # DDP
    if world_size > 1:
        model = DDP(model, device_ids=[local_rank])
        if rank == 0:
            print(f"âœ… DDP (world_size={world_size})")
    
    # æ•°æ®åŠ è½½å™¨
    if rank == 0:
        print("\nğŸ“‚ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    eos_token = get_model(model).get_eos_token()
    train_loader, val_loader, test_loader, train_sampler = create_data_loaders(
        args, eos_token, world_size, rank
    )
    if rank == 0:
        print(f"   Train: {len(train_loader)} batches")
        print(f"   Val: {len(val_loader)} batches")
        print(f"   Test: {len(test_loader)} batches")
    
    # ä¼˜åŒ–å™¨
    if rank == 0:
        print("\nâš™ï¸ åˆ›å»ºä¼˜åŒ–å™¨...")
    underlying_model = get_model(model)
    
    param_groups = []
    if args.stage == 0:
        # Stage 0: åªè®­ç»ƒprompt/cls/projector/head
        param_groups.append({"params": [underlying_model.prompt_embeds, underlying_model.cls_embed], "lr": args.lr_prompt})
        param_groups.append({"params": list(underlying_model.cls_projector.parameters()), "lr": args.lr_head})
        param_groups.append({"params": list(underlying_model.cls_head.parameters()), "lr": args.lr_head})
    else:
        # Stage 1: å…¨éƒ¨è®­ç»ƒ
        param_groups.append({"params": list(underlying_model.encoder.parameters()), "lr": args.lr_encoder})
        param_groups.append({"params": list(underlying_model.projector.parameters()), "lr": args.lr_projector})
        param_groups.append({"params": [underlying_model.prompt_embeds, underlying_model.cls_embed], "lr": args.lr_prompt})
        param_groups.append({"params": list(underlying_model.cls_projector.parameters()), "lr": args.lr_head})
        param_groups.append({"params": list(underlying_model.cls_head.parameters()), "lr": args.lr_head})
        
        if use_lora:
            lora_params = underlying_model.get_lora_parameters()
            if lora_params:
                param_groups.append({"params": lora_params, "lr": args.lr_lora})
    
    optimizer = AdamW(param_groups, weight_decay=args.weight_decay)
    
    # å­¦ä¹ ç‡è°ƒåº¦
    steps_per_epoch = len(train_loader) // args.gradient_accumulation_steps
    total_steps = args.epochs * steps_per_epoch
    warmup_steps = int(args.warmup_ratio * total_steps)
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_steps,
    )
    
    if rank == 0:
        print(f"   Total steps: {total_steps}")
        print(f"   Warmup steps: {warmup_steps}")
    
    # è®­ç»ƒå¾ªç¯
    if rank == 0:
        print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    best_val_acc = 0.0
    patience_counter = 0
    loss_history = []
    start_epoch = 1  # é»˜è®¤ä»1å¼€å§‹
    
    # å¦‚æœæ˜¯å®Œå…¨æ¢å¤è®­ç»ƒï¼ŒåŠ è½½è®­ç»ƒçŠ¶æ€
    if args.continue_training:
        ckpt = torch.load(args.continue_training, map_location=device, weights_only=False)
        
        # æ¢å¤æ¨¡å‹æƒé‡
        underlying_model.prompt_embeds.data = ckpt["prompt_embeds"].to(device)
        underlying_model.cls_embed.data = ckpt["cls_embed"].to(device)
        if "cls_projector_state" in ckpt:
            underlying_model.cls_projector.load_state_dict(ckpt["cls_projector_state"])
        underlying_model.cls_head.load_state_dict(ckpt["cls_head_state"])
        underlying_model.encoder.load_state_dict(ckpt["encoder_state"])
        underlying_model.projector.load_state_dict(ckpt["projector_state"])
        underlying_model.load_lora_state_from_checkpoint(ckpt, allow_missing=True)
        
        # æ¢å¤ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨çŠ¶æ€
        if "optimizer_state" in ckpt:
            optimizer.load_state_dict(ckpt["optimizer_state"])
        if "scheduler_state" in ckpt:
            scheduler.load_state_dict(ckpt["scheduler_state"])
        
        # æ¢å¤Œepoch
        start_epoch = ckpt.get("epoch", 0) + 1
        best_val_acc = ckpt.get("val_acc", 0.0)
        
        if rank == 0:
            print(f"âœ… ä»epoch {start_epoch} æ¢å¤è®­ç»ƒ")
            print(f"   ä¸Šæ¬¡best_val_acc: {best_val_acc:.4f}")
    
    try:
        for epoch in range(start_epoch, args.epochs + 1):
            if train_sampler is not None:
                train_sampler.set_epoch(epoch)
            
            train_loss = train_one_epoch(
                model, train_loader, optimizer, scheduler,
                args.grad_clip, epoch, args.epochs,
                args.gradient_accumulation_steps, rank
            )
            
            # è¯„ä¼°
            if epoch % args.eval_every == 0 or epoch == args.epochs:
                if rank == 0:
                    print(f"\nğŸ“Š Epoch {epoch} è¯„ä¼°...")
                
                val_results = evaluate(model, val_loader, "Validating", rank)
                val_loss = val_results["loss"]
                val_acc = val_results["accuracy"]
                
                if rank == 0:
                    print(f"   Train Loss: {train_loss:.4f}")
                    print(f"   Val Loss: {val_loss:.4f}")
                    print(f"   Val Accuracy: {val_acc:.4f}")
                    
                    # æ¸©åº¦ä¿¡æ¯
                    temp = underlying_model.cls_head.temperature.item()
                    print(f"   Temperature: {temp:.4f}")
                
                if val_acc > best_val_acc:
                    best_val_acc = val_acc
                    patience_counter = 0
                    save_checkpoint(
                        model, optimizer, scheduler, epoch,
                        val_loss, val_acc,
                        os.path.join(save_dir, f"stage{args.stage}_best.pt"),
                        args, rank
                    )
                else:
                    patience_counter += 1
                    if rank == 0:
                        print(f"   (æ— æ”¹è¿›, patience: {patience_counter}/{args.early_stop})")
                
                if rank == 0:
                    loss_history.append({
                        "epoch": epoch,
                        "train_loss": train_loss,
                        "val_loss": val_loss,
                        "val_acc": val_acc,
                    })
                    with open(os.path.join(save_dir, f"loss_history_stage{args.stage}.json"), "w") as f:
                        json.dump(loss_history, f, indent=2)
            else:
                if rank == 0:
                    print(f"Epoch {epoch}: Train Loss = {train_loss:.4f}")
            
            if patience_counter >= args.early_stop:
                if rank == 0:
                    print(f"\nâ¹ï¸ æ—©åœ!")
                break
        
        # æœ€ç»ˆæµ‹è¯•
        if rank == 0:
            print("\n" + "=" * 60)
            print("ğŸ“‹ æœ€ç»ˆæµ‹è¯•...")
            
            # åŠ è½½æœ€ä½³æ¨¡å‹
            best_path = os.path.join(save_dir, f"stage{args.stage}_best.pt")
            if os.path.exists(best_path):
                ckpt = torch.load(best_path, map_location=device, weights_only=False)
                underlying_model.prompt_embeds.data = ckpt["prompt_embeds"].to(device)
                underlying_model.cls_embed.data = ckpt["cls_embed"].to(device)
                if "cls_projector_state" in ckpt:
                    underlying_model.cls_projector.load_state_dict(ckpt["cls_projector_state"])
                underlying_model.cls_head.load_state_dict(ckpt["cls_head_state"])
                underlying_model.encoder.load_state_dict(ckpt["encoder_state"])
                underlying_model.projector.load_state_dict(ckpt["projector_state"])
            
            test_results = evaluate(model, test_loader, "Testing", rank)
            
            print(f"\nâœ… æµ‹è¯•ç»“æœ:")
            print(f"   Test Loss: {test_results['loss']:.4f}")
            print(f"   Test Accuracy: {test_results['accuracy']:.4f}")
            
            final_results = {
                "dataset": args.dataset,
                "stage": args.stage,
                "best_val_acc": best_val_acc,
                "test_loss": test_results["loss"],
                "test_accuracy": test_results["accuracy"],
                "epochs_trained": epoch,
            }
            
            with open(os.path.join(save_dir, f"final_results_stage{args.stage}.json"), "w") as f:
                json.dump(final_results, f, indent=2)
            
            print("=" * 60)
            print(f"ç»“æœä¿å­˜åˆ°: {save_dir}")
    
    finally:
        cleanup_distributed()


if __name__ == "__main__":
    main()
