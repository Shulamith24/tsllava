#!/usr/bin/env python3
# SPDX-FileCopyrightText: 2025 Stanford University, ETH Zurich, and the project authors (see CONTRIBUTORS.md)
# SPDX-FileCopyrightText: 2025 This source file is part of the OpenTSLM open-source project.
#
# SPDX-License-Identifier: MIT

"""
M2: UEAå¤šå˜é‡æ•°æ®é›†åˆ†ç±»è®­ç»ƒï¼ˆåŸºäºStage2é¢„è®­ç»ƒæ¨¡å‹ï¼‰

åŠ è½½curriculum learningçš„stage2é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œåˆ†ç±»å¾®è°ƒã€‚
ç¼–ç å™¨å’ŒæŠ•å½±å±‚è§£å†»ï¼ŒLLMä½¿ç”¨LoRAè®­ç»ƒã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
    # å•GPUè®­ç»ƒ
    python scripts/train_uea_classification_pretrained.py \
        --pretrained_model OpenTSLM/llama-3.2-1b-m4-sp \
        --dataset Epilepsy \
        --epochs 30 \
        --batch_size 4
    
    # DDPå¤šGPUè®­ç»ƒ
    torchrun --nproc_per_node=2 scripts/train_uea_classification_pretrained.py \
        --pretrained_model OpenTSLM/llama-3.2-1b-m4-sp \
        --dataset Epilepsy \
        --gradient_accumulation_steps 4 \
        --gradient_checkpointing

è®­ç»ƒé…ç½®ï¼š
- LoRA: r=16, alpha=32 (é»˜è®¤å¯ç”¨)
- Encoder LR: 2e-4
- Projector LR: 1e-4
- LoRA LR: 1e-4
"""

import os
import sys
import json
import argparse
import datetime
from pathlib import Path
from typing import List, Dict, Any

import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.optim import AdamW
from torch.nn.utils import clip_grad_norm_
from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler
from tqdm.auto import tqdm
from transformers import get_linear_schedule_with_warmup

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from opentslm.model.llm.OpenTSLM import OpenTSLM
from opentslm.model.llm.OpenTSLMSP import OpenTSLMSP
from opentslm.time_series_datasets.uea.UEAClassificationDataset import UEAClassificationDataset
from opentslm.time_series_datasets.util import extend_time_series_to_match_patch_size_and_aggregate
from opentslm.model_config import PATCH_SIZE, ENCODER_OUTPUT_DIM


def parse_args():
    parser = argparse.ArgumentParser(description="M2: UEAå¤šå˜é‡æ•°æ®é›†åˆ†ç±»è®­ç»ƒï¼ˆåŸºäºStage2é¢„è®­ç»ƒæ¨¡å‹ï¼‰")
    
    # æ•°æ®ç›¸å…³
    parser.add_argument("--dataset", type=str, default="Epilepsy", help="UEAæ•°æ®é›†åç§°")
    
    # æ¨¡å‹ç›¸å…³ - ä½¿ç”¨HuggingFaceé¢„è®­ç»ƒæ¨¡å‹
    parser.add_argument("--pretrained_model", type=str, default=None,
                        help="é¢„è®­ç»ƒæ¨¡å‹ID (HuggingFace repo_idï¼Œå¦‚ OpenTSLM/llama-3.2-1b-m4-sp)")
    
    # æ¨¡å‹ç›¸å…³ - ä½¿ç”¨æœ¬åœ°checkpointï¼ˆå¦‚train_curriculum_pretrain.pyäº§ç”Ÿçš„ï¼‰
    parser.add_argument("--local_checkpoint", type=str, default=None,
                        help="æœ¬åœ°checkpointè·¯å¾„ (å¦‚ results/curriculum_pretrain/.../best_model.pt)")
    parser.add_argument("--encoder_type", type=str, default="transformer_cnn",
                        choices=["transformer_cnn", "tslanet"],
                        help="ç¼–ç å™¨ç±»å‹ï¼ˆä½¿ç”¨local_checkpointæ—¶å¿…é¡»æŒ‡å®šï¼‰")
    parser.add_argument("--llm_id", type=str, default="meta-llama/Llama-3.2-1B",
                        help="LLMæ¨¡å‹IDï¼ˆä½¿ç”¨local_checkpointæ—¶éœ€è¦ï¼‰")
    parser.add_argument("--tslanet_patch_size", type=int, default=8,
                        help="TSLANetçš„patch_sizeï¼ˆä½¿ç”¨tslanetç¼–ç å™¨æ—¶ï¼‰")
    
    # LoRAç›¸å…³ (é»˜è®¤å¯ç”¨)
    parser.add_argument("--no_lora", action="store_true", help="ç¦ç”¨LoRAï¼ˆä¸æ¨èï¼‰")
    parser.add_argument("--lora_r", type=int, default=16, help="LoRA rank")
    parser.add_argument("--lora_alpha", type=int, default=32, help="LoRA alpha")
    
    # è®­ç»ƒç›¸å…³
    parser.add_argument("--epochs", type=int, default=30, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=16, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--lr_encoder", type=float, default=2e-4, help="ç¼–ç å™¨å­¦ä¹ ç‡")
    parser.add_argument("--lr_projector", type=float, default=1e-4, help="æŠ•å½±å±‚å­¦ä¹ ç‡")
    parser.add_argument("--lr_lora", type=float, default=1e-4, help="LoRAå­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=1e-2, help="æƒé‡è¡°å‡")
    parser.add_argument("--grad_clip", type=float, default=1.0, help="æ¢¯åº¦è£å‰ª")
    parser.add_argument("--warmup_ratio", type=float, default=0.03, help="é¢„çƒ­æ¯”ä¾‹")
    
    # ä¿å­˜ç›¸å…³
    parser.add_argument("--save_dir", type=str, default="results/m2_uea_pretrained", help="ç»“æœä¿å­˜ç›®å½•")
    
    # DDPå’Œæ¢¯åº¦ç›¸å…³
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    parser.add_argument("--gradient_checkpointing", action="store_true", help="å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
    parser.add_argument("--freeze_encoder", action="store_true", help="å†»ç»“ç¼–ç å™¨å‚æ•°")
    
    # å…¶ä»–
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--device", type=str, default="cuda", help="è®¾å¤‡")
    parser.add_argument("--eval_every", type=int, default=5, help="æ¯Nè½®è¯„ä¼°ä¸€æ¬¡")
    parser.add_argument("--early_stop", type=int, default=10, help="æ—©åœè€å¿ƒå€¼")
    parser.add_argument("--max_new_tokens", type=int, default=10, help="ç”Ÿæˆæœ€å¤§tokenæ•°")
    parser.add_argument("--eval_batch_size", type=int, default=8, help="è¯„ä¼°æ‰¹æ¬¡å¤§å°")
    
    return parser.parse_args()


def setup_distributed():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒï¼ˆç”¨äºtorchrunï¼‰"""
    if "LOCAL_RANK" in os.environ:
        local_rank = int(os.environ["LOCAL_RANK"])
        world_size = int(os.environ.get("WORLD_SIZE", 1))
        rank = int(os.environ.get("RANK", 0))
        
        torch.cuda.set_device(local_rank)
        dist.init_process_group(backend="nccl", init_method="env://")
        
        return local_rank, world_size, rank
    return 0, 1, 0


def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ"""
    if dist.is_initialized():
        dist.destroy_process_group()


def get_model(model):
    """è·å–åº•å±‚æ¨¡å‹ï¼ˆå…¼å®¹DDPåŒ…è£…ï¼‰"""
    return model.module if hasattr(model, "module") else model


def set_seed(seed: int):
    """è®¾ç½®éšæœºç§å­"""
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def calculate_accuracy(predictions: List[str], labels: List[str]) -> float:
    """è®¡ç®—åˆ†ç±»å‡†ç¡®ç‡"""
    correct = 0
    for pred, label in zip(predictions, labels):
        pred_clean = pred.strip()
        pred_label = None
        
        if len(pred_clean) == 1 and pred_clean.isalpha():
            pred_label = pred_clean.upper()
        elif pred_clean:
            words = pred_clean.split()
            if words:
                last_word = words[-1].strip(".,!?:;")
                if last_word and last_word[0].isalpha():
                    pred_label = last_word[0].upper()
        
        label_clean = label.strip().upper()
        if pred_label == label_clean:
            correct += 1
    
    return correct / len(predictions) if predictions else 0.0


def create_data_loaders(args, eos_token: str, world_size: int = 1, rank: int = 0):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    train_dataset = UEAClassificationDataset(
        split="train",
        EOS_TOKEN=eos_token,
        dataset_name=args.dataset,
    )
    
    val_dataset = UEAClassificationDataset(
        split="validation",
        EOS_TOKEN=eos_token,
        dataset_name=args.dataset,
    )
    
    test_dataset = UEAClassificationDataset(
        split="test",
        EOS_TOKEN=eos_token,
        dataset_name=args.dataset,
    )
    
    def collate_fn(batch):
        return extend_time_series_to_match_patch_size_and_aggregate(
            batch, patch_size=PATCH_SIZE
        )
    
    # åˆ†å¸ƒå¼é‡‡æ ·å™¨ï¼ˆä»…è®­ç»ƒé›†ï¼‰
    train_sampler = None
    if world_size > 1:
        train_sampler = DistributedSampler(
            train_dataset, num_replicas=world_size, rank=rank, shuffle=True
        )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=(train_sampler is None),
        sampler=train_sampler,
        collate_fn=collate_fn,
    )
    
    # è¯„ä¼°ç”¨DataLoaderï¼ˆæ”¯æŒæ‰¹é‡è¯„ä¼°ï¼‰
    eval_batch_size = getattr(args, 'eval_batch_size', 8)
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=eval_batch_size,
        shuffle=False,
        collate_fn=collate_fn,
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=eval_batch_size,
        shuffle=False,
        collate_fn=collate_fn,
    )
    
    return train_loader, val_loader, test_loader, train_sampler


def train_one_epoch(
    model,
    train_loader: DataLoader,
    optimizer,
    scheduler,
    grad_clip: float,
    epoch: int,
    num_epochs: int,
    gradient_accumulation_steps: int = 1,
    rank: int = 0,
) -> float:
    """è®­ç»ƒä¸€ä¸ªepochï¼ˆæ”¯æŒæ¢¯åº¦ç´¯ç§¯å’ŒDDPï¼‰"""
    # 1. è®­ç»ƒæ¨¡å¼+åˆå§‹åŒ–
    model.train()
    total_loss = 0.0
    num_batches = 0
    optimizer.zero_grad()
    
    pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{num_epochs}", disable=(rank != 0))
    for step, batch in enumerate(pbar):
        # 2. è®¡ç®—æŸå¤±ï¼ˆç¼©æ”¾ç”¨äºæ¢¯åº¦ç´¯ç§¯ï¼‰
        loss = model(batch)
        loss = loss / gradient_accumulation_steps
        
        # 3. åå‘ä¼ æ’­,ddpä¼šåœ¨gpuä¹‹é—´è‡ªåŠ¨åŒæ­¥æ¢¯åº¦
        loss.backward()
        
        # 4. æ¢¯åº¦ç´¯ç§¯å®Œæˆåæ›´æ–°
        if (step + 1) % gradient_accumulation_steps == 0:
            clip_grad_norm_(model.parameters(), max_norm=grad_clip)
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()
        
        total_loss += loss.item() * gradient_accumulation_steps
        num_batches += 1
        
        if rank == 0:
            pbar.set_postfix({
                "loss": f"{loss.item() * gradient_accumulation_steps:.4f}",
                "lr": f"{scheduler.get_last_lr()[0]:.2e}"
            })
    
    # å¤„ç†æœ€åä¸è¶³accumulation_stepsçš„batch
    if num_batches % gradient_accumulation_steps != 0:
        clip_grad_norm_(model.parameters(), max_norm=grad_clip)
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()
    
    return total_loss / max(num_batches, 1)


@torch.no_grad()
def evaluate(
    model,
    data_loader: DataLoader,
    max_new_tokens: int,
    desc: str = "Evaluating",
    rank: int = 0,
) -> Dict[str, Any]:
    
    #1. è®¾ç½®æ¨¡å‹è¯„ä¼°å’Œåˆå§‹åŒ–
    model.eval()
    underlying_model = get_model(model)
    all_predictions = []
    all_labels = []
    total_loss = 0.0
    num_batches = 0
    
    #2. éå†æ•°æ®
    for batch in tqdm(data_loader, desc=desc, disable=(rank != 0)):
        # è¯„ä¼°æ—¶ä¸éœ€è¦æ¢¯åº¦åŒæ­¥ï¼Œæ‰€ä»¥å¯ä»¥ç›´æ¥è°ƒç”¨åº•å±‚æ¨¡å‹
        loss = underlying_model.compute_loss(batch)
        total_loss += loss.item()
        num_batches += 1
        
        predictions = underlying_model.generate(batch, max_new_tokens=max_new_tokens)
        
        for sample, pred in zip(batch, predictions):
            all_predictions.append(pred)
            all_labels.append(sample["answer"].replace(underlying_model.get_eos_token(), "").strip())
    
    avg_loss = total_loss / max(num_batches, 1)
    accuracy = calculate_accuracy(all_predictions, all_labels)
    
    return {
        "loss": avg_loss,
        "accuracy": accuracy,
        "predictions": all_predictions,
        "labels": all_labels,
    }


def save_checkpoint(
    model,
    optimizer,
    scheduler,
    epoch: int,
    val_loss: float,
    val_acc: float,
    save_path: str,
    args,
    rank: int = 0,
):
    """ä¿å­˜checkpointï¼ˆä»…rank=0æ‰§è¡Œï¼‰"""
    if rank != 0:
        return
    
    underlying_model = get_model(model)
    checkpoint = {
        "encoder_state": underlying_model.encoder.state_dict(),
        "projector_state": underlying_model.projector.state_dict(),
        "optimizer_state": optimizer.state_dict(),
        "scheduler_state": scheduler.state_dict(),
        "epoch": epoch,
        "val_loss": val_loss,
        "val_acc": val_acc,
        "args": vars(args),
    }
    
    underlying_model.save_lora_state_to_checkpoint(checkpoint)
    torch.save(checkpoint, save_path)
    print(f"ğŸ’¾ Saved checkpoint to: {save_path}")


def main():
    args = parse_args()
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
    local_rank, world_size, rank = setup_distributed()
    
    # ä»…rank=0æ‰“å°ä¿¡æ¯
    if rank == 0:
        print("=" * 60)
        print("M2: UEAå¤šå˜é‡æ•°æ®é›†åˆ†ç±»è®­ç»ƒï¼ˆåŸºäºStage2é¢„è®­ç»ƒæ¨¡å‹ï¼‰")
        print("=" * 60)
        print(f"æ—¶é—´: {datetime.datetime.now()}")
        print(f"æ•°æ®é›†: {args.dataset}")
        print(f"é¢„è®­ç»ƒæ¨¡å‹: {args.pretrained_model}")
        print(f"LoRA: {not args.no_lora}")
        print(f"DDP: world_size={world_size}")
        print(f"æ¢¯åº¦ç´¯ç§¯: {args.gradient_accumulation_steps}")
        print(f"æ¢¯åº¦æ£€æŸ¥ç‚¹: {args.gradient_checkpointing}")
        print("=" * 60)
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed + rank)
    
    # è®¾ç½®è®¾å¤‡
    if world_size > 1:
        device = f"cuda:{local_rank}"
    elif args.device == "cuda" and torch.cuda.is_available():
        device = "cuda"
    else:
        if rank == 0:
            print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        device = "cpu"
    
    # åˆ›å»ºä¿å­˜ç›®å½•ï¼ˆä»…rank=0ï¼‰
    save_dir = os.path.join(args.save_dir, args.dataset)
    if rank == 0:
        os.makedirs(save_dir, exist_ok=True)
        with open(os.path.join(save_dir, "config.json"), "w") as f:
            json.dump(vars(args), f, indent=2)
    
    # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
    if world_size > 1:
        dist.barrier()
    
    # åŠ è½½æ¨¡å‹
    if rank == 0:
        print("\nğŸ”§ åŠ è½½æ¨¡å‹...")
    
    use_lora = not args.no_lora
    
    if args.local_checkpoint:
        # ä½¿ç”¨æœ¬åœ°checkpointåŠ è½½ï¼ˆå¦‚train_curriculum_pretrain.pyäº§ç”Ÿçš„ï¼‰
        if rank == 0:
            print(f"ğŸ“‚ ä»æœ¬åœ°checkpointåŠ è½½: {args.local_checkpoint}")
            print(f"   ç¼–ç å™¨ç±»å‹: {args.encoder_type}")
            print(f"   LLM: {args.llm_id}")
        
        # åˆ›å»ºæ¨¡å‹
        tslanet_config = {
            "patch_size": args.tslanet_patch_size,
            "output_dim": ENCODER_OUTPUT_DIM,
        }
        model = OpenTSLMSP(
            llm_id=args.llm_id,
            device=device,
            encoder_type=args.encoder_type,
            tslanet_config=tslanet_config if args.encoder_type == "tslanet" else None,
        )
        
        # åŠ è½½checkpointæƒé‡
        checkpoint = torch.load(args.local_checkpoint, map_location=device, weights_only=False)
        model.encoder.load_state_dict(checkpoint["encoder_state"])
        model.projector.load_state_dict(checkpoint["projector_state"])
        if rank == 0:
            print(f"âœ… å·²åŠ è½½encoderå’Œprojectoræƒé‡")
        
        # å¯ç”¨LoRA
        if use_lora:
            model.enable_lora(lora_r=args.lora_r, lora_alpha=args.lora_alpha)
            # å°è¯•åŠ è½½checkpointä¸­çš„LoRAæƒé‡ï¼ˆå¦‚æœæœ‰ï¼‰
            model.load_lora_state_from_checkpoint(checkpoint, allow_missing=True)
    
    elif args.pretrained_model:
        # ä½¿ç”¨HuggingFaceé¢„è®­ç»ƒæ¨¡å‹
        if rank == 0:
            print(f"ğŸ“‚ ä»HuggingFaceåŠ è½½: {args.pretrained_model}")
        
        model = OpenTSLM.load_pretrained(
            repo_id=args.pretrained_model,
            device=device,
            enable_lora=use_lora,
        )
        
        # å¦‚æœéœ€è¦è‡ªå®šä¹‰LoRAå‚æ•°
        if use_lora and (args.lora_r != 16 or args.lora_alpha != 32):
            model.disable_lora()
            model.enable_lora(lora_r=args.lora_r, lora_alpha=args.lora_alpha)
            if rank == 0:
                print(f"ğŸ“ é‡æ–°é…ç½®LoRA: r={args.lora_r}, alpha={args.lora_alpha}")
    
    else:
        raise ValueError("å¿…é¡»æŒ‡å®š --pretrained_model æˆ– --local_checkpoint ä¹‹ä¸€")
    
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
    if args.gradient_checkpointing:
        model.enable_gradient_checkpointing()
    
    # å†»ç»“ç¼–ç å™¨
    if args.freeze_encoder:
        for param in model.encoder.parameters():
            param.requires_grad = False
        if rank == 0:
            print("ğŸ§Š ç¼–ç å™¨å‚æ•°å·²å†»ç»“")
    
    # DDPåŒ…è£…
    if world_size > 1:
        model = DDP(model, device_ids=[local_rank])
        if rank == 0:
            print(f"âœ… æ¨¡å‹å·²ç”¨DDPåŒ…è£… (world_size={world_size})")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    if rank == 0:
        print("\nğŸ“‚ åŠ è½½æ•°æ®...")
    eos_token = get_model(model).get_eos_token()
    train_loader, val_loader, test_loader, train_sampler = create_data_loaders(
        args, eos_token, world_size, rank
    )
    
    if rank == 0:
        print(f"   Train batches: {len(train_loader)}")
        print(f"   Val batches: {len(val_loader)}")
        print(f"   Test batches: {len(test_loader)}")
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    if rank == 0:
        print("\nâš™ï¸ åˆ›å»ºä¼˜åŒ–å™¨...")
    underlying_model = get_model(model)
    
    # æ ¹æ®æ˜¯å¦å†»ç»“ç¼–ç å™¨å†³å®šå‚æ•°ç»„
    param_groups = []
    if not args.freeze_encoder:
        param_groups.append({"params": underlying_model.encoder.parameters(), "lr": args.lr_encoder})
    param_groups.append({"params": underlying_model.projector.parameters(), "lr": args.lr_projector})
    
    if use_lora:
        lora_params = underlying_model.get_lora_parameters()
        if lora_params:
            param_groups.append({"params": lora_params, "lr": args.lr_lora})
    
    optimizer = AdamW(param_groups, weight_decay=args.weight_decay)
    
    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆè€ƒè™‘æ¢¯åº¦ç´¯ç§¯ï¼‰
    effective_batch_size = args.batch_size * args.gradient_accumulation_steps * world_size
    steps_per_epoch = len(train_loader) // args.gradient_accumulation_steps
    total_steps = args.epochs * steps_per_epoch
    warmup_steps = int(args.warmup_ratio * total_steps)
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_steps,
    )
    
    if rank == 0:
        print(f"   Effective batch size: {effective_batch_size}")
        print(f"   Total steps: {total_steps}")
        print(f"   Warmup steps: {warmup_steps}")
    
    # è®­ç»ƒå¾ªç¯
    if rank == 0:
        print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    best_val_acc = 0.0
    patience_counter = 0
    loss_history = []
    epoch = 0
    
    try:
        for epoch in range(1, args.epochs + 1):
            # è®¾ç½®samplerçš„epochï¼ˆDDPå¿…éœ€ï¼‰
            if train_sampler is not None:
                train_sampler.set_epoch(epoch)
            
            # è®­ç»ƒ
            train_loss = train_one_epoch(
                model, train_loader, optimizer, scheduler,
                args.grad_clip, epoch, args.epochs,
                args.gradient_accumulation_steps, rank
            )
            
            # å®šæœŸè¯„ä¼°
            if epoch % args.eval_every == 0 or epoch == args.epochs:
                if rank == 0:
                    print(f"\nğŸ“Š Epoch {epoch} è¯„ä¼°...")
                
                val_results = evaluate(model, val_loader, args.max_new_tokens, "Validating", rank)
                val_loss = val_results["loss"]
                val_acc = val_results["accuracy"]
                
                if rank == 0:
                    print(f"   Train Loss: {train_loss:.4f}")
                    print(f"   Val Loss: {val_loss:.4f}")
                    print(f"   Val Accuracy: {val_acc:.4f}")
                    
                    print("   Sample predictions:")
                    for i in range(min(3, len(val_results["predictions"]))):
                        pred = val_results["predictions"][i]
                        label = val_results["labels"][i]
                        pred_short = pred[-50:] if len(pred) > 50 else pred
                        print(f"     Pred: '{pred_short}' | Label: '{label}'")
                
                if val_acc > best_val_acc:
                    best_val_acc = val_acc
                    patience_counter = 0
                    save_checkpoint(
                        model, optimizer, scheduler, epoch,
                        val_loss, val_acc,
                        os.path.join(save_dir, "best_model.pt"),
                        args, rank
                    )
                else:
                    patience_counter += 1
                    if rank == 0:
                        print(f"   (æ— æ”¹è¿›, patience: {patience_counter}/{args.early_stop})")
                
                if rank == 0:
                    loss_history.append({
                        "epoch": epoch,
                        "train_loss": train_loss,
                        "val_loss": val_loss,
                        "val_acc": val_acc,
                    })
                    with open(os.path.join(save_dir, "loss_history.json"), "w") as f:
                        json.dump(loss_history, f, indent=2)
            else:
                if rank == 0:
                    print(f"Epoch {epoch}: Train Loss = {train_loss:.4f}")
            
            if patience_counter >= args.early_stop:
                if rank == 0:
                    print(f"\nâ¹ï¸ æ—©åœ! éªŒè¯å‡†ç¡®ç‡ {args.early_stop} è½®æœªæ”¹è¿›")
                break
        
        # æœ€ç»ˆæµ‹è¯•ï¼ˆä»…rank=0ï¼‰
        if rank == 0:
            print("\n" + "=" * 60)
            print("ğŸ“‹ æœ€ç»ˆæµ‹è¯•è¯„ä¼°...")
            
            best_ckpt = torch.load(os.path.join(save_dir, "best_model.pt"), map_location=device, weights_only=False)
            underlying_model.encoder.load_state_dict(best_ckpt["encoder_state"])
            underlying_model.projector.load_state_dict(best_ckpt["projector_state"])
            underlying_model.load_lora_state_from_checkpoint(best_ckpt, allow_missing=True)
            
            test_results = evaluate(model, test_loader, args.max_new_tokens, "Testing", rank)
            
            print(f"\nâœ… æµ‹è¯•ç»“æœ:")
            print(f"   Test Loss: {test_results['loss']:.4f}")
            print(f"   Test Accuracy: {test_results['accuracy']:.4f}")
            
            final_results = {
                "dataset": args.dataset,
                "pretrained_model": args.pretrained_model,
                "best_val_acc": best_val_acc,
                "test_loss": test_results["loss"],
                "test_accuracy": test_results["accuracy"],
                "epochs_trained": epoch,
            }
            
            with open(os.path.join(save_dir, "final_results.json"), "w") as f:
                json.dump(final_results, f, indent=2)
            
            with open(os.path.join(save_dir, "test_predictions.json"), "w") as f:
                json.dump({
                    "predictions": test_results["predictions"],
                    "labels": test_results["labels"],
                }, f, indent=2)
            
            print("=" * 60)
            print(f"ç»“æœä¿å­˜åˆ°: {save_dir}")
            print("=" * 60)
    
    finally:
        cleanup_distributed()


if __name__ == "__main__":
    main()
