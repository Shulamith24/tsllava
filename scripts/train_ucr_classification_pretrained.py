#!/usr/bin/env python3
# SPDX-FileCopyrightText: 2025 Stanford University, ETH Zurich, and the project authors (see CONTRIBUTORS.md)
# SPDX-FileCopyrightText: 2025 This source file is part of the OpenTSLM open-source project.
#
# SPDX-License-Identifier: MIT

"""
M2: UCRå•æ•°æ®é›†åˆ†ç±»è®­ç»ƒï¼ˆåŸºäºStage2é¢„è®­ç»ƒæ¨¡å‹ï¼‰

åŠ è½½curriculum learningçš„stage2é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œåˆ†ç±»å¾®è°ƒã€‚
ç¼–ç å™¨å’ŒæŠ•å½±å±‚è§£å†»ï¼ŒLLMä½¿ç”¨LoRAè®­ç»ƒã€‚
ä½¿ç”¨ç‰¹æ®Šç±»åˆ«token: <c0>, <c1>, ... <cK-1>

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/train_ucr_classification_pretrained.py \
        --pretrained_model OpenTSLM/llama-3.2-1b-m4-sp \
        --dataset ECG5000 \
        --epochs 30 \
        --batch_size 4

è®­ç»ƒé…ç½®ï¼š
- LoRA: r=16, alpha=32 (é»˜è®¤å¯ç”¨)
- Encoder LR: 2e-4
- Projector LR: 1e-4
- LoRA LR: 1e-4
- ä½¿ç”¨ç‰¹æ®Šç±»åˆ«token (<c0>, <c1>, ...) æ›¿ä»£å­—æ¯æ ‡ç­¾
- çº¦æŸè§£ç ï¼šåªå…è®¸è¾“å‡ºç±»åˆ«token + EOS
"""

import os
import sys
import json
import argparse
import datetime
from pathlib import Path
from typing import List, Dict, Any

import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.optim import AdamW
from torch.nn.utils import clip_grad_norm_
from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler
from tqdm.auto import tqdm
from transformers import get_linear_schedule_with_warmup, LogitsProcessor, LogitsProcessorList

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from opentslm.model.llm.OpenTSLM import OpenTSLM
from opentslm.model.llm.OpenTSLMSP import OpenTSLMSP
from opentslm.time_series_datasets.ucr.UCRClassificationDataset import UCRClassificationDataset
from opentslm.time_series_datasets.util import extend_time_series_to_match_patch_size_and_aggregate
from opentslm.model_config import PATCH_SIZE, ENCODER_OUTPUT_DIM


def parse_args():
    parser = argparse.ArgumentParser(description="M2: UCRå•æ•°æ®é›†åˆ†ç±»è®­ç»ƒï¼ˆåŸºäºStage2é¢„è®­ç»ƒæ¨¡å‹ï¼‰")

    # å¿…é¡»æŒ‡å®š
    parser.add_argument("--gradient_checkpointing", action="store_true", help="å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
    parser.add_argument("--freeze_encoder", action="store_true", help="å†»ç»“ç¼–ç å™¨å‚æ•°")

    # æ•°æ®ç›¸å…³
    parser.add_argument("--dataset", type=str, default="CricketZ", help="UCRæ•°æ®é›†åç§°")
    parser.add_argument("--data_path", type=str, default="./data", help="UCRæ•°æ®æ ¹ç›®å½•")
    
    # æ¨¡å‹ç›¸å…³ - ä½¿ç”¨HuggingFaceé¢„è®­ç»ƒæ¨¡å‹
    parser.add_argument("--pretrained_model", type=str, default=None, 
                        help="é¢„è®­ç»ƒæ¨¡å‹ID (HuggingFace repo_idï¼Œå¦‚ OpenTSLM/llama-3.2-1b-m4-sp)")
    
    # æ¨¡å‹ç›¸å…³ - ä½¿ç”¨æœ¬åœ°checkpointï¼ˆå¦‚train_curriculum_pretrain.pyäº§ç”Ÿçš„ï¼‰
    parser.add_argument("--local_checkpoint", type=str, default=None,
                        help="æœ¬åœ°checkpointè·¯å¾„ (å¦‚ results/curriculum_pretrain/.../best_model.pt)")
    parser.add_argument("--encoder_type", type=str, default="transformer_cnn",
                        choices=["transformer_cnn", "tslanet"],
                        help="ç¼–ç å™¨ç±»å‹ï¼ˆä½¿ç”¨local_checkpointæ—¶å¿…é¡»æŒ‡å®šï¼‰")
    parser.add_argument("--llm_id", type=str, default="meta-llama/Llama-3.2-1B",
                        help="LLMæ¨¡å‹IDï¼ˆä½¿ç”¨local_checkpointæ—¶éœ€è¦ï¼‰")
    parser.add_argument("--tslanet_patch_size", type=int, default=8,
                        help="TSLANetçš„patch_sizeï¼ˆä½¿ç”¨tslanetç¼–ç å™¨æ—¶ï¼‰")
    
    # LoRAç›¸å…³ (é»˜è®¤å¯ç”¨)
    parser.add_argument("--no_lora", action="store_true", help="ç¦ç”¨LoRAï¼ˆä¸æ¨èï¼‰")
    parser.add_argument("--lora_r", type=int, default=16, help="LoRA rank")
    parser.add_argument("--lora_alpha", type=int, default=32, help="LoRA alpha")
    
    # è®­ç»ƒç›¸å…³
    parser.add_argument("--epochs", type=int, default=30, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=32, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--lr_encoder", type=float, default=2e-4, help="ç¼–ç å™¨å­¦ä¹ ç‡")
    parser.add_argument("--lr_projector", type=float, default=1e-4, help="æŠ•å½±å±‚å­¦ä¹ ç‡")
    parser.add_argument("--lr_lora", type=float, default=1e-4, help="LoRAå­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=1e-2, help="æƒé‡è¡°å‡")
    parser.add_argument("--grad_clip", type=float, default=1.0, help="æ¢¯åº¦è£å‰ª")
    parser.add_argument("--warmup_ratio", type=float, default=0.03, help="é¢„çƒ­æ¯”ä¾‹")
    
    # ä¿å­˜ç›¸å…³
    parser.add_argument("--save_dir", type=str, default="results/m2_ucr_pretrained", help="ç»“æœä¿å­˜ç›®å½•")
    
    # DDPå’Œæ¢¯åº¦ç›¸å…³
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    
    # å…¶ä»–
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--device", type=str, default="cuda", help="è®¾å¤‡")
    parser.add_argument("--eval_every", type=int, default=5, help="æ¯Nè½®è¯„ä¼°ä¸€æ¬¡")
    parser.add_argument("--early_stop", type=int, default=10, help="æ—©åœè€å¿ƒå€¼")
    parser.add_argument("--max_new_tokens", type=int, default=2, help="ç”Ÿæˆæœ€å¤§tokenæ•°ï¼ˆç±»åˆ«token + EOSï¼‰")
    parser.add_argument("--eval_batch_size", type=int, default=32, help="è¯„ä¼°æ‰¹æ¬¡å¤§å°")
    
    return parser.parse_args()


def setup_distributed():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒï¼ˆç”¨äºtorchrunï¼‰"""
    if "LOCAL_RANK" in os.environ:
        local_rank = int(os.environ["LOCAL_RANK"])
        world_size = int(os.environ.get("WORLD_SIZE", 1))
        rank = int(os.environ.get("RANK", 0))
        
        torch.cuda.set_device(local_rank)
        dist.init_process_group(backend="nccl", init_method="env://")
        
        return local_rank, world_size, rank
    return 0, 1, 0


def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ"""
    if dist.is_initialized():
        dist.destroy_process_group()


def get_model(model):
    """è·å–åº•å±‚æ¨¡å‹ï¼ˆå…¼å®¹DDPåŒ…è£…ï¼‰"""
    return model.module if hasattr(model, "module") else model


def set_seed(seed: int):
    """è®¾ç½®éšæœºç§å­"""
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def calculate_accuracy(predictions: List[str], labels: List[str]) -> float:
    """
    è®¡ç®—åˆ†ç±»å‡†ç¡®ç‡ - é€‚é…ç‰¹æ®Štokenæ ¼å¼ (<c0>, <c1>, ...)
    
    ç›´æ¥æ¯”è¾ƒç”Ÿæˆçš„tokenä¸çœŸå®æ ‡ç­¾
    """
    import re
    correct = 0
    for pred, label in zip(predictions, labels):
        pred_clean = pred.strip()
        label_clean = label.strip()
        
        # å°è¯•ä»é¢„æµ‹ä¸­æå– <cN> æ ¼å¼çš„token
        match = re.search(r'<c\d+>', pred_clean)
        if match:
            pred_token = match.group()
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨æ•´ä¸ªé¢„æµ‹
            pred_token = pred_clean
        
        # ç›´æ¥æ¯”è¾ƒ
        if pred_token == label_clean:
            correct += 1
    
    return correct / len(predictions) if predictions else 0.0


def add_class_tokens_to_model(model, num_classes: int, device: str, rank: int = 0):
    """
    æ·»åŠ ç±»åˆ«ç‰¹æ®Štokenåˆ°tokenizerå’Œembeddingå±‚
    
    Args:
        model: OpenTSLMSP æ¨¡å‹
        num_classes: ç±»åˆ«æ•°é‡
        device: è®¾å¤‡
        rank: DDP rank
    
    Returns:
        class_tokens: ç±»åˆ«tokenåˆ—è¡¨ ['<c0>', '<c1>', ...]
        class_token_ids: å¯¹åº”çš„token IDåˆ—è¡¨
    """
    class_tokens = [f"<c{i}>" for i in range(num_classes)]
    
    # æ·»åŠ åˆ°tokenizer
    num_added = model.tokenizer.add_tokens(class_tokens, special_tokens=True)
    if rank == 0:
        print(f"âœ… Added {num_added} class tokens to tokenizer")
    
    # è°ƒæ•´embeddingå¤§å°
    old_vocab_size = model.llm.get_input_embeddings().weight.shape[0]
    model.llm.resize_token_embeddings(len(model.tokenizer))
    new_vocab_size = model.llm.get_input_embeddings().weight.shape[0]
    
    if rank == 0:
        print(f"   Vocabulary size: {old_vocab_size} -> {new_vocab_size}")
    
    # æ”¹è¿›çš„åˆå§‹åŒ–ï¼šæ¯ä¸ªç±»åˆ«tokenä½¿ç”¨ä¸åŒçš„åˆå§‹åŒ–
    # ä»å·²æœ‰tokenä¸­éšæœºé‡‡æ ·ï¼Œå¹¶æ·»åŠ å°çš„æ‰°åŠ¨
    with torch.no_grad():
        embedding = model.llm.get_input_embeddings()
        lm_head = model.llm.lm_head
        
        if num_added > 0:
            # è·å–å·²æœ‰embeddingçš„ç»Ÿè®¡ä¿¡æ¯
            old_embeddings = embedding.weight[:-num_added]
            emb_mean = old_embeddings.mean(dim=0)
            emb_std = old_embeddings.std(dim=0)
            
            # ä¸ºæ¯ä¸ªç±»åˆ«tokenç”Ÿæˆä¸åŒçš„åˆå§‹åŒ–
            for i in range(num_added):
                # æ–¹æ³•ï¼šå‡å€¼ + éšæœºæ‰°åŠ¨ (æ‰°åŠ¨å¹…åº¦ä¸ºæ ‡å‡†å·®çš„10%)
                noise = torch.randn_like(emb_mean) * emb_std * 0.1
                embedding.weight[-num_added + i] = emb_mean + noise
            
            # åŒæ ·å¤„ç†lm_head
            old_head = lm_head.weight[:-num_added]
            head_mean = old_head.mean(dim=0)
            head_std = old_head.std(dim=0)
            
            for i in range(num_added):
                noise = torch.randn_like(head_mean) * head_std * 0.1
                lm_head.weight[-num_added + i] = head_mean + noise
            
            if rank == 0:
                print(f"   Initialized {num_added} class tokens with mean + random perturbation")
    
    # ç¡®ä¿æ–°tokençš„embeddingå¯è®­ç»ƒ
    embedding.weight.requires_grad = True
    lm_head.weight.requires_grad = True
    
    # è·å–token IDs
    class_token_ids = [model.tokenizer.convert_tokens_to_ids(t) for t in class_tokens]
    if rank == 0:
        print(f"   Class token IDs: {class_token_ids[:5]}..." if len(class_token_ids) > 5 else f"   Class token IDs: {class_token_ids}")
    
    return class_tokens, class_token_ids


class AllowedTokensLogitsProcessor(LogitsProcessor):
    """
    çº¦æŸè§£ç çš„Logitså¤„ç†å™¨ï¼šåªå…è®¸ç‰¹å®štokenè¢«ç”Ÿæˆ
    """
    def __init__(self, allowed_token_ids: List[int]):
        self.allowed_token_ids = set(allowed_token_ids)
    
    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:
        # åˆ›å»ºmaskï¼Œåªä¿ç•™å…è®¸çš„token
        mask = torch.full_like(scores, float('-inf'))
        for token_id in self.allowed_token_ids:
            if token_id < scores.shape[-1]:
                mask[:, token_id] = 0
        return scores + mask


class IndexedDataset(torch.utils.data.Dataset):
    """
    ä¸ºæ•°æ®é›†åŒ…è£…ä¸€ä¸ªç´¢å¼•ï¼Œç”¨äºåˆ†å¸ƒå¼è¯„ä¼°æ—¶çš„å»é‡
    """
    def __init__(self, dataset):
        self.dataset = dataset
    
    def __len__(self):
        return len(self.dataset)
    
    def __getitem__(self, idx):
        sample = self.dataset[idx]
        # æ·»åŠ åŸå§‹ç´¢å¼•åˆ°æ ·æœ¬ä¸­
        sample["_sample_idx"] = idx
        return sample


def create_data_loaders(args, eos_token: str, world_size: int = 1, rank: int = 0):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = UCRClassificationDataset(
        split="train",
        EOS_TOKEN=eos_token,
        dataset_name=args.dataset,
        raw_data_path=args.data_path,
    )
    
    val_dataset = UCRClassificationDataset(
        split="validation",
        EOS_TOKEN=eos_token,
        dataset_name=args.dataset,
        raw_data_path=args.data_path,
    )
    
    test_dataset = UCRClassificationDataset(
        split="test",
        EOS_TOKEN=eos_token,
        dataset_name=args.dataset,
        raw_data_path=args.data_path,
    )
    
    # ç”¨IndexedDatasetåŒ…è£…è¯„ä¼°æ•°æ®é›†ï¼Œä¸ºæ¯ä¸ªæ ·æœ¬æ·»åŠ ç´¢å¼•
    indexed_val_dataset = IndexedDataset(val_dataset)
    indexed_test_dataset = IndexedDataset(test_dataset)
    
    # Collateå‡½æ•°
    def collate_fn(batch):
        return extend_time_series_to_match_patch_size_and_aggregate(
            batch, patch_size=PATCH_SIZE
        )
    
    # åˆ†å¸ƒå¼é‡‡æ ·å™¨
    train_sampler = None
    val_sampler = None
    test_sampler = None
    if world_size > 1:
        train_sampler = DistributedSampler(
            train_dataset, num_replicas=world_size, rank=rank, shuffle=True
        )
        # è¯„ä¼°é›†ä½¿ç”¨åˆ†å¸ƒå¼é‡‡æ ·å™¨ï¼ˆshuffle=Falseä¿æŒé¡ºåºï¼‰
        val_sampler = DistributedSampler(
            indexed_val_dataset, num_replicas=world_size, rank=rank, shuffle=False
        )
        test_sampler = DistributedSampler(
            indexed_test_dataset, num_replicas=world_size, rank=rank, shuffle=False
        )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=(train_sampler is None),
        sampler=train_sampler,
        collate_fn=collate_fn,
    )
    
    # è¯„ä¼°ç”¨DataLoaderï¼ˆä½¿ç”¨åˆ†å¸ƒå¼é‡‡æ ·+ç´¢å¼•è·Ÿè¸ªï¼‰
    eval_batch_size = getattr(args, 'eval_batch_size', 8)
    
    val_loader = DataLoader(
        indexed_val_dataset,
        batch_size=eval_batch_size,
        shuffle=False,
        sampler=val_sampler,
        collate_fn=collate_fn,
    )
    
    test_loader = DataLoader(
        indexed_test_dataset,
        batch_size=eval_batch_size,
        shuffle=False,
        sampler=test_sampler,
        collate_fn=collate_fn,
    )
    
    return train_loader, val_loader, test_loader, train_sampler, len(val_dataset), len(test_dataset)


def train_one_epoch(
    model,
    train_loader: DataLoader,
    optimizer,
    scheduler,
    grad_clip: float,
    epoch: int,
    num_epochs: int,
    gradient_accumulation_steps: int = 1,
    rank: int = 0,
) -> float:
    """è®­ç»ƒä¸€ä¸ªepochï¼ˆæ”¯æŒæ¢¯åº¦ç´¯ç§¯å’ŒDDPï¼‰"""
    model.train()
    total_loss = 0.0
    num_batches = 0
    optimizer.zero_grad()
    
    pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{num_epochs}", disable=(rank != 0))
    for step, batch in enumerate(pbar):
        # è®¡ç®—æŸå¤±ï¼ˆç¼©æ”¾ç”¨äºæ¢¯åº¦ç´¯ç§¯ï¼‰
        # ä½¿ç”¨model(batch)è°ƒç”¨forwardæ–¹æ³•ï¼ŒDDPæ¢¯åº¦åŒæ­¥åœ¨backward()æ—¶è‡ªåŠ¨è¿›è¡Œ
        loss = model(batch)
        loss = loss / gradient_accumulation_steps
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        # æ¢¯åº¦ç´¯ç§¯å®Œæˆåæ›´æ–°
        if (step + 1) % gradient_accumulation_steps == 0:
            clip_grad_norm_(model.parameters(), max_norm=grad_clip)
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()
        
        total_loss += loss.item() * gradient_accumulation_steps
        num_batches += 1
        
        if rank == 0:
            pbar.set_postfix({
                "loss": f"{loss.item() * gradient_accumulation_steps:.4f}",
                "lr": f"{scheduler.get_last_lr()[0]:.2e}"
            })
    
    # å¤„ç†æœ€åä¸è¶³accumulation_stepsçš„batch
    if num_batches % gradient_accumulation_steps != 0:
        clip_grad_norm_(model.parameters(), max_norm=grad_clip)
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()
    
    return total_loss / max(num_batches, 1)


@torch.no_grad()
def evaluate(
    model,
    data_loader: DataLoader,
    max_new_tokens: int,
    class_token_ids: List[int] | None = None,
    desc: str = "Evaluating",
    rank: int = 0,
    world_size: int = 1,
    total_samples: int | None = None,
) -> Dict[str, Any]:
    """
    åˆ†å¸ƒå¼è¯„ä¼°æ¨¡å‹ï¼ˆä½¿ç”¨æ ·æœ¬ç´¢å¼•æ­£ç¡®å»é‡ï¼‰
    
    Args:
        model: æ¨¡å‹ï¼ˆDDP åŒ…è£…æˆ–åº•å±‚æ¨¡å‹éƒ½å¯ä»¥ï¼‰
        data_loader: æ•°æ®åŠ è½½å™¨ï¼ˆä½¿ç”¨IndexedDataset + DistributedSamplerï¼‰
        max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
        class_token_ids: ç±»åˆ«tokençš„IDåˆ—è¡¨ï¼Œç”¨äºçº¦æŸè§£ç 
        desc: è¿›åº¦æ¡æè¿°
        rank: DDP rank
        world_size: GPU æ•°é‡
        total_samples: çœŸå®æ ·æœ¬æ•°ï¼Œç”¨äºéªŒè¯å»é‡ç»“æœ
    """
    import re
    import pickle
    
    # å§‹ç»ˆä½¿ç”¨åº•å±‚æ¨¡å‹è¯„ä¼°
    underlying_model = get_model(model)
    underlying_model.eval()
    
    # ä½¿ç”¨å­—å…¸æŒ‰ç´¢å¼•å­˜å‚¨ç»“æœï¼ˆè‡ªåŠ¨å»é‡ï¼‰
    results_by_idx = {}
    total_loss = 0.0
    num_batches = 0
    
    # è®¾ç½®çº¦æŸè§£ç å¤„ç†å™¨
    logits_processor = None
    if class_token_ids is not None:
        eos_token_id = underlying_model.tokenizer.eos_token_id
        allowed_ids = class_token_ids + [eos_token_id]
        logits_processor = LogitsProcessorList([AllowedTokensLogitsProcessor(allowed_ids)])
    
    for batch in tqdm(data_loader, desc=desc, disable=(rank != 0)):
        # ä½¿ç”¨åº•å±‚æ¨¡å‹
        loss = underlying_model.compute_loss(batch)
        total_loss += loss.item()
        num_batches += 1
        
        # ç”Ÿæˆé¢„æµ‹ï¼ˆä½¿ç”¨çº¦æŸè§£ç ï¼‰
        if logits_processor is not None:
            inputs_embeds, attention_mask = underlying_model.pad_and_apply_batch(batch)
            gen_ids = underlying_model.llm.generate(
                inputs_embeds=inputs_embeds,
                attention_mask=attention_mask,
                max_new_tokens=max_new_tokens,
                logits_processor=logits_processor,
                do_sample=False,
            )
            predictions = underlying_model.tokenizer.batch_decode(gen_ids, skip_special_tokens=False)
            # æ¸…ç†å¤šä½™çš„ç‰¹æ®Štokenï¼Œä¿ç•™<cN>æ ¼å¼
            cleaned_predictions = []
            for p in predictions:
                match = re.search(r'<c\d+>', p)
                if match:
                    cleaned_predictions.append(match.group())
                else:
                    cleaned_predictions.append(p.strip())
            predictions = cleaned_predictions
        else:
            predictions = underlying_model.generate(batch, max_new_tokens=max_new_tokens)
        
        # æ”¶é›†ç»“æœï¼ˆä½¿ç”¨æ ·æœ¬ç´¢å¼•ä½œä¸ºkeyï¼‰
        for sample, pred in zip(batch, predictions):
            idx = sample.get("_sample_idx", -1)
            label = sample["answer"].replace(underlying_model.get_eos_token(), "").strip()
            results_by_idx[idx] = {"prediction": pred, "label": label}
    
    # åˆ†å¸ƒå¼èšåˆï¼šæ”¶é›†æ‰€æœ‰ rank çš„ç»“æœ
    if world_size > 1:
        # åºåˆ—åŒ–æœ¬åœ°ç»“æœ
        local_data = pickle.dumps({
            "results_by_idx": results_by_idx,
            "loss": total_loss,
            "num_batches": num_batches,
        })
        local_size = torch.tensor([len(local_data)], device=underlying_model.device)
        
        # æ”¶é›†æ‰€æœ‰ rank çš„æ•°æ®å¤§å°
        all_sizes = [torch.zeros_like(local_size) for _ in range(world_size)]
        dist.all_gather(all_sizes, local_size)
        max_size = max(s.item() for s in all_sizes)
        
        # å¡«å……åˆ°ç›¸åŒå¤§å°
        local_tensor = torch.zeros(int(max_size), dtype=torch.uint8, device=underlying_model.device)
        local_tensor[:len(local_data)] = torch.tensor(list(local_data), dtype=torch.uint8, device=underlying_model.device)
        
        # æ”¶é›†æ‰€æœ‰æ•°æ®
        all_tensors = [torch.zeros_like(local_tensor) for _ in range(world_size)]
        dist.all_gather(all_tensors, local_tensor)
        
        # ååºåˆ—åŒ–å¹¶åˆå¹¶ï¼ˆå­—å…¸è‡ªåŠ¨å»é‡ï¼šç›¸åŒç´¢å¼•åªä¿ç•™ä¸€ä»½ï¼‰
        merged_results = {}
        total_loss = 0.0
        num_batches = 0
        
        for tensor, size in zip(all_tensors, all_sizes):
            data = pickle.loads(bytes(tensor[:size.item()].cpu().tolist()))
            merged_results.update(data["results_by_idx"])  # è‡ªåŠ¨å»é‡
            total_loss += data["loss"]
            num_batches += data["num_batches"]
        
        results_by_idx = merged_results
    
    # æŒ‰ç´¢å¼•æ’åºå¹¶æå–ç»“æœ
    sorted_indices = sorted(results_by_idx.keys())
    all_predictions = [results_by_idx[idx]["prediction"] for idx in sorted_indices]
    all_labels = [results_by_idx[idx]["label"] for idx in sorted_indices]
    
    # éªŒè¯æ ·æœ¬æ•°é‡
    if total_samples is not None and len(all_predictions) != total_samples:
        if rank == 0:
            print(f"âš ï¸ è­¦å‘Š: æœŸæœ› {total_samples} ä¸ªæ ·æœ¬ï¼Œå®é™… {len(all_predictions)} ä¸ª")
    
    # è®¡ç®—æŒ‡æ ‡
    avg_loss = total_loss / max(num_batches, 1)
    accuracy = calculate_accuracy(all_predictions, all_labels)
    
    return {
        "loss": avg_loss,
        "accuracy": accuracy,
        "predictions": all_predictions,
        "labels": all_labels,
    }


def save_checkpoint(
    model,
    optimizer,
    scheduler,
    epoch: int,
    val_loss: float,
    val_acc: float,
    save_path: str,
    args,
    rank: int = 0,
):
    """ä¿å­˜checkpointï¼ˆä»…rank=0æ‰§è¡Œï¼‰"""
    if rank != 0:
        return
    
    underlying_model = get_model(model)
    checkpoint = {
        "encoder_state": underlying_model.encoder.state_dict(),
        "projector_state": underlying_model.projector.state_dict(),
        "optimizer_state": optimizer.state_dict(),
        "scheduler_state": scheduler.state_dict(),
        "epoch": epoch,
        "val_loss": val_loss,
        "val_acc": val_acc,
        "args": vars(args),
    }
    
    # ä¿å­˜LoRAæƒé‡
    underlying_model.save_lora_state_to_checkpoint(checkpoint)
    
    # ä¿å­˜ class token çš„ embedding å’Œ lm_head æƒé‡
    # è¿™äº›æ˜¯è®­ç»ƒæ—¶æ–°æ·»åŠ çš„ç‰¹æ®Š tokenï¼Œå¿…é¡»ä¿å­˜
    checkpoint["embedding_weight"] = underlying_model.llm.get_input_embeddings().weight.detach().cpu()
    checkpoint["lm_head_weight"] = underlying_model.llm.lm_head.weight.detach().cpu()
    checkpoint["tokenizer_vocab_size"] = len(underlying_model.tokenizer)
    
    torch.save(checkpoint, save_path)
    print(f"ğŸ’¾ Saved checkpoint to: {save_path}")


def main():
    args = parse_args()
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
    local_rank, world_size, rank = setup_distributed()
    
    # ä»…rank=0æ‰“å°ä¿¡æ¯
    if rank == 0:
        print("=" * 60)
        print("M2: UCRå•æ•°æ®é›†åˆ†ç±»è®­ç»ƒï¼ˆåŸºäºStage2é¢„è®­ç»ƒæ¨¡å‹ï¼‰")
        print("=" * 60)
        print(f"æ—¶é—´: {datetime.datetime.now()}")
        print(f"æ•°æ®é›†: {args.dataset}")
        print(f"é¢„è®­ç»ƒæ¨¡å‹: {args.pretrained_model}")
        print(f"LoRA: {not args.no_lora}")
        print(f"DDP: world_size={world_size}")
        print(f"æ¢¯åº¦ç´¯ç§¯: {args.gradient_accumulation_steps}")
        print(f"æ¢¯åº¦æ£€æŸ¥ç‚¹: {args.gradient_checkpointing}")
        print("=" * 60)
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed + rank)  # æ¯ä¸ªrankä½¿ç”¨ä¸åŒçš„éšæœºç§å­
    
    # è®¾ç½®è®¾å¤‡
    if world_size > 1:
        device = f"cuda:{local_rank}"
    elif args.device == "cuda" and torch.cuda.is_available():
        device = "cuda"
    else:
        if rank == 0:
            print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        device = "cpu"
    
    # åˆ›å»ºä¿å­˜ç›®å½•ï¼ˆä»…rank=0ï¼‰
    save_dir = os.path.join(args.save_dir, args.dataset)
    if rank == 0:
        os.makedirs(save_dir, exist_ok=True)
        # ä¿å­˜é…ç½®
        with open(os.path.join(save_dir, "config.json"), "w") as f:
            json.dump(vars(args), f, indent=2)
    
    # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
    if world_size > 1:
        dist.barrier()
    
    # åŠ è½½æ¨¡å‹
    if rank == 0:
        print("\nğŸ”§ åŠ è½½æ¨¡å‹...")
    
    use_lora = not args.no_lora
    
    if args.local_checkpoint:
        # ä½¿ç”¨æœ¬åœ°checkpointåŠ è½½ï¼ˆå¦‚train_curriculum_pretrain.pyäº§ç”Ÿçš„ï¼‰
        if rank == 0:
            print(f"ğŸ“‚ ä»æœ¬åœ°checkpointåŠ è½½: {args.local_checkpoint}")
            print(f"   ç¼–ç å™¨ç±»å‹: {args.encoder_type}")
            print(f"   LLM: {args.llm_id}")
        
        # åˆ›å»ºæ¨¡å‹
        tslanet_config = {
            "patch_size": args.tslanet_patch_size,
            "output_dim": ENCODER_OUTPUT_DIM,
        }
        model = OpenTSLMSP(
            llm_id=args.llm_id,
            device=device,
            encoder_type=args.encoder_type,
            tslanet_config=tslanet_config if args.encoder_type == "tslanet" else None,
        )
        
        # åŠ è½½checkpointæƒé‡
        checkpoint = torch.load(args.local_checkpoint, map_location=device, weights_only=False)
        model.encoder.load_state_dict(checkpoint["encoder_state"])
        model.projector.load_state_dict(checkpoint["projector_state"])
        if rank == 0:
            print(f"âœ… å·²åŠ è½½encoderå’Œprojectoræƒé‡")
        
        # å¯ç”¨LoRA
        if use_lora:
            model.enable_lora(lora_r=args.lora_r, lora_alpha=args.lora_alpha)
            # å°è¯•åŠ è½½checkpointä¸­çš„LoRAæƒé‡ï¼ˆå¦‚æœæœ‰ï¼‰
            model.load_lora_state_from_checkpoint(checkpoint, allow_missing=True)
    
    elif args.pretrained_model:
        # ä½¿ç”¨HuggingFaceé¢„è®­ç»ƒæ¨¡å‹
        if rank == 0:
            print(f"ğŸ“‚ ä»HuggingFaceåŠ è½½: {args.pretrained_model}")
        
        model = OpenTSLM.load_pretrained(
            repo_id=args.pretrained_model,
            device=device,
            enable_lora=use_lora,
        )
        
        # å¦‚æœéœ€è¦è‡ªå®šä¹‰LoRAå‚æ•°
        if use_lora and (args.lora_r != 16 or args.lora_alpha != 32):
            model.disable_lora()
            model.enable_lora(lora_r=args.lora_r, lora_alpha=args.lora_alpha)
            if rank == 0:
                print(f"ğŸ“ é‡æ–°é…ç½®LoRA: r={args.lora_r}, alpha={args.lora_alpha}")
    
    else:
        raise ValueError("å¿…é¡»æŒ‡å®š --pretrained_model æˆ– --local_checkpoint ä¹‹ä¸€")
    
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
    if args.gradient_checkpointing:
        model.enable_gradient_checkpointing()
    
    # å†»ç»“ç¼–ç å™¨ï¼ˆå¯é€‰ï¼‰
    if args.freeze_encoder:
        for param in model.encoder.parameters():
            param.requires_grad = False
        if rank == 0:
            print("ğŸ§Š ç¼–ç å™¨å‚æ•°å·²å†»ç»“")
    
    # DDPåŒ…è£…
    if world_size > 1:
        model = DDP(model, device_ids=[local_rank])
        if rank == 0:
            print(f"âœ… æ¨¡å‹å·²ç”¨DDPåŒ…è£… (world_size={world_size})")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    if rank == 0:
        print("\nğŸ“‚ åŠ è½½æ•°æ®...")
    eos_token = get_model(model).get_eos_token()
    train_loader, val_loader, test_loader, train_sampler, val_size, test_size = create_data_loaders(
        args, eos_token, world_size, rank
    )
    
    if rank == 0:
        print(f"   Train batches: {len(train_loader)}")
        print(f"   Val batches: {len(val_loader)}")
        print(f"   Test batches: {len(test_loader)}")
    
    # æ·»åŠ ç±»åˆ«ç‰¹æ®Štokenåˆ°æ¨¡å‹
    if rank == 0:
        print("\nğŸ¯ æ·»åŠ ç±»åˆ«token...")
    num_classes = UCRClassificationDataset.get_num_classes()
    underlying_model_for_tokens = get_model(model)
    class_tokens, class_token_ids = add_class_tokens_to_model(
        underlying_model_for_tokens, num_classes, device, rank
    )
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    if rank == 0:
        print("\nâš™ï¸ åˆ›å»ºä¼˜åŒ–å™¨...")
    underlying_model = get_model(model)
    
    # æ ¹æ®æ˜¯å¦å†»ç»“ç¼–ç å™¨å†³å®šå‚æ•°ç»„
    param_groups = []
    if not args.freeze_encoder:
        param_groups.append({"params": underlying_model.encoder.parameters(), "lr": args.lr_encoder})
    param_groups.append({"params": underlying_model.projector.parameters(), "lr": args.lr_projector})
    
    if use_lora:
        lora_params = underlying_model.get_lora_parameters()
        if lora_params:
            param_groups.append({"params": lora_params, "lr": args.lr_lora})
    
    # æ·»åŠ æ–°å¢çš„ç±»åˆ«tokençš„embeddingå’Œlm_headæƒé‡åˆ°ä¼˜åŒ–å™¨
    # è¿™äº›æƒé‡éœ€è¦æ›´é«˜çš„å­¦ä¹ ç‡æ¥å¿«é€Ÿå­¦ä¹ 
    embedding_weight = underlying_model.llm.get_input_embeddings().weight
    lm_head_weight = underlying_model.llm.lm_head.weight
    param_groups.append({
        "params": [embedding_weight, lm_head_weight], 
        "lr": args.lr_lora * 2  # ä½¿ç”¨æ›´é«˜çš„å­¦ä¹ ç‡
    })
    if rank == 0:
        print(f"   Added embedding and lm_head to optimizer (lr={args.lr_lora * 2:.2e})")
    
    optimizer = AdamW(param_groups, weight_decay=args.weight_decay)
    
    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆè€ƒè™‘æ¢¯åº¦ç´¯ç§¯ï¼‰
    effective_batch_size = args.batch_size * args.gradient_accumulation_steps * world_size
    steps_per_epoch = len(train_loader) // args.gradient_accumulation_steps
    total_steps = args.epochs * steps_per_epoch
    warmup_steps = int(args.warmup_ratio * total_steps)
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_steps,
    )
    
    if rank == 0:
        print(f"   Effective batch size: {effective_batch_size}")
        print(f"   Total steps: {total_steps}")
        print(f"   Warmup steps: {warmup_steps}")
    
    # è®­ç»ƒå¾ªç¯
    if rank == 0:
        print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    best_val_acc = 0.0
    patience_counter = 0
    loss_history = []
    epoch = 0  # åˆå§‹åŒ–é˜²æ­¢unbound
    
    try:
        for epoch in range(1, args.epochs + 1):
            # è®¾ç½®samplerçš„epochï¼ˆDDPå¿…éœ€ï¼‰
            if train_sampler is not None:
                train_sampler.set_epoch(epoch)
            
            # è®­ç»ƒ
            train_loss = train_one_epoch(
                model, train_loader, optimizer, scheduler,
                args.grad_clip, epoch, args.epochs,
                args.gradient_accumulation_steps, rank
            )
            
            # å®šæœŸè¯„ä¼°
            # æ‰€æœ‰rankå‚ä¸åˆ†å¸ƒå¼è¯„ä¼°ï¼Œä½¿ç”¨ç´¢å¼•è‡ªåŠ¨å»é‡
            if epoch % args.eval_every == 0 or epoch == args.epochs:
                if rank == 0:
                    print(f"\nğŸ“Š Epoch {epoch} è¯„ä¼°...")
                
                # åˆ†å¸ƒå¼è¯„ä¼°ï¼šæ‰€æœ‰rankå‚ä¸ï¼Œç»“æœé€šè¿‡ç´¢å¼•è‡ªåŠ¨å»é‡
                val_results = evaluate(
                    model, val_loader, args.max_new_tokens, 
                    class_token_ids=class_token_ids, desc="Validating",
                    rank=rank, world_size=world_size, total_samples=val_size
                )
                val_loss = val_results["loss"]
                val_acc = val_results["accuracy"]
                
                if rank == 0:
                    print(f"   Train Loss: {train_loss:.4f}")
                    print(f"   Val Loss: {val_loss:.4f}")
                    print(f"   Val Accuracy: {val_acc:.4f}")
                    
                    # æ˜¾ç¤ºä¸€äº›é¢„æµ‹æ ·æœ¬
                    print("   Sample predictions:")
                    for i in range(min(3, len(val_results["predictions"]))):
                        pred = val_results["predictions"][i]
                        label = val_results["labels"][i]
                        pred_short = pred[-50:] if len(pred) > 50 else pred
                        print(f"     Pred: '{pred_short}' | Label: '{label}'")
                    
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    if val_acc > best_val_acc:
                        best_val_acc = val_acc
                        patience_counter = 0
                        save_checkpoint(
                            model, optimizer, scheduler, epoch,
                            val_loss, val_acc,
                            os.path.join(save_dir, "best_model.pt"),
                            args, rank
                        )
                    else:
                        patience_counter += 1
                        print(f"   (æ— æ”¹è¿›, patience: {patience_counter}/{args.early_stop})")
                    
                    # è®°å½•å†å²
                    loss_history.append({
                        "epoch": epoch,
                        "train_loss": train_loss,
                        "val_loss": val_loss,
                        "val_acc": val_acc,
                    })
                    with open(os.path.join(save_dir, "loss_history.json"), "w") as f:
                        json.dump(loss_history, f, indent=2)
                
                # åŒæ­¥ patience_counter å’Œ best_val_acc ç»™æ‰€æœ‰ rank
                if world_size > 1:
                    patience_tensor = torch.tensor([patience_counter], device=device)
                    best_val_acc_tensor = torch.tensor([best_val_acc], device=device)
                    dist.broadcast(patience_tensor, src=0)
                    dist.broadcast(best_val_acc_tensor, src=0)
                    patience_counter = int(patience_tensor.item())
                    best_val_acc = float(best_val_acc_tensor.item())
            else:
                if rank == 0:
                    print(f"Epoch {epoch}: Train Loss = {train_loss:.4f}")
            
            # æ—©åœï¼ˆæ‰€æœ‰ rank åŒæ­¥æ£€æŸ¥ï¼‰
            if patience_counter >= args.early_stop:
                if rank == 0:
                    print(f"\nâ¹ï¸ æ—©åœ! éªŒè¯å‡†ç¡®ç‡ {args.early_stop} è½®æœªæ”¹è¿›")
                break
        
        # æœ€ç»ˆæµ‹è¯•ï¼ˆæ‰€æœ‰rankå‚ä¸åˆ†å¸ƒå¼æµ‹è¯•ï¼‰
        if rank == 0:
            print("\n" + "=" * 60)
            print("ğŸ“‹ æœ€ç»ˆæµ‹è¯•è¯„ä¼°...")
        
        # æ‰€æœ‰rankåŠ è½½æœ€ä½³æ¨¡å‹
        best_ckpt = torch.load(os.path.join(save_dir, "best_model.pt"), map_location=device, weights_only=False)
        underlying_model.encoder.load_state_dict(best_ckpt["encoder_state"])
        underlying_model.projector.load_state_dict(best_ckpt["projector_state"])
        underlying_model.load_lora_state_from_checkpoint(best_ckpt, allow_missing=True)
        
        # æ¢å¤ class token çš„ embedding å’Œ lm_head æƒé‡
        if "embedding_weight" in best_ckpt:
            with torch.no_grad():
                underlying_model.llm.get_input_embeddings().weight.copy_(
                    best_ckpt["embedding_weight"].to(device)
                )
                underlying_model.llm.lm_head.weight.copy_(
                    best_ckpt["lm_head_weight"].to(device)
                )
            if rank == 0:
                print("ğŸ“¥ Loaded embedding and lm_head weights")
        
        # åŒæ­¥æ‰€æœ‰rankï¼Œç¡®ä¿éƒ½åŠ è½½å®Œæˆæƒé‡åå†å¼€å§‹æµ‹è¯•
        if world_size > 1:
            dist.barrier()
        
        # åˆ†å¸ƒå¼æµ‹è¯•è¯„ä¼°
        test_results = evaluate(
            model, test_loader, args.max_new_tokens,
            class_token_ids=class_token_ids, desc="Testing",
            rank=rank, world_size=world_size, total_samples=test_size
        )
        
        if rank == 0:
            print(f"\nâœ… æµ‹è¯•ç»“æœ:")
            print(f"   Test Loss: {test_results['loss']:.4f}")
            print(f"   Test Accuracy: {test_results['accuracy']:.4f}")
            
            # ä¿å­˜æµ‹è¯•ç»“æœ
            final_results = {
                "dataset": args.dataset,
                "pretrained_model": args.pretrained_model,
                "best_val_acc": best_val_acc,
                "test_loss": test_results["loss"],
                "test_accuracy": test_results["accuracy"],
                "epochs_trained": epoch,
            }
            
            with open(os.path.join(save_dir, "final_results.json"), "w") as f:
                json.dump(final_results, f, indent=2)
            
            # ä¿å­˜æµ‹è¯•é¢„æµ‹
            with open(os.path.join(save_dir, "test_predictions.json"), "w") as f:
                json.dump({
                    "predictions": test_results["predictions"],
                    "labels": test_results["labels"],
                }, f, indent=2)
            
            print("=" * 60)
            print(f"ç»“æœä¿å­˜åˆ°: {save_dir}")
            print("=" * 60)
    
    finally:
        # æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
        cleanup_distributed()


if __name__ == "__main__":
    main()
