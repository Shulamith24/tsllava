#!/usr/bin/env python3
"""
æµ‹è¯• TSClassifierSmallTransformer æ¨¡å‹

å¿«é€ŸéªŒè¯ï¼š
1. æ¨¡å‹å¯ä»¥æ­£ç¡®åˆå§‹åŒ–ï¼ˆsmall, medium, largeé…ç½®ï¼‰
2. forward pass å¯ä»¥è¿”å›æŸå¤±
3. predict å¯ä»¥è¿”å›é¢„æµ‹ç±»åˆ«
4. å‚æ•°é‡è®¡ç®—æ­£ç¡®
"""

import sys
from pathlib import Path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

import torch
from opentslm.model.llm.TSClassifierSmallTransformer import TSClassifierSmallTransformer
from opentslm.time_series_datasets.ucr.UCRClassificationDataset import UCRClassificationDataset
from opentslm.time_series_datasets.util import extend_time_series_to_match_patch_size_and_aggregate
from opentslm.model_config import PATCH_SIZE


def test_model_initialization():
    """æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–ï¼ˆä¸åŒé…ç½®ï¼‰"""
    print("=" * 60)
    print("æµ‹è¯• 1: æ¨¡å‹åˆå§‹åŒ–")
    print("=" * 60)
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    for config in ["small", "medium", "large"]:
        print(f"\næµ‹è¯•é…ç½®: {config}")
        
        model = TSClassifierSmallTransformer(
            num_classes=5,
            aggregator_config=config,
            device=device,
            encoder_type="transformer_cnn",
        )
        
        print(f"  âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        print(f"    - ç±»åˆ«æ•°: {model.num_classes}")
        print(f"    - Aggregator layers: {model.aggregator.num_layers}")
        print(f"    - Hidden size: {model.hidden_size}")
        print(f"    - æ€»å‚æ•°é‡: {model.count_parameters():,}")
    
    return model, device


def test_forward_pass(device):
    """æµ‹è¯•å‰å‘ä¼ æ’­"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 2: å‰å‘ä¼ æ’­")
    print("=" * 60)
    
    batch = [
        {
            "time_series": [torch.randn(100, device=device)],
            "int_label": 0,
        },
        {
            "time_series": [torch.randn(120, device=device)],
            "int_label": 2,
        },
    ]
    
    model = TSClassifierSmallTransformer(
        num_classes=5,
        aggregator_config="medium",
        device=device,
        encoder_type="transformer_cnn",
    )
    
    model.train()
    loss = model(batch)
    
    print(f"  âœ“ å‰å‘ä¼ æ’­æˆåŠŸ")
    print(f"    - æŸå¤±å€¼: {loss.item():.4f}")
    print(f"    - æŸå¤±æ˜¯å¦ä¸ºæ ‡é‡: {loss.dim() == 0}")
    print(f"    - æŸå¤± requires_grad: {loss.requires_grad}")


def test_prediction(device):
    """æµ‹è¯•é¢„æµ‹"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 3: é¢„æµ‹")
    print("=" * 60)
    
    batch = [
        {
            "time_series": [torch.randn(100, device=device)],
            "int_label": 1,
        },
        {
            "time_series": [torch.randn(150, device=device)],
            "int_label": 3,
        },
        {
            "time_series": [torch.randn(80, device=device)],
            "int_label": 4,
        },
    ]
    
    model = TSClassifierSmallTransformer(
        num_classes=5,
        aggregator_config="medium",
        device=device,
        encoder_type="transformer_cnn",
    )
    
    model.eval()
    with torch.no_grad():
        predictions = model.predict(batch)
    
    print(f"  âœ“ é¢„æµ‹æˆåŠŸ")
    print(f"    - é¢„æµ‹ shape: {predictions.shape}")
    print(f"    - é¢„æµ‹å€¼: {predictions.tolist()}")
    print(f"    - çœŸå®æ ‡ç­¾: {[b['int_label'] for b in batch]}")
    
    assert all(0 <= p < model.num_classes for p in predictions.tolist()), "é¢„æµ‹å€¼è¶…å‡ºèŒƒå›´"
    print(f"    - æ‰€æœ‰é¢„æµ‹åœ¨æœ‰æ•ˆèŒƒå›´ [0, {model.num_classes-1}]")


def test_custom_config(device):
    """æµ‹è¯•è‡ªå®šä¹‰é…ç½®"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 4: è‡ªå®šä¹‰é…ç½®")
    print("=" * 60)
    
    # è‡ªå®šä¹‰ï¼š4å±‚ 512ç»´
    model = TSClassifierSmallTransformer(
        num_classes=10,
        aggregator_config="small",  # åŸºç¡€é…ç½®
        num_layers=4,  # è¦†ç›–å±‚æ•°
        hidden_size=512,  # è¦†ç›–ç»´åº¦
        device=device,
        encoder_type="transformer_cnn",
    )
    
    print(f"  âœ“ è‡ªå®šä¹‰é…ç½®æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    print(f"    - Layers: {model.aggregator.num_layers}")
    print(f"    - Hidden: {model.hidden_size}")
    print(f"    - æ€»å‚æ•°é‡: {model.count_parameters():,}")
    
    # æµ‹è¯•å‰å‘
    batch = [{"time_series": [torch.randn(100, device=device)], "int_label": 5}]
    loss = model(batch)
    print(f"    - æŸå¤±: {loss.item():.4f}")


def test_with_real_dataset():
    """æµ‹è¯•ä½¿ç”¨çœŸå® UCR æ•°æ®é›†"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 5: ä½¿ç”¨çœŸå® UCR æ•°æ®é›†")
    print("=" * 60)
    
    try:
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # åˆ›å»ºæ•°æ®é›†
        print("åŠ è½½ ECG200 æ•°æ®é›†...")
        dataset = UCRClassificationDataset(
            split="train",
            EOS_TOKEN="<eos>",
            dataset_name="ECG200",
            raw_data_path="./data",
        )
        
        num_classes = UCRClassificationDataset.get_num_classes()
        print(f"âœ“ æ•°æ®é›†åŠ è½½æˆåŠŸ")
        print(f"  - æ•°æ®é›†å¤§å°: {len(dataset)}")
        print(f"  - ç±»åˆ«æ•°: {num_classes}")
        
        # åˆ›å»ºæ¨¡å‹
        model = TSClassifierSmallTransformer(
            num_classes=num_classes,
            aggregator_config="medium",
            device=device,
            encoder_type="transformer_cnn",
        )
        print(f"  âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # è·å–æ‰¹æ¬¡
        samples = [dataset[i] for i in range(min(2, len(dataset)))]
        batch = extend_time_series_to_match_patch_size_and_aggregate(samples, patch_size=PATCH_SIZE)
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        model.train()
        loss = model(batch)
        print(f"  âœ“ å‰å‘ä¼ æ’­æˆåŠŸï¼ŒæŸå¤±: {loss.item():.4f}")
        
        # æµ‹è¯•é¢„æµ‹
        model.eval()
        with torch.no_grad():
            predictions = model.predict(batch)
        
        print(f"  âœ“ é¢„æµ‹æˆåŠŸ")
        for i, (pred, sample) in enumerate(zip(predictions.tolist(), batch)):
            print(f"    æ ·æœ¬ {i}: é¢„æµ‹={pred}, çœŸå®={sample['int_label']}")
        
        print("\nâœ… æ‰€æœ‰çœŸå®æ•°æ®é›†æµ‹è¯•é€šè¿‡!")
        
    except Exception as e:
        print(f"âš ï¸ çœŸå®æ•°æ®é›†æµ‹è¯•è·³è¿‡ï¼ˆéœ€è¦ä¸‹è½½æ•°æ®ï¼‰: {e}")


def main():
    print("\n" + "ğŸ§ª" * 30)
    print("TSClassifierSmallTransformer å•å…ƒæµ‹è¯•")
    print("ğŸ§ª" * 30 + "\n")
    
    try:
        # æµ‹è¯• 1: åˆå§‹åŒ–
        model, device = test_model_initialization()
        
        # æµ‹è¯• 2: å‰å‘ä¼ æ’­
        test_forward_pass(device)
        
        # æµ‹è¯• 3: é¢„æµ‹
        test_prediction(device)
        
        # æµ‹è¯• 4: è‡ªå®šä¹‰é…ç½®
        test_custom_config(device)
        
        # æµ‹è¯• 5: çœŸå®æ•°æ®é›†
        test_with_real_dataset()
        
        print("\n" + "=" * 60)
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
        print("=" * 60)
        print("\nä¸‹ä¸€æ­¥: è¿è¡Œå®Œæ•´è®­ç»ƒè„šæœ¬")
        print("  # medium é…ç½®ï¼ˆæ¨èï¼‰")
        print("  python scripts/train_ucr_classification_experiment_d.py \\")
        print("      --dataset Adiac \\")
        print("      --aggregator_config medium \\")
        print("      --epochs 50 \\")
        print("      --batch_size 16")
        print()
        print("  # è‡ªå®šä¹‰é…ç½®")
        print("  python scripts/train_ucr_classification_experiment_d.py \\")
        print("      --dataset Adiac \\")
        print("      --num_layers 4 \\")
        print("      --hidden_size 512 \\")
        print("      --epochs 50 \\")
        print("      --batch_size 16")
        
    except Exception as e:
        print("\n" + "=" * 60)
        print("âŒ æµ‹è¯•å¤±è´¥!")
        print("=" * 60)
        print(f"é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
