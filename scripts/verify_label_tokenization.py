"""
éªŒè¯Excelé£æ ¼æ ‡ç­¾ï¼ˆA, B, ..., Z, AA, AB, ...ï¼‰çš„tokenization

æ£€æŸ¥è¿™äº›æ ‡ç­¾æ˜¯å¦è¢«tokenizerç¼–ç ä¸ºå•ä¸ªtokenï¼Œ
è¿™å¯¹äºlogitsåˆ†ç±»æ¨ç†çš„æ­£ç¡®æ€§è‡³å…³é‡è¦ã€‚
"""

import argparse
from transformers import AutoTokenizer


def index_to_excel_label(index: int) -> str:
    """å°†æ•´æ•°ç´¢å¼•è½¬æ¢ä¸ºç±»ä¼¼Excelåˆ—åçš„å­—æ¯æ ‡ç­¾ã€‚"""
    if index < 0:
        raise ValueError(f"Index must be non-negative, got {index}")
    
    if index < 26:
        return chr(ord('A') + index)
    else:
        adjusted = index - 26
        prefix_idx = adjusted // 26
        suffix_idx = adjusted % 26
        return chr(ord('A') + prefix_idx) + chr(ord('A') + suffix_idx)


def verify_labels_tokenization(tokenizer, max_labels: int = 100):
    """
    éªŒè¯æ ‡ç­¾çš„tokenizationã€‚
    
    Args:
        tokenizer: HuggingFace tokenizer
        max_labels: è¦éªŒè¯çš„æœ€å¤§æ ‡ç­¾æ•°é‡
    
    Returns:
        tuple: (å•tokenæ ‡ç­¾åˆ—è¡¨, å¤štokenæ ‡ç­¾åˆ—è¡¨)
    """
    single_token_labels = []
    multi_token_labels = []
    
    print(f"\n{'='*60}")
    print(f"éªŒè¯å‰ {max_labels} ä¸ªExcelé£æ ¼æ ‡ç­¾çš„tokenization")
    print(f"Tokenizer: {tokenizer.name_or_path}")
    print(f"{'='*60}\n")
    
    for i in range(max_labels):
        label = index_to_excel_label(i)
        
        # ä½¿ç”¨ add_special_tokens=False é¿å…æ·»åŠ  BOS/EOS
        token_ids = tokenizer.encode(label, add_special_tokens=False)
        num_tokens = len(token_ids)
        
        if num_tokens == 1:
            single_token_labels.append((i, label, token_ids[0]))
        else:
            multi_token_labels.append((i, label, token_ids))
    
    # æ‰“å°ç»“æœ
    print("âœ… å•tokenæ ‡ç­¾:")
    print("-" * 40)
    for idx, label, token_id in single_token_labels:
        decoded = tokenizer.decode([token_id])
        print(f"  {idx:3d} -> {label:4s} -> token_id={token_id:5d} -> decoded='{decoded}'")
    
    if multi_token_labels:
        print(f"\nâŒ å¤štokenæ ‡ç­¾ (å…± {len(multi_token_labels)} ä¸ª):")
        print("-" * 40)
        for idx, label, token_ids in multi_token_labels:
            tokens = [tokenizer.decode([tid]) for tid in token_ids]
            print(f"  {idx:3d} -> {label:4s} -> token_ids={token_ids} -> tokens={tokens}")
    
    # ç»Ÿè®¡
    print(f"\n{'='*60}")
    print(f"ç»Ÿè®¡ç»“æœ:")
    print(f"  - å•tokenæ ‡ç­¾æ•°: {len(single_token_labels)}")
    print(f"  - å¤štokenæ ‡ç­¾æ•°: {len(multi_token_labels)}")
    print(f"  - å•tokenæ¯”ä¾‹: {100*len(single_token_labels)/max_labels:.1f}%")
    print(f"{'='*60}")
    
    if multi_token_labels:
        print("\nâš ï¸  è­¦å‘Š: å­˜åœ¨è¢«ç¼–ç ä¸ºå¤štokençš„æ ‡ç­¾!")
        print("   è¿™å¯èƒ½ä¼šå½±å“logitsåˆ†ç±»çš„å‡†ç¡®æ€§ã€‚")
        print("   å»ºè®®:")
        print("   1. åªä½¿ç”¨å•tokenæ ‡ç­¾è¿›è¡Œåˆ†ç±»")
        print("   2. æˆ–è€…å°†è¿™äº›æ ‡ç­¾ä½œä¸ºç‰¹æ®Štokenæ·»åŠ åˆ°tokenizerä¸­")
    else:
        print("\nğŸ‰ æ‰€æœ‰æ ‡ç­¾éƒ½æ˜¯å•tokenï¼Œå¯ä»¥å®‰å…¨ä½¿ç”¨!")
    
    return single_token_labels, multi_token_labels


def main():
    parser = argparse.ArgumentParser(description="éªŒè¯Excelé£æ ¼æ ‡ç­¾çš„tokenization")
    parser.add_argument(
        "--model_id", 
        type=str, 
        default="meta-llama/Llama-3.2-1B",
        help="HuggingFaceæ¨¡å‹ID"
    )
    parser.add_argument(
        "--max_labels", 
        type=int, 
        default=100,
        help="è¦éªŒè¯çš„æœ€å¤§æ ‡ç­¾æ•°é‡ (é»˜è®¤100ï¼Œè¦†ç›–A-Zå’ŒAA-BV)"
    )
    args = parser.parse_args()
    
    print(f"åŠ è½½tokenizer: {args.model_id}")
    tokenizer = AutoTokenizer.from_pretrained(args.model_id, use_fast=True)
    
    single, multi = verify_labels_tokenization(tokenizer, args.max_labels)
    
    # é¢å¤–æµ‹è¯•ï¼šéªŒè¯ä¸€äº›ç‰¹å®šçš„è¾¹ç•Œæƒ…å†µ
    print("\n" + "="*60)
    print("è¾¹ç•Œæƒ…å†µæµ‹è¯•:")
    print("="*60)
    
    test_labels = ["A", "Z", "AA", "AZ", "BA", "ZZ"]
    for label in test_labels:
        token_ids = tokenizer.encode(label, add_special_tokens=False)
        print(f"  '{label}' -> {token_ids} (num_tokens={len(token_ids)})")


if __name__ == "__main__":
    main()
