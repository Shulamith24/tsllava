#!/usr/bin/env python3
# SPDX-FileCopyrightText: 2025 Stanford University, ETH Zurich, and the project authors (see CONTRIBUTORS.md)
# SPDX-FileCopyrightText: 2025 This source file is part of the OpenTSLM open-source project.
#
# SPDX-License-Identifier: MIT

"""
TSLANetæ©ç é¢„è®­ç»ƒè„šæœ¬

åœ¨UCRæ•°æ®é›†ä¸Šåªè¿›è¡Œè‡ªç›‘ç£æ©ç é¢„è®­ç»ƒï¼ˆä¸ä½¿ç”¨æ ‡ç­¾ï¼‰ï¼Œç”¨äºICLåˆ†ç±»çš„ç›¸ä¼¼æ ·æœ¬æ£€ç´¢ã€‚

ä¸åˆ†ç±»è®­ç»ƒä¸åŒï¼Œæ©ç é¢„è®­ç»ƒåªå­¦ä¹ æ—¶é—´åºåˆ—çš„è¡¨å¾ï¼Œä¸æ¶‰åŠç±»åˆ«ä¿¡æ¯ï¼Œ
å› æ­¤åœ¨ICLåœºæ™¯ä¸­ä¸å­˜åœ¨ä¿¡æ¯æ³„éœ²é—®é¢˜ã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/pretrain_tslanet_ucr.py \\
        --dataset ECG5000 \\
        --epochs 100 \\
        --mask_ratio 0.4

è®­ç»ƒæµç¨‹ï¼š
1. åŠ è½½UCRæ•°æ®é›†ï¼ˆåªä½¿ç”¨æ—¶é—´åºåˆ—ï¼Œå¿½ç•¥æ ‡ç­¾ï¼‰
2. ä½¿ç”¨æ©ç é‡å»ºä»»åŠ¡è¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒ
3. ä¿å­˜encoder checkpointç”¨äºåç»­æ£€ç´¢
"""

import os
import sys
import json
import argparse
import datetime
from pathlib import Path
from typing import List, Dict, Any

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import AdamW
from torch.utils.data import DataLoader, Dataset
from tqdm.auto import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from opentslm.model.encoder.TSLANetEncoder import TSLANetEncoder
from opentslm.time_series_datasets.ucr.ucr_loader import (
    load_ucr_dataset, 
    ensure_ucr_data,
    UCRDataset,
    collate_fn
)


def parse_args():
    parser = argparse.ArgumentParser(description="TSLANetæ©ç é¢„è®­ç»ƒï¼ˆè‡ªç›‘ç£ï¼‰")

    # æ•°æ®ç›¸å…³
    parser.add_argument("--dataset", type=str, default="ECG5000", help="UCRæ•°æ®é›†åç§°")
    parser.add_argument("--data_path", type=str, default="./data", help="UCRæ•°æ®æ ¹ç›®å½•")
    
    # æ¨¡å‹ç›¸å…³
    parser.add_argument("--emb_dim", type=int, default=128, help="åµŒå…¥ç»´åº¦")
    parser.add_argument("--depth", type=int, default=2, help="TSLANetå±‚æ•°")
    parser.add_argument("--patch_size", type=int, default=8, help="Patchå¤§å°")
    parser.add_argument("--dropout", type=float, default=0.15, help="Dropoutæ¯”ä¾‹")
    
    # é¢„è®­ç»ƒç›¸å…³
    parser.add_argument("--epochs", type=int, default=200, help="é¢„è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=16, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--lr", type=float, default=1e-3, help="å­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=1e-4, help="æƒé‡è¡°å‡")
    parser.add_argument("--mask_ratio", type=float, default=0.4, help="æ©ç æ¯”ä¾‹")
    
    # ä¿å­˜ç›¸å…³
    parser.add_argument("--save_dir", type=str, default="results/tslanet_pretrain", help="ç»“æœä¿å­˜ç›®å½•")
    
    # å…¶ä»–
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--device", type=str, default="cuda", help="è®¾å¤‡")
    parser.add_argument("--val_ratio", type=float, default=0.1, help="éªŒè¯é›†æ¯”ä¾‹(ä»è®­ç»ƒé›†åˆ’åˆ†)")
    parser.add_argument("--patience", type=int, default=20, help="æ—©åœè€å¿ƒå€¼")
    
    return parser.parse_args()


def set_seed(seed: int):
    """è®¾ç½®éšæœºç§å­"""
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def create_data_loaders(args):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    ensure_ucr_data()
    
    # åŠ è½½æ•°æ®
    train_df, test_df = load_ucr_dataset(args.dataset, raw_data_path=args.data_path)
    
    # è·å–æ•°æ®é›†ä¿¡æ¯
    all_labels = sorted(train_df["label"].unique().tolist())
    num_classes = len(all_labels)
    seq_len = train_df.shape[1] - 1  # å‡å»labelåˆ—
    
    # æ ‡ç­¾é‡æ˜ å°„åˆ°0-indexed (è™½ç„¶é¢„è®­ç»ƒä¸éœ€è¦æ ‡ç­¾ï¼Œä½†ä¿ç•™ä»¥ä¾¿åç»­ä½¿ç”¨)
    label_to_idx = {label: idx for idx, label in enumerate(all_labels)}
    train_df["label"] = train_df["label"].map(label_to_idx)
    test_df["label"] = test_df["label"].map(label_to_idx)
    
    # ä»è®­ç»ƒé›†åˆ’åˆ†éªŒè¯é›†
    train_df = train_df.sample(frac=1, random_state=args.seed).reset_index(drop=True)
    val_size = int(len(train_df) * args.val_ratio)
    
    if val_size > 0:
        val_df = train_df.iloc[:val_size]
        train_df = train_df.iloc[val_size:]
    else:
        val_df = test_df.copy()
    
    print(f"ğŸ“Š Dataset: {args.dataset}")
    print(f"   Classes: {num_classes} (ä¸ç”¨äºé¢„è®­ç»ƒ)")
    print(f"   Sequence length: {seq_len}")
    print(f"   Train samples: {len(train_df)}")
    print(f"   Val samples: {len(val_df)}")
    print(f"   Test samples: {len(test_df)}")
    
    # åˆ›å»ºDataset
    train_dataset = UCRDataset(train_df)
    val_dataset = UCRDataset(val_df)
    test_dataset = UCRDataset(test_df)
    
    # åˆ›å»ºDataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=min(args.batch_size, len(train_dataset)),
        shuffle=True,
        collate_fn=collate_fn,
        drop_last=len(train_dataset) > args.batch_size
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        collate_fn=collate_fn
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        collate_fn=collate_fn
    )
    
    return train_loader, val_loader, test_loader, num_classes, seq_len, label_to_idx


def pretrain_one_epoch(
    encoder: TSLANetEncoder,
    train_loader: DataLoader,
    optimizer,
    mask_ratio: float,
    epoch: int,
    num_epochs: int,
    device: str
) -> float:
    """é¢„è®­ç»ƒä¸€ä¸ªepoch"""
    encoder.train()
    total_loss = 0.0
    num_batches = 0
    
    pbar = tqdm(train_loader, desc=f"Pretrain Epoch {epoch}/{num_epochs}")
    for batch in pbar:
        features, _ = batch  # å¿½ç•¥æ ‡ç­¾
        features = features.to(device)  # [B, L]
        
        # æ©ç é¢„è®­ç»ƒ
        preds, target, mask = encoder.pretrain_forward(features, mask_ratio=mask_ratio)
        
        # è®¡ç®—æ©ç ä½ç½®çš„MSEæŸå¤±
        loss = (preds - target) ** 2
        loss = loss.mean(dim=-1)  # [B, N]
        loss = (loss * mask.float()).sum() / mask.float().sum()
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        num_batches += 1
        pbar.set_postfix({"loss": f"{loss.item():.4f}"})
    
    return total_loss / max(num_batches, 1)


@torch.no_grad()
def evaluate_pretrain(
    encoder: TSLANetEncoder,
    data_loader: DataLoader,
    mask_ratio: float,
    device: str,
    desc: str = "Evaluating"
) -> float:
    """è¯„ä¼°é¢„è®­ç»ƒæ¨¡å‹"""
    encoder.eval()
    total_loss = 0.0
    num_batches = 0
    
    for batch in tqdm(data_loader, desc=desc):
        features, _ = batch
        features = features.to(device)
        
        preds, target, mask = encoder.pretrain_forward(features, mask_ratio=mask_ratio)
        
        loss = (preds - target) ** 2
        loss = loss.mean(dim=-1)
        loss = (loss * mask.float()).sum() / mask.float().sum()
        
        total_loss += loss.item()
        num_batches += 1
    
    return total_loss / max(num_batches, 1)


def main():
    args = parse_args()
    
    print("=" * 60)
    print("TSLANetæ©ç é¢„è®­ç»ƒï¼ˆè‡ªç›‘ç£ï¼‰")
    print("=" * 60)
    print(f"æ—¶é—´: {datetime.datetime.now()}")
    print(f"æ•°æ®é›†: {args.dataset}")
    print(f"æ©ç æ¯”ä¾‹: {args.mask_ratio}")
    print("=" * 60)
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    
    # è®¾ç½®è®¾å¤‡
    if args.device == "cuda" and torch.cuda.is_available():
        device = "cuda"
    else:
        print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        device = "cpu"
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = os.path.join(args.save_dir, args.dataset)
    os.makedirs(save_dir, exist_ok=True)
    
    # ä¿å­˜é…ç½®
    with open(os.path.join(save_dir, "config.json"), "w") as f:
        json.dump(vars(args), f, indent=2)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("\nğŸ“‚ åŠ è½½æ•°æ®...")
    train_loader, val_loader, test_loader, num_classes, seq_len, label_to_idx = create_data_loaders(args)
    
    # è®¡ç®—éœ€è¦çš„patchæ•°é‡å’Œåºåˆ—é•¿åº¦
    padded_seq_len = seq_len
    if seq_len % args.patch_size != 0:
        padded_seq_len = seq_len + (args.patch_size - seq_len % args.patch_size)
    max_seq_len = max(padded_seq_len, 512)
    
    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ”§ åˆ›å»ºTSLANet Encoder...")
    encoder = TSLANetEncoder(
        output_dim=args.emb_dim,
        dropout=args.dropout,
        patch_size=args.patch_size,
        emb_dim=args.emb_dim,
        depth=args.depth,
        max_seq_len=max_seq_len
    ).to(device)
    
    print(f"   Encoder params: {sum(p.numel() for p in encoder.parameters()):,}")
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = AdamW(encoder.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    
    # é¢„è®­ç»ƒ
    print("\nğŸš€ å¼€å§‹æ©ç é¢„è®­ç»ƒ...")
    best_val_loss = float('inf')
    patience_counter = 0
    loss_history = []
    
    for epoch in range(1, args.epochs + 1):
        # è®­ç»ƒ
        train_loss = pretrain_one_epoch(
            encoder, train_loader, optimizer,
            args.mask_ratio, epoch, args.epochs, device
        )
        
        # éªŒè¯
        val_loss = evaluate_pretrain(encoder, val_loader, args.mask_ratio, device, "Validating")
        
        print(f"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}")
        
        # è®°å½•å†å²
        loss_history.append({
            "epoch": epoch,
            "train_loss": train_loss,
            "val_loss": val_loss
        })
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            
            # ä¿å­˜checkpoint (åªä¿å­˜encoderï¼Œä¸å«åˆ†ç±»å¤´)
            checkpoint = {
                "encoder_state": encoder.state_dict(),
                "epoch": epoch,
                "val_loss": best_val_loss,
                "num_classes": num_classes,
                "seq_len": seq_len,
                "max_seq_len": max_seq_len,
                "label_to_idx": label_to_idx,
                "config": vars(args),
                "pretrain_only": True  # æ ‡è®°ä¸ºçº¯é¢„è®­ç»ƒæ¨¡å‹
            }
            torch.save(checkpoint, os.path.join(save_dir, "best_model.pt"))
            print(f"ğŸ’¾ Saved best model (val_loss={best_val_loss:.4f})")
        else:
            patience_counter += 1
            if patience_counter >= args.patience:
                print(f"\nâ¹ï¸ æ—©åœ! éªŒè¯æŸå¤± {args.patience} è½®æœªæ”¹è¿›")
                break
    
    # ä¿å­˜è®­ç»ƒå†å²
    with open(os.path.join(save_dir, "loss_history.json"), "w") as f:
        json.dump(loss_history, f, indent=2)
    
    # æœ€ç»ˆæµ‹è¯•
    print("\n" + "=" * 60)
    print("ğŸ“‹ æœ€ç»ˆæµ‹è¯•è¯„ä¼°...")
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    best_ckpt = torch.load(os.path.join(save_dir, "best_model.pt"), map_location=device, weights_only=False)
    encoder.load_state_dict(best_ckpt["encoder_state"])
    
    test_loss = evaluate_pretrain(encoder, test_loader, args.mask_ratio, device, "Testing")
    
    print(f"\nâœ… æµ‹è¯•ç»“æœ:")
    print(f"   Test Loss (é‡å»ºè¯¯å·®): {test_loss:.4f}")
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    final_results = {
        "dataset": args.dataset,
        "best_val_loss": best_val_loss,
        "test_loss": test_loss,
        "epochs_trained": epoch,
        "num_classes": num_classes,
        "seq_len": seq_len,
        "mask_ratio": args.mask_ratio,
        "pretrain_only": True
    }
    
    with open(os.path.join(save_dir, "final_results.json"), "w") as f:
        json.dump(final_results, f, indent=2)
    
    print("=" * 60)
    print(f"ç»“æœä¿å­˜åˆ°: {save_dir}")
    print("=" * 60)


if __name__ == "__main__":
    main()
