#!/usr/bin/env python3
# SPDX-FileCopyrightText: 2025 Stanford University, ETH Zurich, and the project authors (see CONTRIBUTORS.md)
# SPDX-FileCopyrightText: 2025 This source file is part of the OpenTSLM open-source project.
#
# SPDX-License-Identifier: MIT

"""
TSLANet UEAå¤šå˜é‡æ•°æ®é›†é¢„è®­ç»ƒè„šæœ¬ (ä¿®å¤ç‰ˆ)

æ ¸å¿ƒæœºåˆ¶ï¼šChannel Independence (CI)
æˆ‘ä»¬å°†æ‰€æœ‰å˜é‡(Channels)è§†ä¸ºç‹¬ç«‹çš„å•å˜é‡åºåˆ—è¿›è¡Œé¢„è®­ç»ƒã€‚
è¾“å…¥å½¢çŠ¶å˜æ¢: [Batch, Channel, Length] -> [Batch * Channel, Length]

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/pretrain_tslanet_uea.py --dataset Handwriting
    python scripts/pretrain_tslanet_uea.py --dataset_list src/opentslm/time_series_datasets/uea/uea_pretrain_datasets.txt
"""

import os
import sys
import json
import argparse
import datetime
from pathlib import Path

import numpy as np
import torch
import torch.optim as optim
from torch.utils.data import dataset
from tqdm.auto import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))
dataset_list_path = (project_root / "data" / "Multivariate_ts" / "uea_datasets.txt").resolve()
uea_path = str(dataset_list_path.parent)

from opentslm import data
from opentslm.model.encoder.TSLANetEncoder import TSLANetEncoder
# å¤ç”¨åŠ è½½å™¨é€»è¾‘
from opentslm.time_series_datasets.uea.uea_pretrain_loader import (
    get_uea_pretrain_loader, 
    UEAPretrainDataset,
    collate_fn_pretrain,
)
from aeon.datasets import load_classification

def parse_args():
    parser = argparse.ArgumentParser(description="TSLANet UEAé¢„è®­ç»ƒ")
    
    # æ•°æ®ç›¸å…³
    parser.add_argument("--dataset", type=str, default=None, help="å•ä¸ªUEAæ•°æ®é›†åç§°")
    parser.add_argument("--dataset_list", type=str, default=str(dataset_list_path), help="æ•°æ®é›†åˆ—è¡¨æ–‡ä»¶")
    parser.add_argument("--save_path", type=str, default="pretrained/tslanet_uea.pt", help="ä¿å­˜è·¯å¾„")
    
    # è®­ç»ƒé…ç½®
    parser.add_argument("--epochs", type=int, default=50)
    parser.add_argument("--batch_size", type=int, default=16)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--weight_decay", type=float, default=1e-4)
    parser.add_argument("--mask_ratio", type=float, default=0.4)
    
    # æ¨¡å‹ç»“æ„
    parser.add_argument("--patch_size", type=int, default=4)
    parser.add_argument("--emb_dim", type=int, default=128)
    parser.add_argument("--depth", type=int, default=2)
    parser.add_argument("--dropout", type=float, default=0.15)
    
    # ç³»ç»Ÿ
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--device", type=str, default="cuda")
    parser.add_argument("--num_workers", type=int, default=0)
    parser.add_argument("--early_stop", type=int, default=10)
    parser.add_argument("--val_ratio", type=float, default=0.1, help="éªŒè¯é›†æ¯”ä¾‹(ä»…å½“å•æ•°æ®é›†æ¨¡å¼æœ‰æ•ˆ)")
    
    # åŠ¨æ€é‡‡æ ·å‚æ•°ï¼ˆè§£å†³OOMé—®é¢˜ï¼‰
    parser.add_argument("--max_channels", type=int, default=32, help="æœ€å¤§é€šé“æ•°ï¼Œè¶…è¿‡åˆ™éšæœºé‡‡æ ·")
    parser.add_argument("--max_length", type=int, default=512, help="æœ€å¤§åºåˆ—é•¿åº¦ï¼Œè¶…è¿‡åˆ™éšæœºè£å‰ª")
    parser.add_argument("--skip_variable_length", action="store_true", help="è·³è¿‡å˜é•¿æ•°æ®é›†")
    
    return parser.parse_args()

def set_seed(seed: int):
    torch.manual_seed(seed)
    np.random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

def train_one_epoch(model, train_loader, optimizer, mask_ratio, device):
    model.train()
    total_loss = 0.0
    num_batches = 0
    
    pbar = tqdm(train_loader, desc="Training", leave=False)
    for batch in pbar:
        # batch: [B, C, L]
        # Channel Independence: Flatten to [B*C, L]
        # å±•å¹³æ‰€æœ‰é€šé“ï¼Œè§†ä¸ºç‹¬ç«‹çš„å•å˜é‡æ ·æœ¬
        B, C, L = batch.shape
        batch = batch.view(B * C, L).to(device)
        
        # é¢„è®­ç»ƒå‰å‘ä¼ æ’­
        # preds: [B*C, N, D], target: [B*C, N, D], mask: [B*C, N]
        preds, target, mask = model.pretrain_forward(batch, mask_ratio=mask_ratio)
        
        # è®¡ç®—MSEæŸå¤± (åªåœ¨æ©ç ä½ç½®)
        loss = (preds - target) ** 2
        loss = loss.mean(dim=-1)
        loss = (loss * mask).sum() / (mask.sum() + 1e-8)
        
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        total_loss += loss.item()
        num_batches += 1
        pbar.set_postfix({"loss": f"{loss.item():.4f}"})
    
    return total_loss / max(num_batches, 1)

@torch.no_grad()
def validate(model, val_loader, mask_ratio, device):
    model.eval()
    total_loss = 0.0
    num_batches = 0
    
    for batch in tqdm(val_loader, desc="Validating", leave=False):
        B, C, L = batch.shape
        batch = batch.view(B * C, L).to(device)
        
        preds, target, mask = model.pretrain_forward(batch, mask_ratio=mask_ratio)
        
        loss = (preds - target) ** 2
        loss = loss.mean(dim=-1)
        loss = (loss * mask).sum() / (mask.sum() + 1e-8)
        
        total_loss += loss.item()
        num_batches += 1
        
    return total_loss / max(num_batches, 1)

def main():
    args = parse_args()
    set_seed(args.seed)
    
    print("=" * 60)
    print("TSLANet UEAå¤šå˜é‡æ•°æ®é›†é¢„è®­ç»ƒ (Channel Independence)")
    print("=" * 60)
    print(f"æ—¶é—´: {datetime.datetime.now()}")
    print(f"å‚æ•°: {args}")
    
    if args.device == "cuda" and not torch.cuda.is_available():
        print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        args.device = "cpu"
    device = args.device
    
    # 1. å‡†å¤‡æ•°æ® Loader
    if args.dataset:
        # å•æ•°æ®é›†æ¨¡å¼
        print(f"ğŸ“‚ Loading Single Dataset: {args.dataset}")
        X_train, _ = load_classification(args.dataset, split="train", extract_path=uea_path) # [N, C, L]
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä»…ä½¿ç”¨ train split è¿›è¡Œé¢„è®­ç»ƒï¼Œ
        # å¹¶ä»ä¸­åˆ’åˆ†å‡ºä¸€éƒ¨åˆ†ä½œä¸º valid ç›‘æ§ loss å˜åŒ–
        
        val_size = int(len(X_train) * args.val_ratio)
        if val_size < 1: val_size = 1
        
        indices = np.random.permutation(len(X_train))
        X_val = X_train[indices[:val_size]]
        X_train = X_train[indices[val_size:]]
        
        print(f"   Train samples: {len(X_train)}")
        print(f"   Val samples:   {len(X_val)}")
        print(f"   Channels:      {X_train.shape[1]}")
        
        train_dataset = UEAPretrainDataset(X_train, max_channels=args.max_channels, max_length=args.max_length)
        val_dataset = UEAPretrainDataset(X_val, max_channels=args.max_channels, max_length=args.max_length)
        
        train_loader = torch.utils.data.DataLoader(
            train_dataset, batch_size=args.batch_size, shuffle=True, 
            num_workers=args.num_workers, 
            collate_fn=lambda x: collate_fn_pretrain(x, args.patch_size)
        )
        val_loader = torch.utils.data.DataLoader(
            val_dataset, batch_size=args.batch_size, shuffle=False, 
            num_workers=args.num_workers, 
            collate_fn=lambda x: collate_fn_pretrain(x, args.patch_size)
        )
        
    elif args.dataset_list:
        # å¤šæ•°æ®é›†æ¨¡å¼
        print(f"ğŸ“‚ Loading Multi Datasets list: {args.dataset_list}")
        # ä½¿ç”¨ split='train'ã€‚å¦‚æœæ˜¯å¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œé€šå¸¸ä¸ä¸“é—¨åˆ’åˆ† validï¼Œ
        # æˆ–è€…ç›´æ¥ç”¨ loader çš„ä¸€éƒ¨åˆ†æ•°æ®ã€‚
        # ä¸ºç®€åŒ–ï¼Œè¿™é‡Œæˆ‘ä»¬å°† train_loader è§†ä¸º val_loader (ä»…ç”¨äºæ‰“å° loss è¶‹åŠ¿)
        # å®é™…ç”Ÿäº§ä¸­å»ºè®®ä¸“é—¨ç•™å‡ºéªŒè¯æ•°æ®é›†
        train_loader = get_uea_pretrain_loader(
            args.dataset_list, 
            extract_path=uea_path,
            batch_size=args.batch_size, 
            patch_size=args.patch_size, 
            split="train", 
            num_workers=args.num_workers,
            max_channels=args.max_channels,
            max_length=args.max_length,
            skip_variable_length=args.skip_variable_length,
        )
        val_loader = train_loader 
    else:
        raise ValueError("Must specify --dataset or --dataset_list")

    # 2. åˆ›å»ºæ¨¡å‹
    print("\nğŸ”§ Creating TSLANetEncoder...")
    # å…³é”®ä¿®æ”¹ï¼šç§»é™¤ num_channels å‚æ•°
    # TSLANet (CIç­–ç•¥) åªæ¥å—å•é€šé“è¾“å…¥ï¼Œæˆ‘ä»¬é€šè¿‡ Reshape å°†æ‰€æœ‰é€šé“å †å åˆ° Batch ç»´
    model = TSLANetEncoder(
        patch_size=args.patch_size,
        emb_dim=args.emb_dim,
        depth=args.depth,
        dropout=args.dropout,
        # num_channels=... (REMOVED)
    ).to(device)
    
    print(f"   Model Params: {sum(p.numel() for p in model.parameters()):,}")

    optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)
    
    # 3. è®­ç»ƒå¾ªç¯
    save_dir = os.path.dirname(args.save_path)
    if save_dir:
        os.makedirs(save_dir, exist_ok=True)
        
    best_loss = float("inf")
    patience_counter = 0
    loss_history = []
    
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    for epoch in range(1, args.epochs + 1):
        train_loss = train_one_epoch(model, train_loader, optimizer, args.mask_ratio, device)
        val_loss = validate(model, val_loader, args.mask_ratio, device)
        
        scheduler.step()
        
        print(f"Epoch {epoch}/{args.epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | LR: {scheduler.get_last_lr()[0]:.2e}")
        
        loss_history.append({
            "epoch": epoch,
            "train_loss": train_loss,
            "val_loss": val_loss,
        })
        
        if val_loss < best_loss:
            best_loss = val_loss
            patience_counter = 0
            torch.save({
                "model_state": model.state_dict(),
                "epoch": epoch,
                "val_loss": val_loss,
                "args": vars(args)
            }, args.save_path)
            print(f"   ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {args.save_path}")
        else:
            patience_counter += 1
            if patience_counter >= args.early_stop:
                print(f"\nâ¹ï¸ æ—©åœ! éªŒè¯æŸå¤± {args.early_stop} è½®æœªæ”¹è¿›")
                break
                
    # ä¿å­˜å†å²
    history_path = str(args.save_path).replace(".pt", "_history.json")
    with open(history_path, "w") as f:
        json.dump(loss_history, f, indent=2)

    print("\nâœ… è®­ç»ƒå®Œæˆ!")

if __name__ == "__main__":
    main()
