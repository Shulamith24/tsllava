#!/usr/bin/env python3
# SPDX-FileCopyrightText: 2025 Stanford University, ETH Zurich, and the project authors (see CONTRIBUTORS.md)
# SPDX-FileCopyrightText: 2025 This source file is part of the OpenTSLM open-source project.
#
# SPDX-License-Identifier: MIT

"""
PatchTST åœ¨ UCR æ•°æ®é›†ä¸Šçš„åˆ†ç±»

ä½¿ç”¨ HuggingFace çš„ PatchTSTForClassification è¿›è¡Œæ—¶é—´åºåˆ—åˆ†ç±»

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/train_patchtst_ucr.py \
        --dataset Adiac \
        --epochs 50 \
        --batch_size 32 \
        --lr 1e-3
"""

import os
import sys
import json
import argparse
import datetime
from pathlib import Path
from typing import List, Dict, Any

import torch
import torch.nn.functional as F
from torch.optim import AdamW
from torch.utils.data import DataLoader
from tqdm.auto import tqdm
from transformers import PatchTSTConfig, PatchTSTForClassification
from transformers import get_cosine_schedule_with_warmup

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from opentslm.time_series_datasets.ucr.UCRClassificationDataset import UCRClassificationDataset


def parse_args():
    parser = argparse.ArgumentParser(description="PatchTST UCR åˆ†ç±»")

    # æ•°æ®ç›¸å…³
    parser.add_argument("--dataset", type=str, default="Adiac", help="UCRæ•°æ®é›†åç§°")
    parser.add_argument("--data_path", type=str, default="./data", help="UCRæ•°æ®æ ¹ç›®å½•")
    
    # PatchTST æ¨¡å‹é…ç½®
    parser.add_argument("--context_length", type=int, default=None, 
                       help="ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆNoneåˆ™è‡ªåŠ¨è®¾ç½®ä¸ºæ•°æ®é›†æœ€å¤§é•¿åº¦ï¼‰")
    parser.add_argument("--patch_length", type=int, default=16, help="Patch é•¿åº¦")
    parser.add_argument("--stride", type=int, default=8, help="Patch æ­¥é•¿")
    parser.add_argument("--d_model", type=int, default=128, help="æ¨¡å‹ç»´åº¦")
    parser.add_argument("--num_attention_heads", type=int, default=8, help="Attention heads")
    parser.add_argument("--num_hidden_layers", type=int, default=3, help="Transformer å±‚æ•°")
    parser.add_argument("--ffn_dim", type=int, default=512, help="FFN ç»´åº¦")
    parser.add_argument("--dropout", type=float, default=0.1, help="Dropout")
    parser.add_argument("--use_cls_token", action="store_true", default=True, help="ä½¿ç”¨ CLS token")
    
    # è®­ç»ƒç›¸å…³
    parser.add_argument("--epochs", type=int, default=50, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=32, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--lr", type=float, default=1e-3, help="å­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=1e-4, help="æƒé‡è¡°å‡")
    parser.add_argument("--warmup_ratio", type=float, default=0.1, help="é¢„çƒ­æ¯”ä¾‹")
    parser.add_argument("--grad_clip", type=float, default=1.0, help="æ¢¯åº¦è£å‰ª")
    
    # ä¿å­˜ç›¸å…³
    parser.add_argument("--save_dir", type=str, default="results/patchtst", help="ç»“æœä¿å­˜ç›®å½•")
    
    # å…¶ä»–
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--device", type=str, default="cuda", help="è®¾å¤‡")
    parser.add_argument("--eval_every", type=int, default=5, help="æ¯Nè½®è¯„ä¼°ä¸€æ¬¡")
    parser.add_argument("--early_stop", type=int, default=15, help="æ—©åœè€å¿ƒå€¼")
    parser.add_argument("--eval_batch_size", type=int, default=64, help="è¯„ä¼°æ‰¹æ¬¡å¤§å°")
    
    return parser.parse_args()


def set_seed(seed: int):
    """è®¾ç½®éšæœºç§å­"""
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    import random
    import numpy as np
    random.seed(seed)
    np.random.seed(seed)


def get_dataset_stats(dataset_name: str, data_path: str):
    """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
    temp_dataset = UCRClassificationDataset(
        split="train",
        EOS_TOKEN="<eos>",
        dataset_name=dataset_name,
        raw_data_path=data_path,
    )
    
    num_classes = UCRClassificationDataset.get_num_classes()
    
    # è®¡ç®—æœ€å¤§é•¿åº¦
    max_length = 0
    for sample in temp_dataset:
        for ts in sample["time_series"]:
            max_length = max(max_length, len(ts))
    
    return num_classes, max_length


def prepare_batch_for_patchtst(
    batch: List[Dict],
    context_length: int,
    device: str,
):
    """
    å°† UCR æ‰¹æ¬¡è½¬æ¢ä¸º PatchTST æ ¼å¼
    
    Args:
        batch: UCR æ ¼å¼
        context_length: å›ºå®šä¸Šä¸‹æ–‡é•¿åº¦
        device: è®¾å¤‡
    
    Returns:
        past_values: [B, context_length, 1]
        labels: [B]
    """
    past_values_list = []
    labels = []
    
    for sample in batch:
        # è·å–ç¬¬ä¸€ä¸ªæ—¶é—´åºåˆ—
        ts = sample["time_series"][0]
        
        # å¡«å……æˆ–æˆªæ–­åˆ° context_length
        if len(ts) < context_length:
            # é›¶å¡«å……
            padded = torch.zeros(context_length, device=device)
            padded[:len(ts)] = ts.to(device)
        else:
            # æˆªæ–­
            padded = ts[:context_length].to(device)
        
        past_values_list.append(padded.unsqueeze(-1))  # [L, 1]
        labels.append(sample["int_label"])
    
    past_values = torch.stack(past_values_list, dim=0)  # [B, L, 1]
    labels = torch.tensor(labels, device=device, dtype=torch.long)
    
    return past_values, labels


def create_data_loaders(args, num_classes: int, context_length: int):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = UCRClassificationDataset(
        split="train",
        EOS_TOKEN="<eos>",
        dataset_name=args.dataset,
        raw_data_path=args.data_path,
    )
    
    val_dataset = UCRClassificationDataset(
        split="validation",
        EOS_TOKEN="<eos>",
        dataset_name=args.dataset,
        raw_data_path=args.data_path,
    )
    
    test_dataset = UCRClassificationDataset(
        split="test",
        EOS_TOKEN="<eos>",
        dataset_name=args.dataset,
        raw_data_path=args.data_path,
    )
    
    # ç®€å•çš„ collateï¼ˆä¸åšè½¬æ¢ï¼Œåœ¨è®­ç»ƒå¾ªç¯ä¸­è½¬æ¢ï¼‰
    def collate_fn(batch):
        return batch
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        collate_fn=collate_fn,
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.eval_batch_size,
        shuffle=False,
        collate_fn=collate_fn,
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=args.eval_batch_size,
        shuffle=False,
        collate_fn=collate_fn,
    )
    
    return train_loader, val_loader, test_loader


def train_one_epoch(
    model,
    train_loader: DataLoader,
    optimizer,
    scheduler,
    context_length: int,
    grad_clip: float,
    device: str,
    epoch: int,
    num_epochs: int,
) -> float:
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0.0
    num_batches = 0
    
    pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{num_epochs}")
    for batch in pbar:
        # è½¬æ¢ä¸º PatchTST æ ¼å¼
        past_values, labels = prepare_batch_for_patchtst(batch, context_length, device)
        
        # å‰å‘ä¼ æ’­
        outputs = model(
            past_values=past_values,
            target_values=labels,  # PatchTST æ¥å— target_values è®¡ç®—æŸå¤±
        )
        
        loss = outputs.loss
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)
        
        optimizer.step()
        scheduler.step()
        
        total_loss += loss.item()
        num_batches += 1
        
        pbar.set_postfix({
            "loss": f"{loss.item():.4f}",
            "lr": f"{scheduler.get_last_lr()[0]:.2e}"
        })
    
    return total_loss / max(num_batches, 1)


@torch.no_grad()
def evaluate(
    model,
    data_loader: DataLoader,
    context_length: int,
    device: str,
    desc: str = "Evaluating",
) -> Dict[str, Any]:
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    
    all_predictions = []
    all_labels = []
    total_loss = 0.0
    num_batches = 0
    
    for batch in tqdm(data_loader, desc=desc):
        # è½¬æ¢ä¸º PatchTST æ ¼å¼
        past_values, labels = prepare_batch_for_patchtst(batch, context_length, device)
        
        # å‰å‘ä¼ æ’­
        outputs = model(
            past_values=past_values,
            target_values=labels,
        )
        
        total_loss += outputs.loss.item()
        num_batches += 1
        
        # é¢„æµ‹
        logits = outputs.prediction_logits  # [B, num_classes]
        predictions = torch.argmax(logits, dim=-1)  # [B]
        
        all_predictions.extend(predictions.cpu().tolist())
        all_labels.extend(labels.cpu().tolist())
    
    avg_loss = total_loss / max(num_batches, 1)
    correct = sum(p == l for p, l in zip(all_predictions, all_labels))
    accuracy = correct / len(all_labels) if all_labels else 0.0
    
    return {
        "loss": avg_loss,
        "accuracy": accuracy,
        "predictions": all_predictions,
        "labels": all_labels,
    }


def main():
    args = parse_args()
    
    print("=" * 60)
    print("PatchTST UCR åˆ†ç±»")
    print("=" * 60)
    print(f"æ—¶é—´: {datetime.datetime.now()}")
    print(f"æ•°æ®é›†: {args.dataset}")
    print("=" * 60)
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    
    # è®¾ç½®è®¾å¤‡
    device = args.device if torch.cuda.is_available() else "cpu"
    print(f"\nä½¿ç”¨è®¾å¤‡: {device}")
    
    # è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“‚ åˆ†ææ•°æ®é›†...")
    num_classes, max_length = get_dataset_stats(args.dataset, args.data_path)
    
    # ç¡®å®š context_length
    if args.context_length is None:
        # å‘ä¸Šå–æ•´åˆ° patch_length çš„å€æ•°
        context_length = ((max_length - 1) // args.patch_length + 1) * args.patch_length
    else:
        context_length = args.context_length
    
    print(f"   ç±»åˆ«æ•°: {num_classes}")
    print(f"   æœ€å¤§é•¿åº¦: {max_length}")
    print(f"   Context length: {context_length}")
    
    # è®¡ç®— patch æ•°é‡
    num_patches = (context_length - args.patch_length) // args.stride + 1
    print(f"   é¢„æœŸ patch æ•°: {num_patches}")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = os.path.join(
        args.save_dir, 
        args.dataset, 
        f"L{context_length}_P{args.patch_length}_S{args.stride}"
    )
    os.makedirs(save_dir, exist_ok=True)
    with open(os.path.join(save_dir, "config.json"), "w") as f:
        json.dump(vars(args), f, indent=2)
    
    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ”§ åˆ›å»º PatchTST æ¨¡å‹...")
    config = PatchTSTConfig(
        num_input_channels=1,  # UCR å•å˜é‡
        num_targets=num_classes,
        context_length=context_length,
        patch_length=args.patch_length,
        stride=args.stride,
        d_model=args.d_model,
        num_attention_heads=args.num_attention_heads,
        num_hidden_layers=args.num_hidden_layers,
        ffn_dim=args.ffn_dim,
        dropout=args.dropout,
        use_cls_token=args.use_cls_token,
    )
    
    model = PatchTSTForClassification(config=config).to(device)
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"   æ€»å‚æ•°é‡: {total_params:,}")
    print(f"   d_model: {args.d_model}")
    print(f"   num_layers: {args.num_hidden_layers}")
    print(f"   use_cls_token: {args.use_cls_token}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("\nğŸ“‚ åŠ è½½æ•°æ®...")
    train_loader, val_loader, test_loader = create_data_loaders(
        args, num_classes, context_length
    )
    
    print(f"   Train batches: {len(train_loader)}")
    print(f"   Val batches: {len(val_loader)}")
    print(f"   Test batches: {len(test_loader)}")
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    print("\nâš™ï¸  åˆ›å»ºä¼˜åŒ–å™¨...")
    optimizer = AdamW(
        model.parameters(),
        lr=args.lr,
        weight_decay=args.weight_decay,
    )
    
    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    total_steps = args.epochs * len(train_loader)
    warmup_steps = int(args.warmup_ratio * total_steps)
    
    scheduler = get_cosine_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_steps,
    )
    
    print(f"   Total steps: {total_steps}")
    print(f"   Warmup steps: {warmup_steps}")
    
    # è®­ç»ƒå¾ªç¯
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    best_val_acc = 0.0
    patience_counter = 0
    loss_history = []
    
    try:
        for epoch in range(1, args.epochs + 1):
            train_loss = train_one_epoch(
                model, train_loader, optimizer, scheduler,
                context_length, args.grad_clip, device,
                epoch, args.epochs
            )
            
            if epoch % args.eval_every == 0 or epoch == args.epochs:
                print(f"\nğŸ“Š Epoch {epoch} è¯„ä¼°...")
                
                val_results = evaluate(
                    model, val_loader, context_length, device, "Validating"
                )
                val_loss = val_results["loss"]
                val_acc = val_results["accuracy"]
                
                print(f"   Train Loss: {train_loss:.4f}")
                print(f"   Val Loss: {val_loss:.4f}")
                print(f"   Val Accuracy: {val_acc:.4f}")
                
                # æ˜¾ç¤ºæ ·æœ¬é¢„æµ‹
                print("   Sample predictions (first 5):")
                for i in range(min(5, len(val_results["predictions"]))):
                    pred = val_results["predictions"][i]
                    label = val_results["labels"][i]
                    print(f"     Pred: {pred} | Label: {label} | {'âœ“' if pred == label else 'âœ—'}")
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_acc > best_val_acc:
                    best_val_acc = val_acc
                    patience_counter = 0
                    
                    checkpoint = {
                        "model_state": model.state_dict(),
                        "optimizer_state": optimizer.state_dict(),
                        "scheduler_state": scheduler.state_dict(),
                        "epoch": epoch,
                        "val_loss": val_loss,
                        "val_acc": val_acc,
                        "config": config.to_dict(),
                        "args": vars(args),
                    }
                    torch.save(checkpoint, os.path.join(save_dir, "best_model.pt"))
                    print(f"   ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹")
                else:
                    patience_counter += 1
                    print(f"   (æ— æ”¹è¿›, patience: {patience_counter}/{args.early_stop})")
                
                # è®°å½•å†å²
                loss_history.append({
                    "epoch": epoch,
                    "train_loss": train_loss,
                    "val_loss": val_loss,
                    "val_acc": val_acc,
                })
                with open(os.path.join(save_dir, "loss_history.json"), "w") as f:
                    json.dump(loss_history, f, indent=2)
            else:
                print(f"Epoch {epoch}: Train Loss = {train_loss:.4f}")
            
            # æ—©åœ
            if patience_counter >= args.early_stop:
                print(f"\nâ¹ï¸  æ—©åœ! éªŒè¯å‡†ç¡®ç‡ {args.early_stop} è½®æœªæ”¹è¿›")
                break
        
        # æœ€ç»ˆæµ‹è¯•
        print("\n" + "=" * 60)
        print("ğŸ“‹ æœ€ç»ˆæµ‹è¯•è¯„ä¼°...")
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        best_ckpt = torch.load(
            os.path.join(save_dir, "best_model.pt"),
            map_location=device,
            weights_only=False
        )
        model.load_state_dict(best_ckpt["model_state"])
        
        test_results = evaluate(
            model, test_loader, context_length, device, "Testing"
        )
        
        print(f"\nâœ… æµ‹è¯•ç»“æœ:")
        print(f"   Test Loss: {test_results['loss']:.4f}")
        print(f"   Test Accuracy: {test_results['accuracy']:.4f}")
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        final_results = {
            "dataset": args.dataset,
            "num_classes": num_classes,
            "context_length": context_length,
            "total_params": total_params,
            "best_val_acc": best_val_acc,
            "test_loss": test_results["loss"],
            "test_accuracy": test_results["accuracy"],
            "epochs_trained": epoch,
        }
        
        with open(os.path.join(save_dir, "final_results.json"), "w") as f:
            json.dump(final_results, f, indent=2)
        
        with open(os.path.join(save_dir, "test_predictions.json"), "w") as f:
            json.dump({
                "predictions": test_results["predictions"],
                "labels": test_results["labels"],
            }, f, indent=2)
        
        print("=" * 60)
        print(f"ç»“æœä¿å­˜åˆ°: {save_dir}")
        print("=" * 60)
    
    except KeyboardInterrupt:
        print("\nâš ï¸  è®­ç»ƒè¢«ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
