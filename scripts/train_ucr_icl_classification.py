#!/usr/bin/env python3
# SPDX-FileCopyrightText: 2025 Stanford University, ETH Zurich, and the project authors (see CONTRIBUTORS.md)
# SPDX-FileCopyrightText: 2025 This source file is part of the OpenTSLM open-source project.
#
# SPDX-License-Identifier: MIT

"""
UCR ICLåˆ†ç±»è®­ç»ƒè„šæœ¬

ä½¿ç”¨In-Context LearningèŒƒå¼è¿›è¡Œæ—¶é—´åºåˆ—åˆ†ç±»è®­ç»ƒã€‚

è®­ç»ƒæµç¨‹ï¼š
1. åŠ è½½é¢„è®­ç»ƒçš„OpenTSLM SPæ¨¡å‹
2. åŠ è½½è®­ç»ƒå¥½çš„TSLANet encoderç”¨äºæ£€ç´¢
3. æ„å»ºæ£€ç´¢ç´¢å¼•
4. ä½¿ç”¨ICLæ ¼å¼è¿›è¡Œè®­ç»ƒ/æµ‹è¯•

ä½¿ç”¨æ–¹æ³•ï¼š
    # é¦–å…ˆè®­ç»ƒTSLANet
    python scripts/train_tslanet_ucr.py --dataset ECG5000
    
    # ç„¶åè¿›è¡ŒICLåˆ†ç±»è®­ç»ƒ
    python scripts/train_ucr_icl_classification.py \\
        --dataset ECG5000 \\
        --pretrained_model OpenTSLM/llama-3.2-1b-m4-sp \\
        --tslanet_checkpoint results/tslanet_ucr/ECG5000/best_model.pt \\
        --k_shot 1 \\
        --epochs 10
"""

import os
import sys
import json
import argparse
import datetime
from pathlib import Path
from typing import List, Dict, Any

import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.optim import AdamW
from torch.nn.utils import clip_grad_norm_
from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler
from tqdm.auto import tqdm
from transformers import get_linear_schedule_with_warmup

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from opentslm.model.llm.OpenTSLM import OpenTSLM
from opentslm.model.llm.OpenTSLMSP import OpenTSLMSP
from opentslm.model.encoder.TSLANetEncoder import TSLANetEncoder
from opentslm.retrieval.TSLANetRetriever import TSLANetRetriever
from opentslm.time_series_datasets.ucr.UCRICLClassificationDataset import (
    UCRICLClassificationDataset,
    create_icl_collate_fn
)
from opentslm.time_series_datasets.ucr.ucr_loader import load_ucr_dataset, ensure_ucr_data
from opentslm.time_series_datasets.ucr.UCRClassificationDataset import index_to_excel_label
from opentslm.model_config import PATCH_SIZE, ENCODER_OUTPUT_DIM


def parse_args():
    parser = argparse.ArgumentParser(description="UCR ICLåˆ†ç±»è®­ç»ƒ")

    # æ•°æ®ç›¸å…³
    parser.add_argument("--dataset", type=str, default="ECG5000", help="UCRæ•°æ®é›†åç§°")
    parser.add_argument("--data_path", type=str, default="./data", help="UCRæ•°æ®æ ¹ç›®å½•")
    
    # æ¨¡å‹ç›¸å…³ - OpenTSLM
    parser.add_argument("--pretrained_model", type=str, default=None, 
                        help="é¢„è®­ç»ƒæ¨¡å‹ID (HuggingFace repo_id)")
    parser.add_argument("--local_checkpoint", type=str, default=None,
                        help="æœ¬åœ°checkpointè·¯å¾„")
    parser.add_argument("--encoder_type", type=str, default="tslanet",
                        choices=["transformer_cnn", "tslanet"],
                        help="ç¼–ç å™¨ç±»å‹")
    parser.add_argument("--llm_id", type=str, default="meta-llama/Llama-3.2-1B",
                        help="LLMæ¨¡å‹ID")
    
    # æ¨¡å‹ç›¸å…³ - TSLANetæ£€ç´¢å™¨
    parser.add_argument("--tslanet_checkpoint", type=str, required=True,
                        help="TSLANetåˆ†ç±»å™¨checkpointè·¯å¾„ (ç”¨äºæ£€ç´¢)")
    
    # ICLç›¸å…³
    parser.add_argument("--k_shot", type=int, default=1, 
                        help="æ¯ä¸ªç±»åˆ«çš„æ”¯æŒæ ·æœ¬æ•°")
    parser.add_argument("--top_m", type=int, default=10,
                        help="æ¯ä¸ªç±»åˆ«æ£€ç´¢çš„å€™é€‰æ•°é‡")
    
    # LoRAç›¸å…³
    parser.add_argument("--no_lora", action="store_true", help="ç¦ç”¨LoRA")
    parser.add_argument("--lora_r", type=int, default=16, help="LoRA rank")
    parser.add_argument("--lora_alpha", type=int, default=32, help="LoRA alpha")
    
    # è®­ç»ƒç›¸å…³
    parser.add_argument("--epochs", type=int, default=10, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=4, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--lr_encoder", type=float, default=2e-4, help="ç¼–ç å™¨å­¦ä¹ ç‡")
    parser.add_argument("--lr_projector", type=float, default=1e-4, help="æŠ•å½±å±‚å­¦ä¹ ç‡")
    parser.add_argument("--lr_lora", type=float, default=1e-4, help="LoRAå­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=1e-2, help="æƒé‡è¡°å‡")
    parser.add_argument("--grad_clip", type=float, default=1.0, help="æ¢¯åº¦è£å‰ª")
    parser.add_argument("--warmup_ratio", type=float, default=0.03, help="é¢„çƒ­æ¯”ä¾‹")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    parser.add_argument("--gradient_checkpointing", action="store_true", help="å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
    parser.add_argument("--freeze_encoder", action="store_true", help="å†»ç»“ç¼–ç å™¨å‚æ•°")
    
    # ä¿å­˜ç›¸å…³
    parser.add_argument("--save_dir", type=str, default="results/icl_classification", help="ç»“æœä¿å­˜ç›®å½•")
    
    # å…¶ä»–
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--device", type=str, default="cuda", help="è®¾å¤‡")
    parser.add_argument("--eval_every", type=int, default=1, help="æ¯Nè½®è¯„ä¼°ä¸€æ¬¡")
    parser.add_argument("--early_stop", type=int, default=5, help="æ—©åœè€å¿ƒå€¼")
    parser.add_argument("--max_new_tokens", type=int, default=10, help="ç”Ÿæˆæœ€å¤§tokenæ•°")
    parser.add_argument("--eval_batch_size", type=int, default=8, help="è¯„ä¼°æ‰¹æ¬¡å¤§å°")
    
    return parser.parse_args()


def setup_distributed():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    if "LOCAL_RANK" in os.environ:
        local_rank = int(os.environ["LOCAL_RANK"])
        world_size = int(os.environ.get("WORLD_SIZE", 1))
        rank = int(os.environ.get("RANK", 0))
        
        torch.cuda.set_device(local_rank)
        dist.init_process_group(backend="nccl", init_method="env://")
        
        return local_rank, world_size, rank
    return 0, 1, 0


def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ"""
    if dist.is_initialized():
        dist.destroy_process_group()


def get_model(model):
    """è·å–åº•å±‚æ¨¡å‹ï¼ˆå…¼å®¹DDPåŒ…è£…ï¼‰"""
    return model.module if hasattr(model, "module") else model


def set_seed(seed: int):
    """è®¾ç½®éšæœºç§å­"""
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def calculate_accuracy(predictions: List[str], labels: List[str]) -> float:
    """è®¡ç®—åˆ†ç±»å‡†ç¡®ç‡"""
    correct = 0
    for pred, label in zip(predictions, labels):
        pred_clean = pred.strip().upper()
        label_clean = label.strip().upper()
        
        # æå–é¢„æµ‹æ ‡ç­¾
        pred_label = None
        if len(pred_clean) == 1 and pred_clean.isalpha():
            pred_label = pred_clean
        elif len(pred_clean) == 2 and pred_clean.isalpha():
            pred_label = pred_clean  # AA, ABç­‰
        elif pred_clean:
            # å–æœ€åä¸€ä¸ªè¯
            words = pred_clean.split()
            if words:
                last_word = words[-1].strip(".,!?:;")
                if len(last_word) <= 2 and last_word.isalpha():
                    pred_label = last_word.upper()
        
        if pred_label == label_clean:
            correct += 1
    
    return correct / len(predictions) if predictions else 0.0


def load_tslanet_for_retrieval(checkpoint_path: str, device: str):
    """åŠ è½½TSLANetç”¨äºæ£€ç´¢"""
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    config = checkpoint.get("config", {})
    encoder_state = checkpoint["encoder_state"]
    patch_size = config.get("patch_size", 8)
    
    # è·å–max_seq_len: ä¼˜å…ˆä½¿ç”¨ä¿å­˜çš„å€¼ï¼Œå¦åˆ™ä»pos_embedæ¨æ–­
    if "max_seq_len" in checkpoint:
        max_seq_len = checkpoint["max_seq_len"]
    else:
        # ä»pos_embedå½¢çŠ¶æ¨æ–­ (å…¼å®¹æ—§ç‰ˆæœ¬checkpoint)
        pos_embed_shape = encoder_state["pos_embed"].shape  # [1, num_patches, emb_dim]
        num_patches = pos_embed_shape[1]
        stride = patch_size // 2
        max_seq_len = (num_patches - 1) * stride + patch_size
    
    # åˆ›å»ºencoder
    encoder = TSLANetEncoder(
        output_dim=config.get("emb_dim", 128),
        dropout=config.get("dropout", 0.15),
        patch_size=patch_size,
        emb_dim=config.get("emb_dim", 128),
        depth=config.get("depth", 2),
        max_seq_len=max_seq_len
    )
    
    # åŠ è½½æƒé‡
    encoder.load_state_dict(encoder_state)
    encoder = encoder.to(device)
    encoder.eval()
    
    print(f"âœ… åŠ è½½TSLANetæ£€ç´¢å™¨: {checkpoint_path}")
    print(f"   åºåˆ—é•¿åº¦: {checkpoint.get('seq_len', 'unknown')}")
    print(f"   ç±»åˆ«æ•°: {checkpoint.get('num_classes', 'unknown')}")
    print(f"   max_seq_len: {max_seq_len}")
    
    return encoder, checkpoint


def create_datasets(args, retriever, eos_token: str):
    """åˆ›å»ºICL Dataset"""
    ensure_ucr_data()
    
    # åŠ è½½æ•°æ®
    train_df, test_df = load_ucr_dataset(args.dataset, raw_data_path=args.data_path)
    
    # è·å–ç±»åˆ«ä¿¡æ¯
    all_labels = sorted(train_df["label"].unique().tolist())
    label_to_idx = {label: idx for idx, label in enumerate(all_labels)}
    
    # æå–æ—¶é—´åºåˆ—å’Œæ ‡ç­¾
    feature_cols = [col for col in train_df.columns if col != "label"]
    
    def df_to_tensors(df):
        ts = torch.tensor(df[feature_cols].values, dtype=torch.float32)
        labels = torch.tensor([label_to_idx[l] for l in df["label"]], dtype=torch.long)
        return ts, labels
    
    train_ts, train_labels = df_to_tensors(train_df)
    test_ts, test_labels = df_to_tensors(test_df)
    
    print(f"ğŸ“Š Dataset: {args.dataset}")
    print(f"   Classes: {len(all_labels)}")
    print(f"   Train samples: {len(train_ts)}")
    print(f"   Test samples: {len(test_ts)}")
    
    # æ„å»ºæ£€ç´¢ç´¢å¼• (åªç”¨è®­ç»ƒé›†)
    print("\nğŸ”§ æ„å»ºæ£€ç´¢ç´¢å¼•...")
    retriever.build_index(train_ts, train_labels)
    
    # åˆ›å»ºDataset
    train_dataset = UCRICLClassificationDataset(
        time_series=train_ts,
        labels=train_labels,
        retriever=retriever,
        dataset_name=args.dataset,
        k_shot=args.k_shot,
        top_m=args.top_m,
        eos_token=eos_token,
        split="train",
        exclude_query=True
    )
    
    # æµ‹è¯•é›†ä¹Ÿç”¨è®­ç»ƒé›†çš„ç´¢å¼•è¿›è¡Œæ£€ç´¢
    test_dataset = UCRICLClassificationDataset(
        time_series=test_ts,
        labels=test_labels,
        retriever=retriever,
        dataset_name=args.dataset,
        k_shot=args.k_shot,
        top_m=args.top_m,
        eos_token=eos_token,
        split="test",
        exclude_query=False
    )
    
    return train_dataset, test_dataset


def train_one_epoch(
    model,
    train_loader: DataLoader,
    optimizer,
    scheduler,
    grad_clip: float,
    epoch: int,
    num_epochs: int,
    gradient_accumulation_steps: int = 1,
    rank: int = 0,
) -> float:
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0.0
    num_batches = 0
    optimizer.zero_grad()
    
    pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{num_epochs}", disable=(rank != 0))
    for step, batch in enumerate(pbar):
        loss = model(batch)
        loss = loss / gradient_accumulation_steps
        
        loss.backward()
        
        if (step + 1) % gradient_accumulation_steps == 0:
            clip_grad_norm_(model.parameters(), max_norm=grad_clip)
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()
        
        total_loss += loss.item() * gradient_accumulation_steps
        num_batches += 1
        
        if rank == 0:
            pbar.set_postfix({
                "loss": f"{loss.item() * gradient_accumulation_steps:.4f}",
                "lr": f"{scheduler.get_last_lr()[0]:.2e}"
            })
    
    if num_batches % gradient_accumulation_steps != 0:
        clip_grad_norm_(model.parameters(), max_norm=grad_clip)
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()
    
    return total_loss / max(num_batches, 1)


@torch.no_grad()
def evaluate(
    model,
    data_loader: DataLoader,
    max_new_tokens: int,
    desc: str = "Evaluating",
    rank: int = 0,
) -> Dict[str, Any]:
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    underlying_model = get_model(model)
    
    all_predictions = []
    all_labels = []
    total_loss = 0.0
    num_batches = 0
    
    for batch in tqdm(data_loader, desc=desc, disable=(rank != 0)):
        loss = underlying_model.compute_loss(batch)
        total_loss += loss.item()
        num_batches += 1
        
        predictions = underlying_model.generate(batch, max_new_tokens=max_new_tokens)
        
        for sample, pred in zip(batch, predictions):
            all_predictions.append(pred)
            all_labels.append(sample["letter_label"])
    
    avg_loss = total_loss / max(num_batches, 1)
    accuracy = calculate_accuracy(all_predictions, all_labels)
    
    return {
        "loss": avg_loss,
        "accuracy": accuracy,
        "predictions": all_predictions,
        "labels": all_labels,
    }


def main():
    args = parse_args()
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
    local_rank, world_size, rank = setup_distributed()
    
    if rank == 0:
        print("=" * 60)
        print("UCR ICLåˆ†ç±»è®­ç»ƒ")
        print("=" * 60)
        print(f"æ—¶é—´: {datetime.datetime.now()}")
        print(f"æ•°æ®é›†: {args.dataset}")
        print(f"K-shot: {args.k_shot}")
        print(f"Top-M: {args.top_m}")
        print("=" * 60)
    
    set_seed(args.seed + rank)
    
    # è®¾ç½®è®¾å¤‡
    if world_size > 1:
        device = f"cuda:{local_rank}"
    elif args.device == "cuda" and torch.cuda.is_available():
        device = "cuda"
    else:
        if rank == 0:
            print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        device = "cpu"
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = os.path.join(args.save_dir, args.dataset)
    if rank == 0:
        os.makedirs(save_dir, exist_ok=True)
        with open(os.path.join(save_dir, "config.json"), "w") as f:
            json.dump(vars(args), f, indent=2)
    
    if world_size > 1:
        dist.barrier()
    
    # åŠ è½½TSLANetç”¨äºæ£€ç´¢
    if rank == 0:
        print("\nğŸ”§ åŠ è½½TSLANetæ£€ç´¢å™¨...")
    tslanet_encoder, tslanet_ckpt = load_tslanet_for_retrieval(args.tslanet_checkpoint, device)
    retriever = TSLANetRetriever(tslanet_encoder, device=device)
    
    # åŠ è½½OpenTSLMæ¨¡å‹
    if rank == 0:
        print("\nğŸ”§ åŠ è½½OpenTSLMæ¨¡å‹...")
    
    use_lora = not args.no_lora
    
    if args.pretrained_model:
        model = OpenTSLM.load_pretrained(
            repo_id=args.pretrained_model,
            device=device,
            enable_lora=use_lora,
        )
        if use_lora and (args.lora_r != 16 or args.lora_alpha != 32):
            model.disable_lora()
            model.enable_lora(lora_r=args.lora_r, lora_alpha=args.lora_alpha)
    elif args.local_checkpoint:
        tslanet_config = {
            "patch_size": tslanet_ckpt.get("config", {}).get("patch_size", 8),
            "output_dim": ENCODER_OUTPUT_DIM,
        }
        model = OpenTSLMSP(
            llm_id=args.llm_id,
            device=device,
            encoder_type=args.encoder_type,
            tslanet_config=tslanet_config if args.encoder_type == "tslanet" else None,
        )
        checkpoint = torch.load(args.local_checkpoint, map_location=device, weights_only=False)
        model.encoder.load_state_dict(checkpoint["encoder_state"])
        model.projector.load_state_dict(checkpoint["projector_state"])
        if use_lora:
            model.enable_lora(lora_r=args.lora_r, lora_alpha=args.lora_alpha)
            model.load_lora_state_from_checkpoint(checkpoint, allow_missing=True)
    else:
        raise ValueError("å¿…é¡»æŒ‡å®š --pretrained_model æˆ– --local_checkpoint")
    
    if args.gradient_checkpointing:
        model.enable_gradient_checkpointing()
    
    if args.freeze_encoder:
        for param in model.encoder.parameters():
            param.requires_grad = False
        if rank == 0:
            print("ğŸ§Š ç¼–ç å™¨å‚æ•°å·²å†»ç»“")
    
    # åˆ›å»ºæ•°æ®é›†
    if rank == 0:
        print("\nğŸ“‚ åˆ›å»ºICLæ•°æ®é›†...")
    eos_token = get_model(model).get_eos_token() if hasattr(model, "module") else model.get_eos_token()
    train_dataset, test_dataset = create_datasets(args, retriever, eos_token)
    
    # åˆ›å»ºDataLoader
    collate_fn = create_icl_collate_fn(patch_size=PATCH_SIZE)
    
    train_sampler = None
    if world_size > 1:
        train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=(train_sampler is None),
        sampler=train_sampler,
        collate_fn=collate_fn,
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=args.eval_batch_size,
        shuffle=False,
        collate_fn=collate_fn,
    )
    
    if rank == 0:
        print(f"   Train batches: {len(train_loader)}")
        print(f"   Test batches: {len(test_loader)}")
    
    # DDPåŒ…è£…
    if world_size > 1:
        model = DDP(model, device_ids=[local_rank])
        if rank == 0:
            print(f"âœ… æ¨¡å‹å·²ç”¨DDPåŒ…è£… (world_size={world_size})")
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    underlying_model = get_model(model)
    param_groups = []
    if not args.freeze_encoder:
        param_groups.append({"params": underlying_model.encoder.parameters(), "lr": args.lr_encoder})
    param_groups.append({"params": underlying_model.projector.parameters(), "lr": args.lr_projector})
    
    if use_lora:
        lora_params = underlying_model.get_lora_parameters()
        if lora_params:
            param_groups.append({"params": lora_params, "lr": args.lr_lora})
    
    optimizer = AdamW(param_groups, weight_decay=args.weight_decay)
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    effective_batch_size = args.batch_size * args.gradient_accumulation_steps * world_size
    steps_per_epoch = len(train_loader) // args.gradient_accumulation_steps
    total_steps = args.epochs * steps_per_epoch
    warmup_steps = int(args.warmup_ratio * total_steps)
    scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)
    
    if rank == 0:
        print(f"\nâš™ï¸ è®­ç»ƒé…ç½®:")
        print(f"   Effective batch size: {effective_batch_size}")
        print(f"   Total steps: {total_steps}")
        print(f"   Warmup steps: {warmup_steps}")
    
    # è®­ç»ƒå¾ªç¯
    if rank == 0:
        print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    best_test_acc = 0.0
    patience_counter = 0
    loss_history = []
    
    try:
        for epoch in range(1, args.epochs + 1):
            if train_sampler is not None:
                train_sampler.set_epoch(epoch)
            
            # è®­ç»ƒ
            train_loss = train_one_epoch(
                model, train_loader, optimizer, scheduler,
                args.grad_clip, epoch, args.epochs,
                args.gradient_accumulation_steps, rank
            )
            
            # è¯„ä¼°
            if epoch % args.eval_every == 0 or epoch == args.epochs:
                if rank == 0:
                    print(f"\nğŸ“Š Epoch {epoch} è¯„ä¼°...")
                
                test_results = evaluate(model, test_loader, args.max_new_tokens, "Testing", rank)
                test_loss = test_results["loss"]
                test_acc = test_results["accuracy"]
                
                if rank == 0:
                    print(f"   Train Loss: {train_loss:.4f}")
                    print(f"   Test Loss: {test_loss:.4f}")
                    print(f"   Test Accuracy: {test_acc:.4f}")
                    
                    # æ˜¾ç¤ºæ ·æœ¬é¢„æµ‹
                    print("   Sample predictions:")
                    for i in range(min(3, len(test_results["predictions"]))):
                        pred = test_results["predictions"][i]
                        label = test_results["labels"][i]
                        pred_short = pred[:30] if len(pred) > 30 else pred
                        print(f"     Pred: '{pred_short}' | Label: '{label}'")
                    
                    loss_history.append({
                        "epoch": epoch,
                        "train_loss": train_loss,
                        "test_loss": test_loss,
                        "test_acc": test_acc,
                    })
                    
                    with open(os.path.join(save_dir, "loss_history.json"), "w") as f:
                        json.dump(loss_history, f, indent=2)
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if test_acc > best_test_acc:
                    best_test_acc = test_acc
                    patience_counter = 0
                    
                    if rank == 0:
                        checkpoint = {
                            "encoder_state": underlying_model.encoder.state_dict(),
                            "projector_state": underlying_model.projector.state_dict(),
                            "epoch": epoch,
                            "test_acc": best_test_acc,
                            "args": vars(args),
                        }
                        underlying_model.save_lora_state_to_checkpoint(checkpoint)
                        torch.save(checkpoint, os.path.join(save_dir, "best_model.pt"))
                        print(f"ğŸ’¾ Saved best model (test_acc={best_test_acc:.4f})")
                else:
                    patience_counter += 1
                    if rank == 0:
                        print(f"   (æ— æ”¹è¿›, patience: {patience_counter}/{args.early_stop})")
                
                if patience_counter >= args.early_stop:
                    if rank == 0:
                        print(f"\nâ¹ï¸ æ—©åœ! æµ‹è¯•å‡†ç¡®ç‡ {args.early_stop} è½®æœªæ”¹è¿›")
                    break
            else:
                if rank == 0:
                    print(f"Epoch {epoch}: Train Loss = {train_loss:.4f}")
        
        # æœ€ç»ˆç»“æœ
        if rank == 0:
            print("\n" + "=" * 60)
            print("ğŸ“‹ æœ€ç»ˆç»“æœ")
            print(f"   Best Test Accuracy: {best_test_acc:.4f}")
            
            final_results = {
                "dataset": args.dataset,
                "k_shot": args.k_shot,
                "top_m": args.top_m,
                "best_test_acc": best_test_acc,
                "epochs_trained": epoch,
            }
            
            with open(os.path.join(save_dir, "final_results.json"), "w") as f:
                json.dump(final_results, f, indent=2)
            
            print("=" * 60)
            print(f"ç»“æœä¿å­˜åˆ°: {save_dir}")
            print("=" * 60)
    
    finally:
        cleanup_distributed()


if __name__ == "__main__":
    main()
