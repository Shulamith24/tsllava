#!/usr/bin/env python3
# SPDX-FileCopyrightText: 2025 Stanford University, ETH Zurich, and the project authors (see CONTRIBUTORS.md)
# SPDX-FileCopyrightText: 2025 This source file is part of the OpenTSLM open-source project.
#
# SPDX-License-Identifier: MIT

"""
TSLANetç¼–ç å™¨åœ¨UCRæ•°æ®é›†ä¸Šçš„é¢„è®­ç»ƒè„šæœ¬

ä½¿ç”¨æ©ç é¢„è®­ç»ƒæ–¹æ³•ï¼ˆç±»ä¼¼MAE/PatchTSTï¼‰åœ¨UCRçš„98ä¸ªè®­ç»ƒæ•°æ®é›†ä¸Šé¢„è®­ç»ƒTSLANetç¼–ç å™¨ã€‚

ä½¿ç”¨æ–¹æ³•:
    python scripts/pretrain_tslanet.py \
        --dataset_list src/opentslm/time_series_datasets/ucr/ucr_train_98_datasets.txt \
        --save_path pretrained/tslanet_ucr98.pt \
        --epochs 50 \
        --batch_size 64 \
        --mask_ratio 0.4

å‚æ•°è¯´æ˜:
    --dataset_list: è®­ç»ƒæ•°æ®é›†åˆ—è¡¨æ–‡ä»¶è·¯å¾„
    --save_path: é¢„è®­ç»ƒæƒé‡ä¿å­˜è·¯å¾„
    --epochs: è®­ç»ƒè½®æ•° (é»˜è®¤50)
    --batch_size: æ‰¹æ¬¡å¤§å° (é»˜è®¤64)
    --mask_ratio: æ©ç æ¯”ä¾‹ (é»˜è®¤0.4)
    --lr: å­¦ä¹ ç‡ (é»˜è®¤1e-3)
    --patch_size: patchå¤§å° (é»˜è®¤8)
    --emb_dim: åµŒå…¥ç»´åº¦ (é»˜è®¤128)
    --depth: ç¼–ç å™¨æ·±åº¦ (é»˜è®¤2)
    --data_path: UCRæ•°æ®æ ¹ç›®å½• (é»˜è®¤./data)
"""

import os
import sys
import argparse
import datetime
from pathlib import Path

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from opentslm.model.encoder.TSLANetEncoder import TSLANetEncoder
from opentslm.time_series_datasets.ucr.ucr_pretrain_loader import (
    get_ucr_pretrain_loader,
    load_dataset_list,
)


def parse_args():
    parser = argparse.ArgumentParser(description="TSLANet UCRé¢„è®­ç»ƒ")
    
    # æ•°æ®ç›¸å…³
    parser.add_argument(
        "--dataset_list",
        type=str,
        default="src/opentslm/time_series_datasets/ucr/ucr_train_98_datasets.txt",
        help="è®­ç»ƒæ•°æ®é›†åˆ—è¡¨æ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument(
        "--data_path",
        type=str,
        default="./data",
        help="UCRæ•°æ®æ ¹ç›®å½•",
    )
    parser.add_argument(
        "--save_path",
        type=str,
        default="pretrained/tslanet_ucr98.pt",
        help="é¢„è®­ç»ƒæƒé‡ä¿å­˜è·¯å¾„",
    )
    
    # è®­ç»ƒç›¸å…³
    parser.add_argument("--epochs", type=int, default=50, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=64, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--lr", type=float, default=1e-3, help="å­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=1e-4, help="æƒé‡è¡°å‡")
    parser.add_argument("--mask_ratio", type=float, default=0.4, help="æ©ç æ¯”ä¾‹")
    
    # æ¨¡å‹ç»“æ„
    parser.add_argument("--patch_size", type=int, default=8, help="patchå¤§å°")
    parser.add_argument("--emb_dim", type=int, default=128, help="åµŒå…¥ç»´åº¦")
    parser.add_argument("--depth", type=int, default=2, help="ç¼–ç å™¨æ·±åº¦")
    parser.add_argument("--dropout", type=float, default=0.15, help="dropoutæ¯”ä¾‹")
    
    # å…¶ä»–
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--device", type=str, default="cuda", help="è®¾å¤‡")
    parser.add_argument("--num_workers", type=int, default=0, help="æ•°æ®åŠ è½½çº¿ç¨‹æ•°")
    parser.add_argument("--val_ratio", type=float, default=0.1, help="éªŒè¯é›†æ¯”ä¾‹")
    parser.add_argument("--early_stop", type=int, default=10, help="æ—©åœè€å¿ƒå€¼")
    parser.add_argument("--dry_run", action="store_true", help="å¹²è¿è¡Œæ¨¡å¼(åªè¿è¡Œ1ä¸ªbatch)")
    
    return parser.parse_args()


def set_seed(seed: int):
    """è®¾ç½®éšæœºç§å­"""
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def train_one_epoch(
    model: TSLANetEncoder,
    train_loader: DataLoader,
    optimizer: optim.Optimizer,
    mask_ratio: float,
    device: str,
    dry_run: bool = False,
) -> float:
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0.0
    num_batches = 0
    
    pbar = tqdm(train_loader, desc="Training")
    for batch in pbar:
        batch = batch.to(device)
        
        # æ©ç é¢„è®­ç»ƒå‰å‘ä¼ æ’­
        preds, target, mask = model.pretrain_forward(batch, mask_ratio=mask_ratio)
        
        # è®¡ç®—æŸå¤± (åªåœ¨è¢«æ©ç çš„ä½ç½®è®¡ç®—)
        loss = (preds - target) ** 2
        loss = loss.mean(dim=-1)  # [B, N]
        loss = (loss * mask).sum() / (mask.sum() + 1e-8)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()
        
        total_loss += loss.item()
        num_batches += 1
        
        pbar.set_postfix({"loss": f"{loss.item():.4f}"})
        
        if dry_run:
            break
    
    return total_loss / max(num_batches, 1)


@torch.no_grad()
def validate(
    model: TSLANetEncoder,
    val_loader: DataLoader,
    mask_ratio: float,
    device: str,
    dry_run: bool = False,
) -> float:
    """éªŒè¯"""
    model.eval()
    total_loss = 0.0
    num_batches = 0
    
    for batch in val_loader:
        batch = batch.to(device)
        
        preds, target, mask = model.pretrain_forward(batch, mask_ratio=mask_ratio)
        
        loss = (preds - target) ** 2
        loss = loss.mean(dim=-1)
        loss = (loss * mask).sum() / (mask.sum() + 1e-8)
        
        total_loss += loss.item()
        num_batches += 1
        
        if dry_run:
            break
    
    return total_loss / max(num_batches, 1)


def main():
    args = parse_args()
    
    print("=" * 60)
    print("TSLANet UCRé¢„è®­ç»ƒ")
    print("=" * 60)
    print(f"æ—¶é—´: {datetime.datetime.now()}")
    print(f"å‚æ•°: {args}")
    print("=" * 60)
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    
    # è®¾ç½®è®¾å¤‡
    if args.device == "cuda" and not torch.cuda.is_available():
        print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        args.device = "cpu"
    device = args.device
    
    # æ£€æŸ¥æ•°æ®é›†åˆ—è¡¨æ–‡ä»¶
    if not os.path.exists(args.dataset_list):
        print(f"âŒ æ•°æ®é›†åˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {args.dataset_list}")
        sys.exit(1)
    
    # åŠ è½½æ•°æ®
    print("\nğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®...")
    train_loader = get_ucr_pretrain_loader(
        dataset_list_file=args.dataset_list,
        split="train",
        batch_size=args.batch_size,
        shuffle=True,
        raw_data_path=args.data_path,
        patch_size=args.patch_size,
        num_workers=args.num_workers,
    )
    
    # åŠ è½½éªŒè¯æ•°æ® (ä½¿ç”¨æµ‹è¯•é›†çš„ä¸€éƒ¨åˆ†ä½œä¸ºéªŒè¯)
    print("\nğŸ“‚ åŠ è½½éªŒè¯æ•°æ®...")
    val_loader = get_ucr_pretrain_loader(
        dataset_list_file=args.dataset_list,
        split="test",  # ä½¿ç”¨test splitçš„æ•°æ®ä½œä¸ºéªŒè¯
        batch_size=args.batch_size,
        shuffle=False,
        raw_data_path=args.data_path,
        patch_size=args.patch_size,
        num_workers=args.num_workers,
    )
    
    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ”§ åˆ›å»ºTSLANetç¼–ç å™¨...")
    model = TSLANetEncoder(
        output_dim=args.emb_dim,
        patch_size=args.patch_size,
        emb_dim=args.emb_dim,
        depth=args.depth,
        dropout=args.dropout,
        use_icb=True,
        use_asb=True,
        adaptive_filter=True,
    ).to(device)
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    num_params = sum(p.numel() for p in model.parameters())
    num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"   æ€»å‚æ•°é‡: {num_params:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°: {num_trainable:,}")
    
    # ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(
        model.parameters(),
        lr=args.lr,
        weight_decay=args.weight_decay,
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=args.epochs,
        eta_min=1e-6,
    )
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = os.path.dirname(args.save_path)
    if save_dir:
        os.makedirs(save_dir, exist_ok=True)
    
    # è®­ç»ƒå¾ªç¯
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    best_val_loss = float("inf")
    patience_counter = 0
    
    for epoch in range(1, args.epochs + 1):
        print(f"\nEpoch {epoch}/{args.epochs}")
        print("-" * 40)
        
        # è®­ç»ƒ
        train_loss = train_one_epoch(
            model, train_loader, optimizer, args.mask_ratio, device, args.dry_run
        )
        
        # éªŒè¯
        val_loss = validate(model, val_loader, args.mask_ratio, device, args.dry_run)
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        current_lr = scheduler.get_last_lr()[0]
        
        print(f"Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f} | LR: {current_lr:.2e}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            
            checkpoint = {
                "epoch": epoch,
                "model_state": model.state_dict(),
                "optimizer_state": optimizer.state_dict(),
                "scheduler_state": scheduler.state_dict(),
                "val_loss": val_loss,
                "train_loss": train_loss,
                "args": vars(args),
            }
            torch.save(checkpoint, args.save_path)
            print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°: {args.save_path}")
        else:
            patience_counter += 1
            print(f"   (æ— æ”¹è¿›, patience: {patience_counter}/{args.early_stop})")
        
        # æ—©åœ
        if patience_counter >= args.early_stop:
            print(f"\nâ¹ï¸ æ—©åœ! éªŒè¯æŸå¤± {args.early_stop} è½®æœªæ”¹è¿›")
            break
        
        if args.dry_run:
            print("\nğŸ§ª å¹²è¿è¡Œæ¨¡å¼ï¼Œæå‰é€€å‡º")
            break
    
    print("\n" + "=" * 60)
    print(f"âœ… è®­ç»ƒå®Œæˆ!")
    print(f"   æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
    print(f"   æ¨¡å‹ä¿å­˜è·¯å¾„: {args.save_path}")
    print("=" * 60)


if __name__ == "__main__":
    main()
