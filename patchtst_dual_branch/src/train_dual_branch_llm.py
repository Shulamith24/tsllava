#!/usr/bin/env python3
# SPDX-FileCopyrightText: 2025 Stanford University, ETH Zurich, and the project authors
# SPDX-License-Identifier: MIT

"""
PatchTST + VisionEncoder åŒåˆ†æ”¯ LLM åˆ†ç±»è®­ç»ƒè„šæœ¬

ä½¿ç”¨ LLM (Llama-3.2-1B) è¿›è¡Œæ—¶é—´åºåˆ—åˆ†ç±»ï¼Œæ”¯æŒï¼š
- åŒåˆ†æ”¯èåˆï¼šPatchTST æ—¶åºç¼–ç  + ViT å›¾åƒç¼–ç 
- DDP åˆ†å¸ƒå¼è®­ç»ƒ
- LoRA å¾®è°ƒ
- æ˜¾å­˜ä¼˜åŒ–ï¼šFP16 æ··åˆç²¾åº¦ã€æ¢¯åº¦ç´¯ç§¯ã€æ¢¯åº¦æ£€æŸ¥ç‚¹

ä½¿ç”¨æ–¹æ³•ï¼š
    # å•å¡è®­ç»ƒ
    uv run -m src.train_dual_branch_llm --dataset Adiac --epochs 30 --use_lora

    # å¯ç”¨æ˜¾å­˜ä¼˜åŒ–
    uv run -m src.train_dual_branch_llm --dataset Adiac --fp16 --gradient_accumulation_steps 4 --use_lora

    # å¤šå¡ DDP è®­ç»ƒ
    torchrun --nproc_per_node=2 -m src.train_dual_branch_llm --dataset Adiac --use_ddp --fp16 --use_lora
"""

import os
import sys
import json
import argparse
import datetime
from pathlib import Path
from typing import List, Dict, Any

import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler
from torch.cuda.amp import GradScaler, autocast
from torch.optim import AdamW
from torch.nn.utils import clip_grad_norm_
from tqdm.auto import tqdm
from transformers import get_linear_schedule_with_warmup

from .dual_branch_llm_model import DualBranchLLMModel
from .ucr_llm_dataset import UCRLLMClassificationDataset
from .ucr_dataset import get_dataset_info
from .model_config import PATCH_SIZE


def parse_args():
    parser = argparse.ArgumentParser(description="åŒåˆ†æ”¯ LLM æ—¶åºåˆ†ç±»è®­ç»ƒ")

    # æ•°æ®ç›¸å…³
    parser.add_argument("--dataset", type=str, default="Adiac", help="UCRæ•°æ®é›†åç§°")
    parser.add_argument("--data_path", type=str, default="./data", help="UCRæ•°æ®æ ¹ç›®å½•")
    
    # LLM ç›¸å…³
    parser.add_argument("--llm_id", type=str, default="meta-llama/Llama-3.2-1B", help="LLMæ¨¡å‹ID")
    
    # LoRA ç›¸å…³
    parser.add_argument("--use_lora", action="store_true", help="æ˜¯å¦ä½¿ç”¨LoRA")
    parser.add_argument("--lora_r", type=int, default=16, help="LoRA rank")
    parser.add_argument("--lora_alpha", type=int, default=32, help="LoRA alpha")
    
    # åˆ†æ”¯æ§åˆ¶
    parser.add_argument("--branch_mode", type=str, default="both",
                       choices=["both", "ts_only", "vision_only"],
                       help="åˆ†æ”¯æ¨¡å¼: both(åŒåˆ†æ”¯), ts_only(ä»…æ—¶åº), vision_only(ä»…è§†è§‰)")
    
    # PatchTST æ—¶åºåˆ†æ”¯é…ç½®
    parser.add_argument("--context_length", type=int, default=None,
                       help="ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆNoneåˆ™è‡ªåŠ¨è®¾ç½®ï¼‰")
    parser.add_argument("--patch_length", type=int, default=16, help="Patch é•¿åº¦")
    parser.add_argument("--stride", type=int, default=8, help="Patch æ­¥é•¿")
    parser.add_argument("--d_model", type=int, default=128, help="PatchTST æ¨¡å‹ç»´åº¦")
    parser.add_argument("--num_attention_heads", type=int, default=8, help="PatchTST Attention heads")
    parser.add_argument("--num_hidden_layers", type=int, default=3, help="PatchTST Transformer å±‚æ•°")
    parser.add_argument("--ffn_dim", type=int, default=512, help="PatchTST FFN ç»´åº¦")
    parser.add_argument("--dropout", type=float, default=0.1, help="Dropout")
    
    # Vision åˆ†æ”¯é…ç½®
    parser.add_argument("--vit_model_name", type=str, default="facebook/dinov2-base",
                       help="ViTæ¨¡å‹åç§°")
    parser.add_argument("--vit_layer_idx", type=int, default=-1, help="ViT ç‰¹å¾æå–å±‚ç´¢å¼•")
    parser.add_argument("--vit_patch_size", type=int, default=16, help="æ—¶åºå›¾åƒåŒ– patch å¤§å°")
    parser.add_argument("--vit_stride", type=float, default=0.5, help="æ—¶åºå›¾åƒåŒ–æ­¥é•¿æ¯”ä¾‹")
    
    # æŠ•å½±å±‚é…ç½®
    parser.add_argument("--projector_type", type=str, default="mlp",
                       choices=["mlp", "linear"], help="æŠ•å½±å±‚ç±»å‹")
    parser.add_argument("--projector_dropout", type=float, default=0.1, help="æŠ•å½±å±‚Dropout")
    
    # å†»ç»“é€‰é¡¹
    parser.add_argument("--freeze_ts_backbone", action="store_true", help="å†»ç»“ PatchTST backbone")
    parser.add_argument("--freeze_vision_backbone", action="store_true", default=True,
                       help="å†»ç»“ Vision backboneï¼ˆé»˜è®¤å¼€å¯ï¼‰")
    parser.add_argument("--no_freeze_vision_backbone", action="store_true",
                       help="ä¸å†»ç»“ Vision backbone")
    parser.add_argument("--freeze_encoder", action="store_true", help="å†»ç»“æ‰€æœ‰ç¼–ç å™¨")
    
    # è®­ç»ƒç›¸å…³
    parser.add_argument("--epochs", type=int, default=30, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=4, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--lr_encoder", type=float, default=2e-4, help="ç¼–ç å™¨å­¦ä¹ ç‡")
    parser.add_argument("--lr_projector", type=float, default=1e-4, help="æŠ•å½±å±‚å­¦ä¹ ç‡")
    parser.add_argument("--lr_lora", type=float, default=1e-4, help="LoRAå­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=1e-2, help="æƒé‡è¡°å‡")
    parser.add_argument("--grad_clip", type=float, default=1.0, help="æ¢¯åº¦è£å‰ª")
    parser.add_argument("--warmup_ratio", type=float, default=0.03, help="é¢„çƒ­æ¯”ä¾‹")
    
    # DDP åˆ†å¸ƒå¼è®­ç»ƒ
    parser.add_argument("--use_ddp", action="store_true", help="å¯ç”¨ DDP åˆ†å¸ƒå¼è®­ç»ƒ")
    parser.add_argument("--local_rank", type=int, default=-1, help="DDP local rank")
    
    # æ˜¾å­˜ä¼˜åŒ–
    parser.add_argument("--fp16", action="store_true", help="å¯ç”¨ FP16 æ··åˆç²¾åº¦")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1,
                       help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    parser.add_argument("--gradient_checkpointing", action="store_true",
                       help="å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
    
    # ä¿å­˜ç›¸å…³
    parser.add_argument("--save_dir", type=str, default="results/dual_branch_llm",
                       help="ç»“æœä¿å­˜ç›®å½•")
    
    # å…¶ä»–
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--device", type=str, default="cuda", help="è®¾å¤‡")
    parser.add_argument("--eval_every", type=int, default=5, help="æ¯Nè½®è¯„ä¼°ä¸€æ¬¡")
    parser.add_argument("--early_stop", type=int, default=10, help="æ—©åœè€å¿ƒå€¼")
    parser.add_argument("--max_new_tokens", type=int, default=10, help="ç”Ÿæˆæœ€å¤§tokenæ•°")
    parser.add_argument("--eval_batch_size", type=int, default=8, help="è¯„ä¼°æ‰¹æ¬¡å¤§å°")
    
    args = parser.parse_args()
    
    # å¤„ç†å†»ç»“é€‰é¡¹å†²çª
    if args.no_freeze_vision_backbone:
        args.freeze_vision_backbone = False
    
    return args


def set_seed(seed: int, rank: int = 0):
    """è®¾ç½®éšæœºç§å­"""
    seed = seed + rank
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    import random
    import numpy as np
    random.seed(seed)
    np.random.seed(seed)


def setup_ddp(args):
    """åˆå§‹åŒ– DDP"""
    if args.use_ddp:
        if args.local_rank == -1:
            args.local_rank = int(os.environ.get("LOCAL_RANK", 0))
        
        dist.init_process_group(backend="nccl")
        torch.cuda.set_device(args.local_rank)
        args.device = f"cuda:{args.local_rank}"
        args.world_size = dist.get_world_size()
        args.rank = dist.get_rank()
        
        if args.rank == 0:
            print(f"ğŸŒ DDP åˆå§‹åŒ–å®Œæˆï¼Œworld_size={args.world_size}")
    else:
        args.world_size = 1
        args.rank = 0


def cleanup_ddp(args):
    """æ¸…ç† DDP"""
    if args.use_ddp:
        dist.destroy_process_group()


def is_main_process(args):
    """åˆ¤æ–­æ˜¯å¦ä¸ºä¸»è¿›ç¨‹"""
    return args.rank == 0


def get_model(model):
    """è·å–åº•å±‚æ¨¡å‹ï¼ˆå…¼å®¹DDPåŒ…è£…ï¼‰"""
    return model.module if hasattr(model, "module") else model


def collate_fn(batch):
    """Collate å‡½æ•°ï¼šå¤„ç†å˜é•¿æ—¶é—´åºåˆ—"""
    # ç›´æ¥è¿”å› batchï¼Œåœ¨æ¨¡å‹å†…éƒ¨å¤„ç† padding
    return batch


def calculate_accuracy(predictions: List[str], labels: List[str]) -> float:
    """
    è®¡ç®—åˆ†ç±»å‡†ç¡®ç‡
    
    å¯¹ç”Ÿæˆæ–‡æœ¬è¿›è¡Œåå¤„ç†ï¼Œæå–é¢„æµ‹æ ‡ç­¾å¹¶ä¸çœŸå®æ ‡ç­¾æ¯”è¾ƒ
    """
    correct = 0
    for pred, label in zip(predictions, labels):
        pred_clean = pred.strip()
        
        # å°è¯•æå– <cN> æ ¼å¼çš„æ ‡ç­¾
        pred_label = None
        
        # 1. æŸ¥æ‰¾ <cN> æ¨¡å¼
        import re
        match = re.search(r'<c(\d+)>', pred_clean)
        if match:
            pred_label = f"<c{match.group(1)}>"
        # 2. å¦‚æœé¢„æµ‹å°±æ˜¯å•ä¸ªå­—æ¯
        elif len(pred_clean) == 1 and pred_clean.isalpha():
            pred_label = pred_clean.upper()
        # 3. å–æœ€åä¸€ä¸ª <cN> æˆ–å­—æ¯
        elif pred_clean:
            words = pred_clean.split()
            if words:
                last_word = words[-1].strip(".,!?:;")
                if last_word.startswith("<c") and last_word.endswith(">"):
                    pred_label = last_word
                elif last_word and last_word[0].isalpha():
                    pred_label = last_word[0].upper()
        
        # æ¯”è¾ƒ
        label_clean = label.strip()
        if pred_label == label_clean:
            correct += 1
    
    return correct / len(predictions) if predictions else 0.0


def create_data_loaders(args, eos_token: str):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    train_dataset = UCRLLMClassificationDataset(
        split="train",
        EOS_TOKEN=eos_token,
        dataset_name=args.dataset,
        raw_data_path=args.data_path,
    )
    
    val_dataset = UCRLLMClassificationDataset(
        split="validation",
        EOS_TOKEN=eos_token,
        dataset_name=args.dataset,
        raw_data_path=args.data_path,
    )
    
    test_dataset = UCRLLMClassificationDataset(
        split="test",
        EOS_TOKEN=eos_token,
        dataset_name=args.dataset,
        raw_data_path=args.data_path,
    )
    
    # DDP é‡‡æ ·å™¨
    train_sampler = DistributedSampler(train_dataset, shuffle=True) if args.use_ddp else None
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=(train_sampler is None),
        sampler=train_sampler,
        collate_fn=collate_fn,
        num_workers=0,
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.eval_batch_size,
        shuffle=False,
        collate_fn=collate_fn,
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=args.eval_batch_size,
        shuffle=False,
        collate_fn=collate_fn,
    )
    
    return train_loader, val_loader, test_loader, train_sampler


def train_one_epoch(
    model,
    train_loader: DataLoader,
    optimizer,
    scheduler,
    grad_clip: float,
    epoch: int,
    num_epochs: int,
    args,
    scaler: GradScaler = None,
) -> float:
    """è®­ç»ƒä¸€ä¸ª epoch"""
    model.train()
    total_loss = 0.0
    num_batches = 0
    optimizer.zero_grad()
    
    # DDP è®¾ç½® epoch
    if args.use_ddp and hasattr(train_loader, 'sampler') and train_loader.sampler is not None:
        train_loader.sampler.set_epoch(epoch)
    
    pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{num_epochs}", disable=not is_main_process(args))
    
    for step, batch in enumerate(pbar):
        # æ··åˆç²¾åº¦è®­ç»ƒ
        if args.fp16:
            with autocast():
                loss = model(batch)
                loss = loss / args.gradient_accumulation_steps
            scaler.scale(loss).backward()
        else:
            loss = model(batch)
            loss = loss / args.gradient_accumulation_steps
            loss.backward()
        
        # æ¢¯åº¦ç´¯ç§¯
        if (step + 1) % args.gradient_accumulation_steps == 0:
            if args.fp16:
                scaler.unscale_(optimizer)
                clip_grad_norm_(model.parameters(), max_norm=grad_clip)
                scaler.step(optimizer)
                scaler.update()
            else:
                clip_grad_norm_(model.parameters(), max_norm=grad_clip)
                optimizer.step()
            
            scheduler.step()
            optimizer.zero_grad()
        
        total_loss += loss.item() * args.gradient_accumulation_steps
        num_batches += 1
        
        if is_main_process(args):
            pbar.set_postfix({
                "loss": f"{loss.item() * args.gradient_accumulation_steps:.4f}",
                "lr": f"{scheduler.get_last_lr()[0]:.2e}"
            })
    
    # å¤„ç†å‰©ä½™æ¢¯åº¦
    if num_batches % args.gradient_accumulation_steps != 0:
        if args.fp16:
            scaler.unscale_(optimizer)
            clip_grad_norm_(model.parameters(), max_norm=grad_clip)
            scaler.step(optimizer)
            scaler.update()
        else:
            clip_grad_norm_(model.parameters(), max_norm=grad_clip)
            optimizer.step()
        scheduler.step()
        optimizer.zero_grad()
    
    return total_loss / max(num_batches, 1)


@torch.no_grad()
def evaluate(
    model,
    data_loader: DataLoader,
    max_new_tokens: int,
    args,
    desc: str = "Evaluating",
) -> Dict[str, Any]:
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    underlying_model = get_model(model)
    
    all_predictions = []
    all_labels = []
    total_loss = 0.0
    num_batches = 0
    
    for batch in tqdm(data_loader, desc=desc, disable=not is_main_process(args)):
        # è®¡ç®—æŸå¤±
        loss = underlying_model.compute_loss(batch)
        total_loss += loss.item()
        num_batches += 1
        
        # ç”Ÿæˆé¢„æµ‹
        predictions = underlying_model.generate(batch, max_new_tokens=max_new_tokens)
        
        # æ”¶é›†ç»“æœ
        for sample, pred in zip(batch, predictions):
            all_predictions.append(pred)
            all_labels.append(sample["class_token"])
    
    # è®¡ç®—æŒ‡æ ‡
    avg_loss = total_loss / max(num_batches, 1)
    accuracy = calculate_accuracy(all_predictions, all_labels)
    
    return {
        "loss": avg_loss,
        "accuracy": accuracy,
        "predictions": all_predictions,
        "labels": all_labels,
    }


def main():
    args = parse_args()
    
    # åˆå§‹åŒ– DDP
    setup_ddp(args)
    
    if is_main_process(args):
        print("=" * 60)
        print("åŒåˆ†æ”¯ LLM æ—¶åºåˆ†ç±»è®­ç»ƒ")
        print("=" * 60)
        print(f"æ—¶é—´: {datetime.datetime.now()}")
        print(f"æ•°æ®é›†: {args.dataset}")
        print(f"åˆ†æ”¯æ¨¡å¼: {args.branch_mode}")
        print(f"LLM: {args.llm_id}")
        print(f"LoRA: {args.use_lora}")
        print(f"DDP: {args.use_ddp}, FP16: {args.fp16}")
        print(f"æ¢¯åº¦ç´¯ç§¯: {args.gradient_accumulation_steps}")
        print("=" * 60)
    
    set_seed(args.seed, args.rank)
    
    device = args.device if torch.cuda.is_available() else "cpu"
    if is_main_process(args):
        print(f"\nä½¿ç”¨è®¾å¤‡: {device}")
    
    # è·å–æ•°æ®é›†ä¿¡æ¯
    if is_main_process(args):
        print("\nğŸ“‚ åˆ†ææ•°æ®é›†...")
    num_classes, max_length = get_dataset_info(args.dataset, args.data_path)
    
    # è®¾ç½® context_length
    if args.context_length is None:
        context_length = ((max_length - 1) // args.patch_length + 1) * args.patch_length
    else:
        context_length = args.context_length
    
    if is_main_process(args):
        print(f"   ç±»åˆ«æ•°: {num_classes}")
        print(f"   æœ€å¤§é•¿åº¦: {max_length}")
        print(f"   Context length: {context_length}")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    vit_short_name = args.vit_model_name.split("/")[-1].replace("-", "_")
    save_subdir = f"{args.branch_mode}_{vit_short_name}"
    if args.use_lora:
        save_subdir += "_lora"
    
    save_dir = os.path.join(args.save_dir, args.dataset, save_subdir)
    
    if is_main_process(args):
        os.makedirs(save_dir, exist_ok=True)
        with open(os.path.join(save_dir, "config.json"), "w") as f:
            json.dump(vars(args), f, indent=2)
    
    # åˆ›å»ºæ¨¡å‹
    if is_main_process(args):
        print("\nğŸ”§ åˆ›å»ºæ¨¡å‹...")
    
    model = DualBranchLLMModel(
        llm_id=args.llm_id,
        branch_mode=args.branch_mode,
        context_length=context_length,
        patch_length=args.patch_length,
        stride=args.stride,
        d_model=args.d_model,
        num_attention_heads=args.num_attention_heads,
        num_hidden_layers=args.num_hidden_layers,
        ffn_dim=args.ffn_dim,
        dropout=args.dropout,
        vit_model_name=args.vit_model_name,
        vit_layer_idx=args.vit_layer_idx,
        vit_patch_size=args.vit_patch_size,
        vit_stride=args.vit_stride,
        projector_type=args.projector_type,
        projector_dropout=args.projector_dropout,
        freeze_ts_backbone=args.freeze_ts_backbone or args.freeze_encoder,
        freeze_vision_backbone=args.freeze_vision_backbone or args.freeze_encoder,
        device=device,
    )
    
    # æ¢¯åº¦æ£€æŸ¥ç‚¹
    if args.gradient_checkpointing:
        model.enable_gradient_checkpointing()
    
    # å¯ç”¨ LoRA
    if args.use_lora:
        if is_main_process(args):
            print("ğŸ“ å¯ç”¨LoRA...")
        model.enable_lora(lora_r=args.lora_r, lora_alpha=args.lora_alpha)
    
    # DDP åŒ…è£…
    if args.use_ddp:
        model = DDP(model, device_ids=[args.local_rank], find_unused_parameters=True)
        if is_main_process(args):
            print(f"âœ… æ¨¡å‹å·²ç”¨DDPåŒ…è£…")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    if is_main_process(args):
        print("\nğŸ“‚ åŠ è½½æ•°æ®...")
    eos_token = get_model(model).get_eos_token()
    train_loader, val_loader, test_loader, train_sampler = create_data_loaders(args, eos_token)
    
    if is_main_process(args):
        print(f"   Train batches: {len(train_loader)}")
        print(f"   Val batches: {len(val_loader)}")
        print(f"   Test batches: {len(test_loader)}")
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    if is_main_process(args):
        print("\nâš™ï¸ åˆ›å»ºä¼˜åŒ–å™¨...")
    underlying_model = get_model(model)
    
    param_groups = []
    
    # ç¼–ç å™¨å‚æ•°
    if not args.freeze_encoder:
        if underlying_model.ts_backbone is not None:
            param_groups.append({
                "params": underlying_model.ts_backbone.parameters(),
                "lr": args.lr_encoder
            })
        if underlying_model.vision_encoder is not None:
            param_groups.append({
                "params": underlying_model.vision_encoder.parameters(),
                "lr": args.lr_encoder
            })
    
    # æŠ•å½±å™¨å‚æ•°
    if underlying_model.ts_projector is not None:
        param_groups.append({
            "params": underlying_model.ts_projector.parameters(),
            "lr": args.lr_projector
        })
    if underlying_model.vision_projector is not None:
        param_groups.append({
            "params": underlying_model.vision_projector.parameters(),
            "lr": args.lr_projector
        })
    
    # LoRA å‚æ•°
    if args.use_lora:
        lora_params = underlying_model.get_lora_parameters()
        if lora_params:
            param_groups.append({"params": lora_params, "lr": args.lr_lora})
    
    optimizer = AdamW(param_groups, weight_decay=args.weight_decay)
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    import math
    steps_per_epoch = max(1, math.ceil(len(train_loader) / args.gradient_accumulation_steps))
    total_steps = args.epochs * steps_per_epoch
    warmup_steps = int(args.warmup_ratio * total_steps)
    
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_steps,
    )
    
    if is_main_process(args):
        print(f"   Total steps: {total_steps}")
        print(f"   Warmup steps: {warmup_steps}")
    
    # FP16 æ··åˆç²¾åº¦
    scaler = GradScaler() if args.fp16 else None
    if args.fp16 and is_main_process(args):
        print("âš¡ FP16 æ··åˆç²¾åº¦å·²å¯ç”¨")
    
    # è®­ç»ƒå¾ªç¯
    if is_main_process(args):
        print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    
    best_val_acc = 0.0
    patience_counter = 0
    loss_history = []
    epoch = 0
    
    try:
        for epoch in range(1, args.epochs + 1):
            train_loss = train_one_epoch(
                model, train_loader, optimizer, scheduler,
                args.grad_clip, epoch, args.epochs, args, scaler
            )
            
            if epoch % args.eval_every == 0 or epoch == args.epochs:
                if is_main_process(args):
                    print(f"\nğŸ“Š Epoch {epoch} è¯„ä¼°...")
                
                val_results = evaluate(
                    model, val_loader, args.max_new_tokens, args, "Validating"
                )
                val_loss = val_results["loss"]
                val_acc = val_results["accuracy"]
                
                if is_main_process(args):
                    print(f"   Train Loss: {train_loss:.4f}")
                    print(f"   Val Loss: {val_loss:.4f}")
                    print(f"   Val Accuracy: {val_acc:.4f}")
                    
                    # æ˜¾ç¤ºé¢„æµ‹æ ·æœ¬
                    print("   Sample predictions:")
                    for i in range(min(3, len(val_results["predictions"]))):
                        pred = val_results["predictions"][i]
                        label = val_results["labels"][i]
                        pred_short = pred[-50:] if len(pred) > 50 else pred
                        print(f"     Pred: '{pred_short}' | Label: '{label}'")
                    
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    if val_acc > best_val_acc:
                        best_val_acc = val_acc
                        patience_counter = 0
                        
                        checkpoint = {
                            "ts_backbone_state": underlying_model.ts_backbone.state_dict() if underlying_model.ts_backbone else None,
                            "vision_encoder_state": underlying_model.vision_encoder.state_dict() if underlying_model.vision_encoder else None,
                            "ts_projector_state": underlying_model.ts_projector.state_dict() if underlying_model.ts_projector else None,
                            "vision_projector_state": underlying_model.vision_projector.state_dict() if underlying_model.vision_projector else None,
                            "optimizer_state": optimizer.state_dict(),
                            "scheduler_state": scheduler.state_dict(),
                            "epoch": epoch,
                            "val_loss": val_loss,
                            "val_acc": val_acc,
                            "args": vars(args),
                        }
                        underlying_model.save_lora_state_to_checkpoint(checkpoint)
                        
                        torch.save(checkpoint, os.path.join(save_dir, "best_model.pt"))
                        print(f"   ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹")
                    else:
                        patience_counter += 1
                        print(f"   (æ— æ”¹è¿›, patience: {patience_counter}/{args.early_stop})")
                    
                    loss_history.append({
                        "epoch": epoch,
                        "train_loss": train_loss,
                        "val_loss": val_loss,
                        "val_acc": val_acc,
                    })
                    with open(os.path.join(save_dir, "loss_history.json"), "w") as f:
                        json.dump(loss_history, f, indent=2)
            else:
                if is_main_process(args):
                    print(f"Epoch {epoch}: Train Loss = {train_loss:.4f}")
            
            if patience_counter >= args.early_stop:
                if is_main_process(args):
                    print(f"\nâ¹ï¸ æ—©åœ! éªŒè¯å‡†ç¡®ç‡ {args.early_stop} è½®æœªæ”¹è¿›")
                break
        
        # æœ€ç»ˆæµ‹è¯•
        if is_main_process(args):
            print("\n" + "=" * 60)
            print("ğŸ“‹ æœ€ç»ˆæµ‹è¯•è¯„ä¼°...")
            
            # åŠ è½½æœ€ä½³æ¨¡å‹
            best_ckpt = torch.load(
                os.path.join(save_dir, "best_model.pt"),
                map_location=device,
                weights_only=False
            )
            
            if best_ckpt.get("ts_backbone_state") and underlying_model.ts_backbone:
                underlying_model.ts_backbone.load_state_dict(best_ckpt["ts_backbone_state"])
            if best_ckpt.get("ts_projector_state") and underlying_model.ts_projector:
                underlying_model.ts_projector.load_state_dict(best_ckpt["ts_projector_state"])
            if best_ckpt.get("vision_projector_state") and underlying_model.vision_projector:
                underlying_model.vision_projector.load_state_dict(best_ckpt["vision_projector_state"])
            underlying_model.load_lora_state_from_checkpoint(best_ckpt, allow_missing=True)
            
            test_results = evaluate(
                model, test_loader, args.max_new_tokens, args, "Testing"
            )
            
            print(f"\nâœ… æµ‹è¯•ç»“æœ:")
            print(f"   Test Loss: {test_results['loss']:.4f}")
            print(f"   Test Accuracy: {test_results['accuracy']:.4f}")
            
            final_results = {
                "dataset": args.dataset,
                "num_classes": num_classes,
                "context_length": context_length,
                "branch_mode": args.branch_mode,
                "vit_model_name": args.vit_model_name,
                "use_lora": args.use_lora,
                "best_val_acc": best_val_acc,
                "test_loss": test_results["loss"],
                "test_accuracy": test_results["accuracy"],
                "epochs_trained": epoch,
            }
            
            with open(os.path.join(save_dir, "final_results.json"), "w") as f:
                json.dump(final_results, f, indent=2)
            
            with open(os.path.join(save_dir, "test_predictions.json"), "w") as f:
                json.dump({
                    "predictions": test_results["predictions"],
                    "labels": test_results["labels"],
                }, f, indent=2)
            
            print("=" * 60)
            print(f"ç»“æœä¿å­˜åˆ°: {save_dir}")
            print("=" * 60)
    
    except KeyboardInterrupt:
        if is_main_process(args):
            print("\nâš ï¸ è®­ç»ƒè¢«ä¸­æ–­")
    except Exception as e:
        if is_main_process(args):
            print(f"\nâŒ é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
        cleanup_ddp(args)
        return 1
    
    cleanup_ddp(args)
    return 0


if __name__ == "__main__":
    exit(main())
