# SPDX-FileCopyrightText: 2025 Stanford University, ETH Zurich, and the project authors (see CONTRIBUTORS.md)
# SPDX-FileCopyrightText: 2025 This source file is part of the OpenTSLM open-source project.
#
# SPDX-License-Identifier: MIT

"""
UCRå¤šæ•°æ®é›†é¢„è®­ç»ƒåŠ è½½å™¨

ç”¨äºç»„åˆå¤šä¸ªUCRæ•°æ®é›†è¿›è¡ŒTSLANetç¼–ç å™¨é¢„è®­ç»ƒã€‚
ä»…åŠ è½½æ—¶é—´åºåˆ—æ•°æ®ï¼ˆæ— ç›‘ç£é¢„è®­ç»ƒï¼Œä¸ä½¿ç”¨æ ‡ç­¾ï¼‰ã€‚
"""

import os
from typing import List, Optional, Literal
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader, ConcatDataset

from opentslm.time_series_datasets.ucr.ucr_loader import (
    ensure_ucr_data,
    load_ucr_dataset,
    UCR_DIR,
)


def load_dataset_list(file_path: str) -> List[str]:
    """
    ä»æ–‡ä»¶åŠ è½½æ•°æ®é›†åç§°åˆ—è¡¨
    
    Args:
        file_path: åŒ…å«æ•°æ®é›†åç§°çš„æ–‡æœ¬æ–‡ä»¶è·¯å¾„
    
    Returns:
        æ•°æ®é›†åç§°åˆ—è¡¨
    """
    datasets = []
    with open(file_path, 'r') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#'):
                datasets.append(line)
    return datasets


class UCRPretrainDataset(Dataset):
    """
    å•ä¸ªUCRæ•°æ®é›†çš„é¢„è®­ç»ƒDataset
    
    ä»…è¿”å›å½’ä¸€åŒ–çš„æ—¶é—´åºåˆ—æ•°æ®ï¼ˆç”¨äºæ— ç›‘ç£é¢„è®­ç»ƒï¼‰
    """
    
    def __init__(
        self,
        df: pd.DataFrame,
        feature_cols: Optional[List[str]] = None,
        label_col: str = "label",
        patch_size: int = 8,
    ):
        super().__init__()
        self.df = df.reset_index(drop=True)
        self.feature_cols = feature_cols or [c for c in df.columns if c != label_col]
        self.patch_size = patch_size
    
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        feats = row[self.feature_cols].astype(float).values
        tensor = torch.tensor(feats, dtype=torch.float32)
        
        # å¤„ç†NaNå€¼
        tensor = torch.nan_to_num(tensor, nan=0.0)
        
        # Per-sample z-normalization
        mean = tensor.mean()
        std = tensor.std()
        if std > 1e-8:
            tensor = (tensor - mean) / std
        else:
            tensor = tensor - mean
        
        return tensor


def collate_fn_pretrain(batch: List[torch.Tensor], patch_size: int = 8):
    """
    é¢„è®­ç»ƒæ‰¹æ¬¡collateå‡½æ•°
    
    å°†ä¸åŒé•¿åº¦çš„åºåˆ—å¡«å……åˆ°ç›¸åŒé•¿åº¦ï¼ˆpatch_sizeçš„å€æ•°ï¼‰
    """
    # æ‰¾åˆ°æœ€å¤§é•¿åº¦
    max_len = max(x.shape[0] for x in batch)
    
    # å¡«å……åˆ°patch_sizeçš„å€æ•°
    if max_len % patch_size != 0:
        max_len = max_len + (patch_size - max_len % patch_size)
    
    # å¡«å……
    padded = []
    for x in batch:
        if x.shape[0] < max_len:
            pad_len = max_len - x.shape[0]
            x = torch.nn.functional.pad(x, (0, pad_len))
        padded.append(x)
    
    return torch.stack(padded)


class UCRMultiDatasetForPretrain(Dataset):
    """
    ç»„åˆå¤šä¸ªUCRæ•°æ®é›†ç”¨äºç¼–ç å™¨é¢„è®­ç»ƒ
    
    Args:
        dataset_names: æ•°æ®é›†åç§°åˆ—è¡¨
        split: "train", "test", æˆ– "all"
        raw_data_path: UCRæ•°æ®è·¯å¾„
        patch_size: patchå¤§å°
    """
    
    def __init__(
        self,
        dataset_names: List[str],
        split: Literal["train", "test", "all"] = "train",
        raw_data_path: str = "./data",
        patch_size: int = 8,
    ):
        super().__init__()
        self.patch_size = patch_size
        self.dataset_names = dataset_names
        
        # ç¡®ä¿æ•°æ®å·²ä¸‹è½½
        ensure_ucr_data()
        
        # åŠ è½½æ‰€æœ‰æ•°æ®é›†
        all_datasets = []
        total_samples = 0
        
        for name in dataset_names:
            try:
                train_df, test_df = load_ucr_dataset(name, raw_data_path=raw_data_path)
                
                if split == "train":
                    df = train_df
                elif split == "test":
                    df = test_df
                else:
                    df = pd.concat([train_df, test_df], ignore_index=True)
                
                dataset = UCRPretrainDataset(df, patch_size=patch_size)
                all_datasets.append(dataset)
                total_samples += len(dataset)
                
            except Exception as e:
                print(f"âš ï¸ åŠ è½½æ•°æ®é›† {name} å¤±è´¥: {e}")
                continue
        
        if not all_datasets:
            raise ValueError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ•°æ®é›†ï¼")
        
        self.combined_dataset = ConcatDataset(all_datasets)
        print(f"âœ… åŠ è½½äº† {len(all_datasets)} ä¸ªæ•°æ®é›†ï¼Œå…± {total_samples} ä¸ªæ ·æœ¬")
    
    def __len__(self):
        return len(self.combined_dataset)
    
    def __getitem__(self, idx):
        return self.combined_dataset[idx]


def get_ucr_pretrain_loader(
    dataset_list_file: str,
    split: Literal["train", "test", "all"] = "train",
    batch_size: int = 64,
    shuffle: bool = True,
    raw_data_path: str = "./data",
    patch_size: int = 8,
    num_workers: int = 0,
) -> DataLoader:
    """
    è·å–UCRé¢„è®­ç»ƒDataLoader
    
    Args:
        dataset_list_file: æ•°æ®é›†åˆ—è¡¨æ–‡ä»¶è·¯å¾„
        split: æ•°æ®åˆ’åˆ†
        batch_size: æ‰¹æ¬¡å¤§å°
        shuffle: æ˜¯å¦æ‰“ä¹±
        raw_data_path: æ•°æ®è·¯å¾„
        patch_size: patchå¤§å°
        num_workers: æ•°æ®åŠ è½½çº¿ç¨‹æ•°
    
    Returns:
        DataLoader
    """
    # åŠ è½½æ•°æ®é›†åˆ—è¡¨
    dataset_names = load_dataset_list(dataset_list_file)
    print(f"ğŸ“‚ ä» {dataset_list_file} åŠ è½½ {len(dataset_names)} ä¸ªæ•°æ®é›†")
    
    # åˆ›å»ºç»„åˆæ•°æ®é›†
    dataset = UCRMultiDatasetForPretrain(
        dataset_names=dataset_names,
        split=split,
        raw_data_path=raw_data_path,
        patch_size=patch_size,
    )
    
    # åˆ›å»ºDataLoader
    loader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        collate_fn=lambda batch: collate_fn_pretrain(batch, patch_size=patch_size),
        num_workers=num_workers,
        pin_memory=True,
    )
    
    return loader


# æµ‹è¯•
if __name__ == "__main__":
    # è·å–è„šæœ¬ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    train_list = os.path.join(script_dir, "ucr_train_98_datasets.txt")
    
    if os.path.exists(train_list):
        loader = get_ucr_pretrain_loader(
            dataset_list_file=train_list,
            split="train",
            batch_size=32,
            patch_size=8,
        )
        
        batch = next(iter(loader))
        print(f"Batch shape: {batch.shape}")
        print(f"Batch mean: {batch.mean():.4f}, std: {batch.std():.4f}")
    else:
        print(f"æ•°æ®é›†åˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {train_list}")
