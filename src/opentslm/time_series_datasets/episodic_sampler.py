# SPDX-FileCopyrightText: 2025 Stanford University, ETH Zurich, and the project authors
# SPDX-License-Identifier: MIT

"""
Episodic Batch Sampler

ä¸€ä¸ªbatchä¸€ä¸ªdatasetï¼Œä½¿ç”¨æ¸©åº¦é‡‡æ ·å¹³è¡¡å¤§å°å·®å¼‚ï¼š
p(i) âˆ n_i^Î±ï¼Œå…¶ä¸­ n_i æ˜¯æ•°æ®é›† i çš„æ ·æœ¬æ•°

Args:
    alpha: æ¸©åº¦é‡‡æ ·å‚æ•°
           Î±=0: å®Œå…¨å‡åŒ€ï¼ˆå°æ•°æ®é›†ä¸ä¼šè¢«æ·¹æ²¡ï¼‰
           Î±=0.3~0.5: å¸¸ç”¨æŠ˜ä¸­
           Î±=1: æŒ‰æ•°æ®é‡ï¼ˆå¤§æ•°æ®é›†ä¸»å¯¼ï¼‰
"""

import math
import random
from typing import Iterator, List, Dict, Optional

from torch.utils.data import Sampler

from .multi_dataset import UnifiedPrototypeDataset


class EpisodicBatchSampler(Sampler):
    """
    Episodicé‡‡æ ·å™¨ï¼šä¸€ä¸ªbatchä¸€ä¸ªdataset
    
    ä¸¤å±‚é‡‡æ ·ï¼š
    1. å…ˆé‡‡ dataset i ~ p(i)ï¼Œå…¶ä¸­ p(i) âˆ n_i^Î±
    2. å†ä» dataset i éšæœºé‡‡ä¸€ä¸ª batch
    
    Args:
        dataset: UnifiedPrototypeDataset
        batch_size: æ¯ä¸ªbatchçš„æ ·æœ¬æ•°
        alpha: æ¸©åº¦é‡‡æ ·å‚æ•°ï¼Œé»˜è®¤0.4
        num_episodes: æ¯ä¸ªepochçš„episodeæ•°ï¼ˆé»˜è®¤è‡ªåŠ¨è®¡ç®—ï¼‰
        shuffle: æ˜¯å¦åœ¨æ¯ä¸ªepochå¼€å§‹æ—¶æ‰“ä¹±
        drop_last: æ˜¯å¦ä¸¢å¼ƒæœ€åä¸è¶³batch_sizeçš„batch
    """
    
    def __init__(
        self,
        dataset: UnifiedPrototypeDataset,
        batch_size: int,
        alpha: float = 0.4,
        num_episodes: Optional[int] = None,
        shuffle: bool = True,
        drop_last: bool = False,
    ):
        self.dataset = dataset
        self.batch_size = batch_size
        self.alpha = alpha
        self.shuffle = shuffle
        self.drop_last = drop_last
        
        # è·å–æ¯ä¸ªæ•°æ®é›†çš„æ ·æœ¬ç´¢å¼•
        self.ds_ids = dataset.get_all_ds_ids()
        self.ds_indices: Dict[int, List[int]] = {
            ds_id: dataset.get_indices_for_dataset(ds_id)
            for ds_id in self.ds_ids
        }
        
        # è®¡ç®—é‡‡æ ·æ¦‚ç‡
        sample_counts = {ds_id: len(indices) for ds_id, indices in self.ds_indices.items()}
        self._sample_counts = sample_counts
        
        # p(i) âˆ n_i^Î±
        weights = {ds_id: math.pow(count, alpha) for ds_id, count in sample_counts.items()}
        total_weight = sum(weights.values())
        self.ds_probs = {ds_id: w / total_weight for ds_id, w in weights.items()}
        
        # æ‰“å°é‡‡æ ·æ¦‚ç‡
        print(f"ğŸ“Š EpisodicBatchSampler (Î±={alpha}):")
        for ds_id in self.ds_ids:
            ds_name = dataset.registry.get_dataset_info(ds_id).name
            prob = self.ds_probs[ds_id]
            count = sample_counts[ds_id]
            print(f"   [{ds_id}] {ds_name}: {count} samples, p={prob:.3f}")
        
        # è®¡ç®—episodeæ•°ï¼ˆé»˜è®¤ï¼šæ€»æ ·æœ¬æ•° / batch_sizeï¼‰
        if num_episodes is None:
            self.num_episodes = max(1, len(dataset) // batch_size)
        else:
            self.num_episodes = num_episodes
        
        print(f"   Episodes per epoch: {self.num_episodes}")
        
        # å†…éƒ¨çŠ¶æ€
        self._ds_id_list = list(self.ds_ids)
        self._prob_list = [self.ds_probs[ds_id] for ds_id in self._ds_id_list]
    
    def __iter__(self) -> Iterator[List[int]]:
        """ç”Ÿæˆbatchç´¢å¼•"""
        # æ¯ä¸ªæ•°æ®é›†ç»´æŠ¤ä¸€ä¸ªæ‰“ä¹±çš„ç´¢å¼•é˜Ÿåˆ—
        ds_queues: Dict[int, List[int]] = {}
        
        if self.shuffle:
            for ds_id, indices in self.ds_indices.items():
                shuffled = indices.copy()
                random.shuffle(shuffled)
                ds_queues[ds_id] = shuffled
        else:
            ds_queues = {ds_id: indices.copy() for ds_id, indices in self.ds_indices.items()}
        
        for _ in range(self.num_episodes):
            # 1. é‡‡æ ·æ•°æ®é›†
            ds_id = random.choices(self._ds_id_list, weights=self._prob_list, k=1)[0]
            
            # 2. ä»è¯¥æ•°æ®é›†é‡‡æ ·batch
            queue = ds_queues[ds_id]
            
            # å¦‚æœé˜Ÿåˆ—ä¸è¶³ï¼Œé‡æ–°å¡«å……
            if len(queue) < self.batch_size:
                new_indices = self.ds_indices[ds_id].copy()
                if self.shuffle:
                    random.shuffle(new_indices)
                queue.extend(new_indices)
            
            # å–batch
            batch = queue[:self.batch_size]
            ds_queues[ds_id] = queue[self.batch_size:]
            
            if self.drop_last and len(batch) < self.batch_size:
                continue
            
            yield batch
    
    def __len__(self) -> int:
        return self.num_episodes
    
    def get_dataset_sampling_stats(self) -> Dict[str, float]:
        """è·å–æ•°æ®é›†é‡‡æ ·ç»Ÿè®¡ï¼ˆç”¨äºåˆ†æï¼‰"""
        return {
            self.dataset.registry.get_dataset_info(ds_id).name: prob
            for ds_id, prob in self.ds_probs.items()
        }


# æµ‹è¯•
if __name__ == "__main__":
    from pathlib import Path
    import sys
    sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent / "src"))
    
    from opentslm.time_series_datasets.multi_dataset import (
        MultiDatasetRegistry, 
        UnifiedPrototypeDataset
    )
    
    print("Testing EpisodicBatchSampler...")
    
    registry = MultiDatasetRegistry(data_path="./data")
    registry.register("ECG200")
    registry.register("Coffee")
    
    dataset = UnifiedPrototypeDataset(registry, split="train")
    sampler = EpisodicBatchSampler(dataset, batch_size=8, alpha=0.4, num_episodes=10)
    
    print("\nSampling 5 episodes:")
    ds_count = {}
    for i, batch_indices in enumerate(sampler):
        if i >= 5:
            break
        # æ£€æŸ¥åŒä¸€batchæ˜¯å¦æ¥è‡ªåŒä¸€æ•°æ®é›†
        ds_ids = set(dataset[idx]["ds_id"] for idx in batch_indices)
        assert len(ds_ids) == 1, "Batch should contain samples from single dataset"
        ds_id = list(ds_ids)[0]
        ds_name = registry.get_dataset_info(ds_id).name
        ds_count[ds_name] = ds_count.get(ds_name, 0) + 1
        print(f"Episode {i}: {len(batch_indices)} samples from {ds_name}")
    
    print(f"\nDataset distribution: {ds_count}")
