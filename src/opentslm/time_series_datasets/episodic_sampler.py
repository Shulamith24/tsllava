# SPDX-FileCopyrightText: 2025 Stanford University, ETH Zurich, and the project authors
# SPDX-License-Identifier: MIT

"""
Episodic Batch Sampler

ä¸€ä¸ªbatchä¸€ä¸ªdatasetï¼Œä½¿ç”¨æ¸©åº¦é‡‡æ ·å¹³è¡¡å¤§å°å·®å¼‚ï¼š
p(i) âˆ n_i^Î±ï¼Œå…¶ä¸­ n_i æ˜¯æ•°æ®é›† i çš„æ ·æœ¬æ•°

Args:
    alpha: æ¸©åº¦é‡‡æ ·å‚æ•°
           Î±=0: å®Œå…¨å‡åŒ€ï¼ˆå°æ•°æ®é›†ä¸ä¼šè¢«æ·¹æ²¡ï¼‰
           Î±=0.3~0.5: å¸¸ç”¨æŠ˜ä¸­
           Î±=1: æŒ‰æ•°æ®é‡ï¼ˆå¤§æ•°æ®é›†ä¸»å¯¼ï¼‰
"""

import math
import random
from typing import Iterator, List, Dict, Optional

from torch.utils.data import Sampler

from .multi_dataset import UnifiedPrototypeDataset


class EpisodicBatchSampler(Sampler):
    """
    Episodicé‡‡æ ·å™¨ï¼šä¸€ä¸ªbatchä¸€ä¸ªdataset
    
    ä¸¤å±‚é‡‡æ ·ï¼š
    1. å…ˆé‡‡ dataset i ~ p(i)ï¼Œå…¶ä¸­ p(i) âˆ n_i^Î±
    2. å†ä» dataset i éšæœºé‡‡ä¸€ä¸ª batch
    
    Args:
        dataset: UnifiedPrototypeDataset
        batch_size: æ¯ä¸ªbatchçš„æ ·æœ¬æ•°
        alpha: æ¸©åº¦é‡‡æ ·å‚æ•°ï¼Œé»˜è®¤0.4
        num_episodes: æ¯ä¸ªepochçš„episodeæ•°ï¼ˆé»˜è®¤è‡ªåŠ¨è®¡ç®—ï¼‰
        shuffle: æ˜¯å¦åœ¨æ¯ä¸ªepochå¼€å§‹æ—¶æ‰“ä¹±
        drop_last: æ˜¯å¦ä¸¢å¼ƒæœ€åä¸è¶³batch_sizeçš„batch
    """
    
    def __init__(
        self,
        dataset: UnifiedPrototypeDataset,
        batch_size: int,
        alpha: float = 0.4,
        num_episodes: Optional[int] = None,
        shuffle: bool = True,
        drop_last: bool = False,
        rank: int = 0,
        world_size: int = 1,
        seed: int = 42,
    ):
        self.dataset = dataset
        self.batch_size = batch_size
        self.alpha = alpha
        self.shuffle = shuffle
        self.drop_last = drop_last
        self.rank = rank
        self.world_size = world_size
        self.seed = seed
        self.epoch = 0
        
        # è·å–æ¯ä¸ªæ•°æ®é›†çš„æ ·æœ¬ç´¢å¼•
        self.ds_ids = dataset.get_all_ds_ids()
        self.ds_indices: Dict[int, List[int]] = {
            ds_id: dataset.get_indices_for_dataset(ds_id)
            for ds_id in self.ds_ids
        }
        
        # å°†æ¯ä¸ªæ•°æ®é›†çš„ç´¢å¼•åˆ†ç‰‡ç»™å½“å‰rank
        if world_size > 1:
            for ds_id in self.ds_ids:
                indices = self.ds_indices[ds_id]
                # ç®€å•çš„åˆ†ç‰‡ç­–ç•¥ï¼šæŒ‰é¡ºåºåˆ†é…
                # æ›´å¥½çš„ç­–ç•¥å¯èƒ½æ˜¯å…ˆshuffleå†åˆ†é…ï¼Œä½†ä¸ºäº†ç¡®å®šæ€§ï¼Œè¿™é‡Œä¿æŒç®€å•
                # ç¡®ä¿æ¯ä¸ªrankåˆ†åˆ°çš„æ•°æ®å°½å¯èƒ½å‡åŒ€
                total_size = len(indices)
                per_rank = int(math.ceil(total_size / world_size))
                start = rank * per_rank
                end = min(start + per_rank, total_size)
                self.ds_indices[ds_id] = indices[start:end]
        
        # è®¡ç®—é‡‡æ ·æ¦‚ç‡ (åŸºäºå…¨å±€æ ·æœ¬æ•°è¿˜æ˜¯æœ¬åœ°æ ·æœ¬æ•°? åº”è¯¥åŸºäºå…¨å±€ä»¥ä¿æŒåˆ†å¸ƒä¸€è‡´)
        # è¿™é‡Œæˆ‘ä»¬é‡æ–°ä»registryè·å–å…¨å±€æ ·æœ¬æ•°æ¥è®¡ç®—æ¦‚ç‡
        registry = dataset.registry
        sample_counts = registry.get_sample_counts()
        # åªä¿ç•™å½“å‰å­˜åœ¨çš„ds_id
        sample_counts = {ds_id: sample_counts[ds_id] for ds_id in self.ds_ids}
        self._sample_counts = sample_counts
        
        # p(i) âˆ n_i^Î±
        weights = {ds_id: math.pow(count, alpha) for ds_id, count in sample_counts.items()}
        total_weight = sum(weights.values())
        self.ds_probs = {ds_id: w / total_weight for ds_id, w in weights.items()}
        
        # æ‰“å°é‡‡æ ·æ¦‚ç‡ (åªåœ¨rank 0æ‰“å°)
        if rank == 0:
            print(f"ğŸ“Š EpisodicBatchSampler (Î±={alpha}, world_size={world_size}):")
            for ds_id in self.ds_ids:
                ds_name = registry.get_dataset_info(ds_id).name
                prob = self.ds_probs[ds_id]
                count = sample_counts[ds_id]
                local_count = len(self.ds_indices[ds_id])
                print(f"   [{ds_id}] {ds_name}: {count} global samples ({local_count} local), p={prob:.3f}")
        
        # è®¡ç®—episodeæ•°ï¼ˆé»˜è®¤ï¼šæ€»æ ·æœ¬æ•° / batch_size / world_sizeï¼‰
        if num_episodes is None:
            total_samples = len(dataset)
            self.num_episodes = max(1, total_samples // batch_size // world_size)
        else:
            self.num_episodes = num_episodes
        
        if rank == 0:
            print(f"   Episodes per epoch: {self.num_episodes}")
        
        # å†…éƒ¨çŠ¶æ€
        self._ds_id_list = list(self.ds_ids)
        self._prob_list = [self.ds_probs[ds_id] for ds_id in self._ds_id_list]
    
    def set_epoch(self, epoch: int):
        """è®¾ç½®å½“å‰epochï¼Œç”¨äºæ›´æ–°éšæœºç§å­"""
        self.epoch = epoch
    
    def __iter__(self) -> Iterator[List[int]]:
        """ç”Ÿæˆbatchç´¢å¼•"""
        # 1. ç¡®å®šæ€§åœ°ç”Ÿæˆæœ¬epochçš„æ•°æ®é›†åºåˆ—
        # ä½¿ç”¨ç‹¬ç«‹çš„RNGï¼Œç§å­ä¸º seed + epoch
        # è¿™æ ·æ‰€æœ‰rankç”Ÿæˆçš„åºåˆ—æ˜¯ä¸€æ ·çš„
        rng_ds = random.Random(self.seed + self.epoch)
        
        # 2. å‡†å¤‡æœ¬åœ°æ•°æ®çš„ç´¢å¼•é˜Ÿåˆ—
        ds_queues: Dict[int, List[int]] = {}
        
        # ä½¿ç”¨æœ¬åœ°RNGï¼ˆå¯ä»¥æ˜¯å…¨å±€randomï¼‰æ¥shuffleæ•°æ®ç´¢å¼•
        # æ¯ä¸ªrankçš„shuffleåº”è¯¥æ˜¯ä¸åŒçš„ï¼ˆå› ä¸ºæ•°æ®ä¸åŒï¼Œä¸”é€šå¸¸å¸Œæœ›éšæœºæ€§ï¼‰
        # å¦‚æœéœ€è¦å®Œå…¨å¯å¤ç°ï¼Œå¯ä»¥ä½¿ç”¨ random.Random(self.seed + self.rank + self.epoch)
        rng_data = random.Random(self.seed + self.rank + self.epoch)
        
        if self.shuffle:
            for ds_id, indices in self.ds_indices.items():
                shuffled = indices.copy()
                rng_data.shuffle(shuffled)
                ds_queues[ds_id] = shuffled
        else:
            ds_queues = {ds_id: indices.copy() for ds_id, indices in self.ds_indices.items()}
        
        for _ in range(self.num_episodes):
            # 1. é‡‡æ ·æ•°æ®é›† (æ‰€æœ‰rankç›¸åŒ)
            ds_id = rng_ds.choices(self._ds_id_list, weights=self._prob_list, k=1)[0]
            
            # 2. ä»è¯¥æ•°æ®é›†é‡‡æ ·batch (å„rankä¸åŒ)
            queue = ds_queues[ds_id]
            
            # å¦‚æœé˜Ÿåˆ—ä¸è¶³ï¼Œé‡æ–°å¡«å……
            if len(queue) < self.batch_size:
                # é‡æ–°è·å–å¹¶shuffle
                new_indices = self.ds_indices[ds_id].copy()
                if not new_indices: # é˜²æ­¢ç©ºæ•°æ®é›†æ­»å¾ªç¯
                     yield [] 
                     continue

                if self.shuffle:
                    rng_data.shuffle(new_indices)
                queue.extend(new_indices)
            
            # å–batch
            batch = queue[:self.batch_size]
            ds_queues[ds_id] = queue[self.batch_size:]
            
            if self.drop_last and len(batch) < self.batch_size:
                continue
            
            yield batch
    
    def __len__(self) -> int:
        return self.num_episodes
    
    def get_dataset_sampling_stats(self) -> Dict[str, float]:
        """è·å–æ•°æ®é›†é‡‡æ ·ç»Ÿè®¡ï¼ˆç”¨äºåˆ†æï¼‰"""
        return {
            self.dataset.registry.get_dataset_info(ds_id).name: prob
            for ds_id, prob in self.ds_probs.items()
        }


# æµ‹è¯•
if __name__ == "__main__":
    from pathlib import Path
    import sys
    sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent / "src"))
    
    from opentslm.time_series_datasets.multi_dataset import (
        MultiDatasetRegistry, 
        UnifiedPrototypeDataset
    )
    
    print("Testing EpisodicBatchSampler...")
    
    registry = MultiDatasetRegistry(data_path="./data")
    registry.register("ECG200")
    registry.register("Coffee")
    
    dataset = UnifiedPrototypeDataset(registry, split="train")
    sampler = EpisodicBatchSampler(dataset, batch_size=8, alpha=0.4, num_episodes=10)
    
    print("\nSampling 5 episodes:")
    ds_count = {}
    for i, batch_indices in enumerate(sampler):
        if i >= 5:
            break
        # æ£€æŸ¥åŒä¸€batchæ˜¯å¦æ¥è‡ªåŒä¸€æ•°æ®é›†
        ds_ids = set(dataset[idx]["ds_id"] for idx in batch_indices)
        assert len(ds_ids) == 1, "Batch should contain samples from single dataset"
        ds_id = list(ds_ids)[0]
        ds_name = registry.get_dataset_info(ds_id).name
        ds_count[ds_name] = ds_count.get(ds_name, 0) + 1
        print(f"Episode {i}: {len(batch_indices)} samples from {ds_name}")
    
    print(f"\nDataset distribution: {ds_count}")
