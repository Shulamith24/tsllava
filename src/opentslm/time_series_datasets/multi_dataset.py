# SPDX-FileCopyrightText: 2025 Stanford University, ETH Zurich, and the project authors
# SPDX-License-Identifier: MIT

"""
å¤šæ•°æ®é›†æ³¨å†Œä¸­å¿ƒ + ç»Ÿä¸€æ•°æ®é›†åŒ…è£…å™¨

ç”¨äºå¤šæ•°æ®é›†ç»Ÿä¸€è®­ç»ƒï¼š
1. MultiDatasetRegistry: ç®¡ç†å¤šä¸ªUCRæ•°æ®é›†çš„å…ƒä¿¡æ¯
2. UnifiedPrototypeDataset: ç»Ÿä¸€getitemè¾“å‡ºæ ¼å¼
"""

import os
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass

import torch
from torch.utils.data import Dataset

from opentslm.time_series_datasets.ucr.UCRClassificationDataset import UCRClassificationDataset
from opentslm.time_series_datasets.ucr.ucr_loader import load_ucr_dataset, ensure_ucr_data


@dataclass
class DatasetInfo:
    """æ•°æ®é›†å…ƒä¿¡æ¯"""
    ds_id: int              # æ•°æ®é›†å”¯ä¸€ID (0-indexed)
    name: str               # æ•°æ®é›†åç§°
    num_classes: int        # ç±»åˆ«æ•°
    num_train_samples: int  # è®­ç»ƒæ ·æœ¬æ•°
    num_test_samples: int   # æµ‹è¯•æ ·æœ¬æ•°
    label_to_idx: Dict      # åŸå§‹æ ‡ç­¾ â†’ ç±»åˆ«ç´¢å¼• (0-indexed)
    idx_to_label: Dict      # ç±»åˆ«ç´¢å¼• â†’ åŸå§‹æ ‡ç­¾


class MultiDatasetRegistry:
    """
    å¤šæ•°æ®é›†æ³¨å†Œä¸­å¿ƒ
    
    ä»é…ç½®æ–‡ä»¶è¯»å–æ•°æ®é›†åˆ—è¡¨ï¼ŒåŠ è½½æ¯ä¸ªæ•°æ®é›†çš„å…ƒä¿¡æ¯ã€‚
    
    ä½¿ç”¨æ–¹æ³•:
        registry = MultiDatasetRegistry()
        registry.load_from_file("configs/multi_dataset_ucr.txt")
        print(registry.get_total_datasets())
    """
    
    def __init__(self, data_path: str = "./data"):
        self.data_path = data_path
        self._datasets: Dict[int, DatasetInfo] = {}
        self._name_to_id: Dict[str, int] = {}
        self._next_id = 0
    
    def load_from_file(self, config_path: str) -> None:
        """
        ä»é…ç½®æ–‡ä»¶åŠ è½½æ•°æ®é›†åˆ—è¡¨
        
        é…ç½®æ–‡ä»¶æ ¼å¼ï¼šæ¯è¡Œä¸€ä¸ªæ•°æ®é›†åç§°ï¼Œ#å¼€å¤´ä¸ºæ³¨é‡Š
        """
        ensure_ucr_data()
        
        config_path = Path(config_path)
        if not config_path.exists():
            raise FileNotFoundError(f"Config file not found: {config_path}")
        
        with open(config_path, 'r') as f:
            lines = f.readlines()
        
        for line in lines:
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            self.register(line)
        
        print(f"ğŸ“š Registered {self.get_total_datasets()} datasets from {config_path}")
    
    def register(self, dataset_name: str) -> DatasetInfo:
        """æ³¨å†Œå•ä¸ªæ•°æ®é›†"""
        if dataset_name in self._name_to_id:
            return self._datasets[self._name_to_id[dataset_name]]
        
        # åŠ è½½æ•°æ®é›†è·å–å…ƒä¿¡æ¯
        train_df, test_df = load_ucr_dataset(dataset_name, raw_data_path=self.data_path)
        
        # è·å–æ ‡ç­¾æ˜ å°„
        all_labels = sorted(train_df["label"].unique().tolist())
        label_to_idx = {label: i for i, label in enumerate(all_labels)}
        idx_to_label = {i: label for label, i in label_to_idx.items()}
        
        info = DatasetInfo(
            ds_id=self._next_id,
            name=dataset_name,
            num_classes=len(all_labels),
            num_train_samples=len(train_df),
            num_test_samples=len(test_df),
            label_to_idx=label_to_idx,
            idx_to_label=idx_to_label,
        )
        
        self._datasets[self._next_id] = info
        self._name_to_id[dataset_name] = self._next_id
        self._next_id += 1
        
        print(f"   [{info.ds_id}] {dataset_name}: {info.num_classes} classes, "
              f"{info.num_train_samples} train / {info.num_test_samples} test")
        
        return info
    
    def get_dataset_info(self, ds_id: int) -> DatasetInfo:
        """è·å–æ•°æ®é›†å…ƒä¿¡æ¯"""
        return self._datasets[ds_id]
    
    def get_dataset_by_name(self, name: str) -> DatasetInfo:
        """æŒ‰åç§°è·å–æ•°æ®é›†"""
        return self._datasets[self._name_to_id[name]]
    
    def get_all_datasets(self) -> List[DatasetInfo]:
        """è·å–æ‰€æœ‰æ•°æ®é›†ä¿¡æ¯"""
        return [self._datasets[i] for i in range(self._next_id)]
    
    def get_total_datasets(self) -> int:
        """è·å–æ•°æ®é›†æ€»æ•°"""
        return self._next_id
    
    def get_class_counts(self) -> Dict[int, int]:
        """è·å–æ¯ä¸ªæ•°æ®é›†çš„ç±»åˆ«æ•° {ds_id: num_classes}"""
        return {ds_id: info.num_classes for ds_id, info in self._datasets.items()}
    
    def get_max_classes(self) -> int:
        """è·å–æ‰€æœ‰æ•°æ®é›†ä¸­æœ€å¤§ç±»åˆ«æ•°"""
        return max(info.num_classes for info in self._datasets.values())
    
    def get_sample_counts(self) -> Dict[int, int]:
        """è·å–æ¯ä¸ªæ•°æ®é›†çš„è®­ç»ƒæ ·æœ¬æ•° {ds_id: num_train_samples}"""
        return {ds_id: info.num_train_samples for ds_id, info in self._datasets.items()}


class UnifiedPrototypeDataset(Dataset):
    """
    ç»Ÿä¸€Prototypeæ•°æ®é›†
    
    å°†å¤šä¸ªUCRæ•°æ®é›†åˆå¹¶ä¸ºä¸€ä¸ªDatasetï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å« ds_id æ ‡è¯†ã€‚
    
    è¾“å‡ºæ ¼å¼:
        {
            "time_series": [Tensor],  # æ—¶é—´åºåˆ—åˆ—è¡¨
            "label_index": int,       # ç±»åˆ«ç´¢å¼• (è¯¥æ•°æ®é›†å†…éƒ¨0-indexed)
            "ds_id": int,             # æ•°æ®é›†ID
            "ds_name": str,           # æ•°æ®é›†åç§°
            "_global_idx": int,       # å…¨å±€ç´¢å¼•
        }
    """
    
    def __init__(
        self,
        registry: MultiDatasetRegistry,
        split: str = "train",
        eos_token: str = "<eos>",
    ):
        self.registry = registry
        self.split = split
        self.eos_token = eos_token
        
        # åŠ è½½æ‰€æœ‰æ•°æ®é›†
        self._samples: List[Dict] = []
        self._ds_indices: Dict[int, List[int]] = {}  # ds_id â†’ [sample indices]
        
        self._load_all_datasets()
    
    def _load_all_datasets(self):
        """åŠ è½½æ‰€æœ‰æ•°æ®é›†"""
        ensure_ucr_data()
        
        for ds_info in self.registry.get_all_datasets():
            start_idx = len(self._samples)
            
            # åŠ è½½æ•°æ®
            train_df, test_df = load_ucr_dataset(
                ds_info.name, 
                raw_data_path=self.registry.data_path
            )
            
            # é€‰æ‹©split
            if self.split == "train":
                df = train_df
            else:  # validation/test ä½¿ç”¨ç›¸åŒæ•°æ®
                df = test_df
            
            # è½¬æ¢ä¸ºæ ·æœ¬åˆ—è¡¨
            for _, row in df.iterrows():
                # æå–æ—¶é—´åºåˆ—
                feature_cols = [col for col in row.index if col != "label"]
                values = [row[col] for col in feature_cols]
                ts_tensor = torch.tensor(values, dtype=torch.float32)
                ts_tensor = torch.nan_to_num(ts_tensor, nan=0.0)
                
                # Z-normalization
                mean = ts_tensor.mean()
                std = ts_tensor.std()
                if std > 1e-8:
                    ts_tensor = (ts_tensor - mean) / std
                else:
                    ts_tensor = ts_tensor - mean
                
                # è·å–æ ‡ç­¾ç´¢å¼•
                label_index = ds_info.label_to_idx[row["label"]]
                
                self._samples.append({
                    "time_series": [ts_tensor],
                    "label_index": label_index,
                    "ds_id": ds_info.ds_id,
                    "ds_name": ds_info.name,
                    "_original_label": row["label"],
                })
            
            end_idx = len(self._samples)
            self._ds_indices[ds_info.ds_id] = list(range(start_idx, end_idx))
        
        print(f"ğŸ“¦ UnifiedPrototypeDataset ({self.split}): {len(self._samples)} samples total")
    
    def __len__(self):
        return len(self._samples)
    
    def __getitem__(self, idx):
        sample = self._samples[idx].copy()
        sample["_global_idx"] = idx
        return sample
    
    def get_indices_for_dataset(self, ds_id: int) -> List[int]:
        """è·å–æŒ‡å®šæ•°æ®é›†çš„æ‰€æœ‰æ ·æœ¬ç´¢å¼•"""
        return self._ds_indices.get(ds_id, [])
    
    def get_all_ds_ids(self) -> List[int]:
        """è·å–æ‰€æœ‰æ•°æ®é›†ID"""
        return list(self._ds_indices.keys())


# æµ‹è¯•
if __name__ == "__main__":
    import sys
    sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent / "src"))
    
    print("Testing MultiDatasetRegistry...")
    
    registry = MultiDatasetRegistry(data_path="./data")
    registry.register("ECG200")
    registry.register("Coffee")
    
    print(f"\nTotal datasets: {registry.get_total_datasets()}")
    print(f"Class counts: {registry.get_class_counts()}")
    
    print("\nTesting UnifiedPrototypeDataset...")
    dataset = UnifiedPrototypeDataset(registry, split="train")
    
    print(f"Total samples: {len(dataset)}")
    
    sample = dataset[0]
    print(f"\nSample keys: {sample.keys()}")
    print(f"ds_id: {sample['ds_id']}, ds_name: {sample['ds_name']}")
    print(f"label_index: {sample['label_index']}")
    print(f"time_series shape: {sample['time_series'][0].shape}")
