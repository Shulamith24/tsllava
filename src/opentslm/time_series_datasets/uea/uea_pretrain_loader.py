# SPDX-FileCopyrightText: 2025 Stanford University, ETH Zurich, and the project authors (see CONTRIBUTORS.md)
# SPDX-FileCopyrightText: 2025 This source file is part of the OpenTSLM open-source project.
#
# SPDX-License-Identifier: MIT

"""
UEAå¤šå˜é‡æ•°æ®é›†é¢„è®­ç»ƒåŠ è½½å™¨

ç”¨äºTSLANetåœ¨å¤šä¸ªUEAæ•°æ®é›†ä¸Šçš„é¢„è®­ç»ƒã€‚
"""

import os
from typing import List, Tuple, Optional
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader

# å°è¯•å¯¼å…¥aeonåº“
try:
    from aeon.datasets import load_classification
    AEON_AVAILABLE = True
except ImportError:
    AEON_AVAILABLE = False


def load_dataset_list(file_path: str) -> List[str]:
    """ä»æ–‡ä»¶åŠ è½½æ•°æ®é›†åç§°åˆ—è¡¨"""
    with open(file_path, "r", encoding="utf-8") as f:
        lines = f.readlines()
    
    datasets = []
    for line in lines:
        line = line.strip()
        if line and not line.startswith("#"):
            datasets.append(line)
    
    return datasets


class UEAPretrainDataset(Dataset):
    """
    UEAå•æ•°æ®é›†é¢„è®­ç»ƒDataset
    
    åªè¿”å›å½’ä¸€åŒ–åçš„æ—¶é—´åºåˆ—æ•°æ®ï¼ˆä¸åŒ…å«æ ‡ç­¾ï¼‰
    """
    def __init__(
        self,
        X: np.ndarray,  # [N, C, L]
    ):
        self.X = X
        
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        # è·å–æ ·æœ¬ [C, L]
        x = self.X[idx].astype(np.float32)
        
        # Per-sample normalization (åœ¨é€šé“ç»´åº¦ä¸Š)
        mean = np.nanmean(x, axis=1, keepdims=True)
        std = np.nanstd(x, axis=1, keepdims=True)
        std = np.clip(std, a_min=1e-8, a_max=None)
        x = (x - mean) / std
        
        # å¤„ç†NaN
        x = np.nan_to_num(x, nan=0.0)
        
        return torch.tensor(x, dtype=torch.float32)


class UEAMultiDatasetForPretrain(Dataset):
    """
    åˆå¹¶å¤šä¸ªUEAæ•°æ®é›†ç”¨äºé¢„è®­ç»ƒ
    """
    def __init__(
        self,
        dataset_names: List[str],
        split: str = "train",
    ):
        """
        Args:
            dataset_names: UEAæ•°æ®é›†åç§°åˆ—è¡¨
            split: "train" æˆ– "test"
        """
        if not AEON_AVAILABLE:
            raise ImportError("aeonåº“æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install aeon")
        
        all_samples = []
        
        print(f"ğŸ“‚ Loading {len(dataset_names)} UEA datasets for pretraining...")
        for name in dataset_names:
            try:
                X, _ = load_classification(name, split=split)
                # X: [N, C, L]
                all_samples.append(X)
                print(f"   âœ“ {name}: {X.shape}")
            except Exception as e:
                print(f"   âœ— {name}: {e}")
        
        if not all_samples:
            raise ValueError("No datasets loaded successfully!")
        
        # å­˜å‚¨æ¯ä¸ªæ ·æœ¬çš„ä¿¡æ¯ï¼ˆä¸æ‹¼æ¥ï¼Œå› ä¸ºé€šé“æ•°/é•¿åº¦å¯èƒ½ä¸åŒï¼‰
        self.samples = []
        for X in all_samples:
            for i in range(len(X)):
                self.samples.append(X[i])  # [C, L]
        
        print(f"   Total samples: {len(self.samples)}")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        # è·å–æ ·æœ¬ [C, L]
        x = self.samples[idx].astype(np.float32)
        
        # Per-sample normalization
        mean = np.nanmean(x, axis=1, keepdims=True)
        std = np.nanstd(x, axis=1, keepdims=True)
        std = np.clip(std, a_min=1e-8, a_max=None)
        x = (x - mean) / std
        
        # å¤„ç†NaN
        x = np.nan_to_num(x, nan=0.0)
        
        return torch.tensor(x, dtype=torch.float32)


def collate_fn_pretrain(batch: List[torch.Tensor], patch_size: int = 8) -> torch.Tensor:
    """
    é¢„è®­ç»ƒä¸“ç”¨collateå‡½æ•°
    
    å¤„ç†ä¸åŒé•¿åº¦/é€šé“æ•°çš„æ—¶åºï¼Œå¡«å……åˆ°patch_sizeçš„å€æ•°
    """
    # æ‰¾åˆ°æœ€å¤§é•¿åº¦å’Œæœ€å¤§é€šé“æ•°
    max_len = max(x.shape[1] for x in batch)
    max_channels = max(x.shape[0] for x in batch)
    
    # å¡«å……åˆ°patch_sizeçš„å€æ•°
    rem = max_len % patch_size
    if rem != 0:
        max_len = max_len + (patch_size - rem)
    
    # åˆ›å»ºå¡«å……åçš„batch
    batch_size = len(batch)
    padded_batch = torch.zeros(batch_size, max_channels, max_len)
    
    for i, x in enumerate(batch):
        c, l = x.shape
        padded_batch[i, :c, :l] = x
    
    return padded_batch


def get_uea_pretrain_loader(
    dataset_list_file: str,
    batch_size: int = 16,
    patch_size: int = 8,
    split: str = "train",
    num_workers: int = 0,
) -> DataLoader:
    """
    è·å–UEAå¤šæ•°æ®é›†é¢„è®­ç»ƒDataLoader
    """
    dataset_names = load_dataset_list(dataset_list_file)
    dataset = UEAMultiDatasetForPretrain(dataset_names, split=split)
    
    loader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=(split == "train"),
        num_workers=num_workers,
        collate_fn=lambda batch: collate_fn_pretrain(batch, patch_size),
    )
    
    return loader


# å¸¸ç”¨UEAæ•°æ®é›†åˆ—è¡¨
UEA_PRETRAIN_DATASETS = [
    "ArticularyWordRecognition",
    "AtrialFibrillation",
    "BasicMotions",
    "CharacterTrajectories",
    "Cricket",
    "DuckDuckGeese",
    "ERing",
    "EigenWorms",
    "Epilepsy",
    "EthanolConcentration",
    "FaceDetection",
    "FingerMovements",
    "HandMovementDirection",
    "Handwriting",
    "Heartbeat",
    "InsectWingbeat",
    "JapaneseVowels",
    "Libras",
    "LSST",
    "MotorImagery",
    "NATOPS",
    "PenDigits",
    "PEMS-SF",
    "PhonemeSpectra",
    "RacketSports",
    "SelfRegulationSCP1",
    "SelfRegulationSCP2",
    "SpokenArabicDigits",
    "StandWalkJump",
    "UWaveGestureLibrary",
]


if __name__ == "__main__":
    # æµ‹è¯•å•æ•°æ®é›†åŠ è½½
    from aeon.datasets import load_classification
    
    print("Testing UEA pretrain loader...")
    
    # æµ‹è¯•å•ä¸ªæ•°æ®é›†
    X_train, y_train = load_classification("Handwriting", split="train")
    print(f"Handwriting: {X_train.shape}")
    
    dataset = UEAPretrainDataset(X_train)
    print(f"Dataset size: {len(dataset)}")
    print(f"Sample shape: {dataset[0].shape}")
