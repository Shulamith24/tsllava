# SPDX-FileCopyrightText: 2025 Stanford University, ETH Zurich, and the project authors (see CONTRIBUTORS.md)
# SPDX-FileCopyrightText: 2025 This source file is part of the OpenTSLM open-source project.
#
# SPDX-License-Identifier: MIT

"""
UEAå¤šå˜é‡æ•°æ®é›†é¢„è®­ç»ƒåŠ è½½å™¨

ä¿®å¤ç‰ˆæœ¬ï¼š
1. å¤„ç†å˜é•¿åºåˆ—ï¼ˆè·³è¿‡æˆ–æˆªæ–­ï¼‰
2. å¯¹å¤§é€šé“æ•°/é•¿åº¦è¿›è¡ŒåŠ¨æ€é‡‡æ ·
3. Channel Independenceç­–ç•¥
"""

import os
from typing import List, Tuple, Optional
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader

# å°è¯•å¯¼å…¥aeonåº“
try:
    from aeon.datasets import load_classification
    AEON_AVAILABLE = True
except ImportError:
    AEON_AVAILABLE = False


def load_dataset_list(file_path: str) -> List[str]:
    """ä»æ–‡ä»¶åŠ è½½æ•°æ®é›†åç§°åˆ—è¡¨"""
    with open(file_path, "r", encoding="utf-8") as f:
        lines = f.readlines()
    
    datasets = []
    for line in lines:
        line = line.strip()
        if line and not line.startswith("#"):
            datasets.append(line)
    
    return datasets


def is_variable_length(X) -> bool:
    """æ£€æŸ¥æ•°æ®é›†æ˜¯å¦ä¸ºå˜é•¿åºåˆ—ï¼ˆè¿”å›çš„æ˜¯listè€Œéndarrayï¼‰"""
    return isinstance(X, list) or not hasattr(X, 'shape')


def convert_variable_length_to_fixed(X, max_len: int = 512) -> np.ndarray:
    """
    å°†å˜é•¿åºåˆ—è½¬æ¢ä¸ºå›ºå®šé•¿åº¦
    - è¶…é•¿çš„æˆªæ–­
    - çŸ­çš„å¡«å……
    """
    if not is_variable_length(X):
        return X
    
    samples = []
    for sample in X:
        # sampleå¯èƒ½æ˜¯ [C, L] æˆ– list of list
        if isinstance(sample, np.ndarray):
            c, l = sample.shape
        else:
            # å¤„ç†åµŒå¥—list
            try:
                sample = np.array(sample, dtype=np.float32)
                if sample.ndim == 1:
                    sample = sample.reshape(1, -1)
                c, l = sample.shape
            except:
                continue
        
        # æˆªæ–­æˆ–å¡«å……åˆ°max_len
        if l > max_len:
            sample = sample[:, :max_len]
        elif l < max_len:
            pad = np.zeros((c, max_len - l), dtype=np.float32)
            sample = np.concatenate([sample, pad], axis=1)
        
        samples.append(sample)
    
    if not samples:
        return None
    
    # ç¡®ä¿æ‰€æœ‰æ ·æœ¬é€šé“æ•°ä¸€è‡´
    max_c = max(s.shape[0] for s in samples)
    result = []
    for s in samples:
        if s.shape[0] < max_c:
            pad = np.zeros((max_c - s.shape[0], s.shape[1]), dtype=np.float32)
            s = np.concatenate([s, pad], axis=0)
        result.append(s)
    
    return np.array(result, dtype=np.float32)


class UEAPretrainDataset(Dataset):
    """
    UEAå•æ•°æ®é›†é¢„è®­ç»ƒDatasetï¼ˆå¸¦åŠ¨æ€é‡‡æ ·ï¼‰
    
    Args:
        X: [N, C, L] æ•°æ®
        max_channels: æœ€å¤§é€šé“æ•°ï¼Œè¶…è¿‡åˆ™éšæœºé‡‡æ ·
        max_length: æœ€å¤§åºåˆ—é•¿åº¦ï¼Œè¶…è¿‡åˆ™éšæœºè£å‰ªç‰‡æ®µ
    """
    def __init__(
        self,
        X: np.ndarray,
        max_channels: int = 32,
        max_length: int = 512,
    ):
        self.X = X
        self.max_channels = max_channels
        self.max_length = max_length
        
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        # è·å–æ ·æœ¬ [C, L]
        x = self.X[idx].astype(np.float32)
        c, l = x.shape
        
        # åŠ¨æ€é€šé“é‡‡æ ·ï¼šå¦‚æœé€šé“æ•°è¶…é™ï¼Œéšæœºé€‰æ‹©max_channelsä¸ªé€šé“
        if c > self.max_channels:
            channel_indices = np.random.choice(c, self.max_channels, replace=False)
            channel_indices = np.sort(channel_indices)  # ä¿æŒé¡ºåº
            x = x[channel_indices, :]
            c = self.max_channels
        
        # åŠ¨æ€é•¿åº¦é‡‡æ ·ï¼šå¦‚æœé•¿åº¦è¶…é™ï¼Œéšæœºè£å‰ªä¸€æ®µ
        if l > self.max_length:
            start = np.random.randint(0, l - self.max_length + 1)
            x = x[:, start:start + self.max_length]
            l = self.max_length
        
        # Per-channel normalization
        mean = np.nanmean(x, axis=1, keepdims=True)
        std = np.nanstd(x, axis=1, keepdims=True)
        std = np.clip(std, a_min=1e-8, a_max=None)
        x = (x - mean) / std
        
        # å¤„ç†NaN
        x = np.nan_to_num(x, nan=0.0)
        
        return torch.tensor(x, dtype=torch.float32)


class UEAMultiDatasetForPretrain(Dataset):
    """
    åˆå¹¶å¤šä¸ªUEAæ•°æ®é›†ç”¨äºé¢„è®­ç»ƒï¼ˆå¸¦åŠ¨æ€é‡‡æ ·ï¼‰
    
    Args:
        dataset_names: æ•°æ®é›†åç§°åˆ—è¡¨
        split: "train" æˆ– "test"
        max_channels: æœ€å¤§é€šé“æ•°
        max_length: æœ€å¤§åºåˆ—é•¿åº¦
        skip_variable_length: æ˜¯å¦è·³è¿‡å˜é•¿æ•°æ®é›†
    """
    def __init__(
        self,
        dataset_names: List[str],
        split: str = "train",
        max_channels: int = 32,
        max_length: int = 512,
        skip_variable_length: bool = False,
    ):
        if not AEON_AVAILABLE:
            raise ImportError("aeonåº“æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install aeon")
        
        self.max_channels = max_channels
        self.max_length = max_length
        self.samples = []
        
        print(f"ğŸ“‚ Loading {len(dataset_names)} UEA datasets for pretraining...")
        print(f"   (max_channels={max_channels}, max_length={max_length})")
        
        for name in dataset_names:
            try:
                X, _ = load_classification(name, split=split)
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºå˜é•¿åºåˆ—
                if is_variable_length(X):
                    if skip_variable_length:
                        print(f"   â­ {name}: variable length, skipped")
                        continue
                    else:
                        # å°è¯•è½¬æ¢
                        X = convert_variable_length_to_fixed(X, max_length)
                        if X is None:
                            print(f"   âœ— {name}: failed to convert variable length")
                            continue
                        print(f"   âœ“ {name}: {X.shape} (converted from variable length)")
                else:
                    print(f"   âœ“ {name}: {X.shape}")
                
                # æ·»åŠ æ ·æœ¬
                for i in range(len(X)):
                    self.samples.append(X[i])  # [C, L]
                    
            except Exception as e:
                print(f"   âœ— {name}: {e}")
        
        if not self.samples:
            raise ValueError("No datasets loaded successfully!")
        
        print(f"   Total samples: {len(self.samples)}")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        # è·å–æ ·æœ¬ [C, L]
        x = self.samples[idx].astype(np.float32)
        c, l = x.shape
        
        # åŠ¨æ€é€šé“é‡‡æ ·
        if c > self.max_channels:
            channel_indices = np.random.choice(c, self.max_channels, replace=False)
            channel_indices = np.sort(channel_indices)
            x = x[channel_indices, :]
            c = self.max_channels
        
        # åŠ¨æ€é•¿åº¦é‡‡æ ·
        if l > self.max_length:
            start = np.random.randint(0, l - self.max_length + 1)
            x = x[:, start:start + self.max_length]
            l = self.max_length
        
        # Per-channel normalization
        mean = np.nanmean(x, axis=1, keepdims=True)
        std = np.nanstd(x, axis=1, keepdims=True)
        std = np.clip(std, a_min=1e-8, a_max=None)
        x = (x - mean) / std
        
        x = np.nan_to_num(x, nan=0.0)
        
        return torch.tensor(x, dtype=torch.float32)


def collate_fn_pretrain(batch: List[torch.Tensor], patch_size: int = 8) -> torch.Tensor:
    """
    é¢„è®­ç»ƒä¸“ç”¨collateå‡½æ•°
    
    å¤„ç†ä¸åŒé•¿åº¦/é€šé“æ•°çš„æ—¶åºï¼Œå¡«å……åˆ°patch_sizeçš„å€æ•°
    """
    # æ‰¾åˆ°æœ€å¤§é•¿åº¦å’Œæœ€å¤§é€šé“æ•°
    max_len = max(x.shape[1] for x in batch)
    max_channels = max(x.shape[0] for x in batch)
    
    # å¡«å……åˆ°patch_sizeçš„å€æ•°
    rem = max_len % patch_size
    if rem != 0:
        max_len = max_len + (patch_size - rem)
    
    # åˆ›å»ºå¡«å……åçš„batch
    batch_size = len(batch)
    padded_batch = torch.zeros(batch_size, max_channels, max_len)
    
    for i, x in enumerate(batch):
        c, l = x.shape
        padded_batch[i, :c, :l] = x
    
    return padded_batch


def get_uea_pretrain_loader(
    dataset_list_file: str,
    batch_size: int = 16,
    patch_size: int = 8,
    split: str = "train",
    num_workers: int = 0,
    max_channels: int = 32,
    max_length: int = 512,
    skip_variable_length: bool = True,
) -> DataLoader:
    """
    è·å–UEAå¤šæ•°æ®é›†é¢„è®­ç»ƒDataLoader
    
    Args:
        dataset_list_file: æ•°æ®é›†åˆ—è¡¨æ–‡ä»¶
        batch_size: æ‰¹æ¬¡å¤§å°
        patch_size: patchå¤§å°
        split: æ•°æ®åˆ’åˆ†
        num_workers: åŠ è½½çº¿ç¨‹æ•°
        max_channels: æœ€å¤§é€šé“æ•°ï¼ˆè¶…è¿‡åˆ™åŠ¨æ€é‡‡æ ·ï¼‰
        max_length: æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆè¶…è¿‡åˆ™åŠ¨æ€è£å‰ªï¼‰
        skip_variable_length: æ˜¯å¦è·³è¿‡å˜é•¿æ•°æ®é›†
    """
    dataset_names = load_dataset_list(dataset_list_file)
    dataset = UEAMultiDatasetForPretrain(
        dataset_names, 
        split=split,
        max_channels=max_channels,
        max_length=max_length,
        skip_variable_length=skip_variable_length,
    )
    
    loader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=(split == "train"),
        num_workers=num_workers,
        collate_fn=lambda batch: collate_fn_pretrain(batch, patch_size),
    )
    
    return loader


# æ¨èçš„UEAæ•°æ®é›†ï¼ˆæ’é™¤å˜é•¿å’Œè¶…å¤§æ•°æ®é›†ï¼‰
UEA_PRETRAIN_DATASETS_SAFE = [
    "ArticularyWordRecognition",
    "AtrialFibrillation",
    "BasicMotions",
    "Cricket",
    "ERing",
    "Epilepsy",
    "EthanolConcentration",
    "FingerMovements",
    "HandMovementDirection",
    "Handwriting",
    "Libras",
    "LSST",
    "NATOPS",
    "PenDigits",
    "RacketSports",
    "SelfRegulationSCP1",
    "SelfRegulationSCP2",
    "StandWalkJump",
    "UWaveGestureLibrary",
]


if __name__ == "__main__":
    print("Testing UEA pretrain loader with dynamic sampling...")
    
    if AEON_AVAILABLE:
        # æµ‹è¯•å•ä¸ªæ•°æ®é›†
        X_train, _ = load_classification("Handwriting", split="train")
        print(f"Handwriting: {X_train.shape}")
        
        dataset = UEAPretrainDataset(X_train, max_channels=32, max_length=512)
        print(f"Dataset size: {len(dataset)}")
        print(f"Sample shape: {dataset[0].shape}")
        
        # æµ‹è¯•å¤§é€šé“æ•°æ®é›†
        X_large, _ = load_classification("FaceDetection", split="train")
        print(f"\nFaceDetection (large channels): {X_large.shape}")
        
        dataset_large = UEAPretrainDataset(X_large, max_channels=32, max_length=512)
        sample = dataset_large[0]
        print(f"Sampled shape: {sample.shape}")
    else:
        print("è¯·å®‰è£…aeon: pip install aeon")

