# SPDX-FileCopyrightText: 2025 Stanford University, ETH Zurich, and the project authors (see CONTRIBUTORS.md)
# SPDX-FileCopyrightText: 2025 This source file is part of the OpenTSLM open-source project.
#
# SPDX-License-Identifier: MIT

"""
OpenTSLMPrototype: åŸºäºPrototypeçš„æ—¶é—´åºåˆ—åˆ†ç±»æ¨¡å‹

æ ¸å¿ƒæ¶æ„:
- è¾“å…¥åºåˆ—: [Learnable Prompt] + [TS_TOKENS] + [CLS]
- è¾“å‡º: CLSéšå‘é‡ â†’ Prototypeå¤´ â†’ logits

ä¸åŸå§‹OpenTSLMSPçš„åŒºåˆ«:
1. ä½¿ç”¨å¯å­¦ä¹ çš„Prompt tokensæ›¿ä»£è‡ªç„¶è¯­è¨€prompt
2. ä½¿ç”¨CLS tokenæå–è¡¨å¾ï¼ˆæ”¾åœ¨TS tokensä¹‹åï¼‰
3. ä½¿ç”¨Prototype + ä½™å¼¦ç›¸ä¼¼åº¦ + æ¸©åº¦è¿›è¡Œåˆ†ç±»
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Dict, Tuple, Optional
from torch.nn.utils.rnn import pad_sequence

from .OpenTSLMSP import OpenTSLMSP


class PrototypeClassificationHead(nn.Module):
    """
    Prototypeåˆ†ç±»å¤´
    
    ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ + å¯å­¦ä¹ æ¸©åº¦è¿›è¡Œåˆ†ç±»
    logits = cosine_similarity(z, prototypes) / temperature
    """
    
    def __init__(self, hidden_size: int, num_classes: int, init_temperature: float = 1.0):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_classes = num_classes
        
        # PrototypeçŸ©é˜µ: [num_classes, hidden_size]
        self.prototypes = nn.Parameter(torch.randn(num_classes, hidden_size) * 0.02)
        
        # å¯å­¦ä¹ æ¸©åº¦å‚æ•°
        self.log_temperature = nn.Parameter(torch.log(torch.tensor(init_temperature)))
    
    @property
    def temperature(self) -> torch.Tensor:
        """è¿”å›æ¸©åº¦å€¼ï¼ˆé€šè¿‡logç¡®ä¿å§‹ç»ˆä¸ºæ­£ï¼‰"""
        return self.log_temperature.exp().clamp(min=0.01, max=100.0)
    
    def forward(self, z: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—åˆ†ç±»logits
        
        Args:
            z: CLSéšå‘é‡ [batch_size, hidden_size]
            
        Returns:
            logits: [batch_size, num_classes]
        """
        # L2å½’ä¸€åŒ–
        z_norm = F.normalize(z, p=2, dim=-1)
        proto_norm = F.normalize(self.prototypes, p=2, dim=-1)
        
        # ä½™å¼¦ç›¸ä¼¼åº¦
        similarity = torch.matmul(z_norm, proto_norm.T)  # [B, num_classes]
        
        # æ¸©åº¦ç¼©æ”¾
        logits = similarity / self.temperature
        
        return logits


class OpenTSLMPrototype(OpenTSLMSP):
    """
    åŸºäºPrototypeçš„æ—¶é—´åºåˆ—åˆ†ç±»æ¨¡å‹
    
    è¾“å…¥åºåˆ—ç»“æ„:
        [Prompt (10 tokens)] + [TS_tokens (L tokens)] + [CLS (1 token)]
    
    è¾“å‡º:
        - å–CLSä½ç½®çš„éšå‘é‡
        - é€šè¿‡Prototypeå¤´è®¡ç®—åˆ†ç±»logits
    
    Args:
        llm_id: LLMæ¨¡å‹ID
        device: è®¾å¤‡
        encoder_type: ç¼–ç å™¨ç±»å‹
        prompt_len: å¯å­¦ä¹ promptçš„é•¿åº¦
        num_classes: åˆ†ç±»ç±»åˆ«æ•°
        init_temperature: æ¸©åº¦åˆå§‹å€¼
        **kwargs: å…¶ä»–ä¼ é€’ç»™OpenTSLMSPçš„å‚æ•°
    """
    
    def __init__(
        self,
        llm_id: str = "meta-llama/Llama-3.2-1B",
        device: str = "cuda",
        encoder_type: str = "transformer_cnn",
        prompt_len: int = 10,
        num_classes: int = 2,
        init_temperature: float = 1.0,
        **kwargs
    ):
        super().__init__(
            llm_id=llm_id,
            device=device,
            encoder_type=encoder_type,
            **kwargs
        )
        
        # è·å–LLMéšå±‚ç»´åº¦
        self.hidden_size = self.llm.config.hidden_size
        self.prompt_len = prompt_len
        self.num_classes = num_classes
        
        # å¯å­¦ä¹ çš„Prompt tokens
        self.prompt_embeds = nn.Parameter(
            torch.randn(prompt_len, self.hidden_size, device=device) * 0.02
        )
        
        # CLS token
        self.cls_embed = nn.Parameter(
            torch.randn(self.hidden_size, device=device) * 0.02
        )
        
        # Prototypeåˆ†ç±»å¤´
        self.cls_head = PrototypeClassificationHead(
            self.hidden_size,
            num_classes,
            init_temperature
        ).to(device)
    
    def freeze_backbone(self):
        """
        Stage 0: å†»ç»“ä¸»å¹²ç½‘ç»œ
        åªè®­ç»ƒ prompt_embeds + cls_embed + cls_head (prototypes + temperature)
        """
        # å†»ç»“ encoder
        for param in self.encoder.parameters():
            param.requires_grad = False
        
        # å†»ç»“ projector
        for param in self.projector.parameters():
            param.requires_grad = False
        
        # å†»ç»“ LLMï¼ˆåŒ…æ‹¬LoRAå¦‚æœæœ‰ï¼‰
        for param in self.llm.parameters():
            param.requires_grad = False
        
        # ç¡®ä¿å¯å­¦ä¹ ç»„ä»¶è§£å†»
        self.prompt_embeds.requires_grad = True
        self.cls_embed.requires_grad = True
        for param in self.cls_head.parameters():
            param.requires_grad = True
        
        print("ğŸ§Š Stage 0: Backbone frozen (encoder + projector + LLM)")
        print("   è®­ç»ƒå‚æ•°: prompt_embeds, cls_embed, cls_head (prototypes + temperature)")
    
    def unfreeze_for_stage1(self, unfreeze_encoder: bool = True):
        """
        Stage 1: è§£å†»ç»„ä»¶è¿›è¡Œè”åˆè®­ç»ƒ
        
        Args:
            unfreeze_encoder: æ˜¯å¦è§£å†»encoderï¼ˆé»˜è®¤Trueï¼‰
        """
        # è§£å†» encoderï¼ˆå¯é€‰ï¼‰
        if unfreeze_encoder:
            for param in self.encoder.parameters():
                param.requires_grad = True
            print("ğŸ”“ Encoder å·²è§£å†»")
        
        # è§£å†» projector
        for param in self.projector.parameters():
            param.requires_grad = True
        print("ğŸ”“ Projector å·²è§£å†»")
        
        # å¦‚æœå¯ç”¨äº†LoRAï¼ŒLoRAå‚æ•°ä¼šè‡ªåŠ¨å¯è®­ç»ƒ
        if self.lora_enabled:
            lora_params = self.get_lora_parameters()
            print(f"ğŸ”“ LoRA å‚æ•°: {len(lora_params)} ä¸ª")
        
        print("âœ… Stage 1: è”åˆè®­ç»ƒæ¨¡å¼")
    
    def _build_prototype_input_embeds(
        self,
        batch: List[Dict[str, any]]
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        æ„å»ºPrototypeæ¨¡å‹çš„è¾“å…¥embedding
        
        è¾“å…¥æ ¼å¼: [Prompt] + [TS_tokens] + [CLS]
        
        Args:
            batch: æ‰¹æ¬¡æ•°æ®ï¼Œæ¯ä¸ªæ ·æœ¬éœ€åŒ…å«:
                - time_series: List[Tensor] æ—¶é—´åºåˆ—æ•°æ®
                - label_index: int ç±»åˆ«ç´¢å¼•ï¼ˆå¯é€‰ï¼Œç”¨äºè®¡ç®—lossï¼‰
        
        Returns:
            inputs_embeds: [B, seq_len, H]
            attention_mask: [B, seq_len]
            cls_positions: [B] CLS tokenåœ¨æ¯ä¸ªæ ·æœ¬ä¸­çš„ä½ç½®
        """
        device = self.device
        B = len(batch)
        H = self.hidden_size
        
        # 1. å¤„ç†æ—¶é—´åºåˆ—
        ts_list = []
        for sample in batch:
            for ts in sample["time_series"]:
                if ts.dim() == 1:
                    ts = ts.unsqueeze(-1)
                ts_list.append(ts)
        
        # Padæ—¶é—´åºåˆ—å¹¶ç¼–ç 
        if ts_list:
            ts_padded = pad_sequence(ts_list, batch_first=True).to(device, non_blocking=True)
            T_max = ts_padded.size(1)
            rem = T_max % self.patch_size
            if rem:
                pad_len = self.patch_size - rem
                pad = ts_padded.new_zeros(ts_padded.size(0), pad_len, ts_padded.size(2))
                ts_padded = torch.cat([ts_padded, pad], dim=1)
            
            # Encode and project
            ts_enc = self.encoder(ts_padded.squeeze(-1))  # [B, N_patches, embed_dim]
            ts_proj = self.projector(ts_enc).to(self.prompt_embeds.dtype)  # [B, N_patches, H]
        else:
            ts_proj = torch.empty(B, 0, H, device=device, dtype=self.prompt_embeds.dtype)
        
        # 2. æ„å»ºæ¯ä¸ªæ ·æœ¬çš„åºåˆ—
        all_seq_embeds = []
        all_seq_masks = []
        cls_positions = []
        
        # Prompt embedding (å…±äº«)
        prompt_embeds = self.prompt_embeds.unsqueeze(0).expand(B, -1, -1)  # [B, prompt_len, H]
        
        ts_offset = 0
        for i, sample in enumerate(batch):
            n_ts = len(sample["time_series"])
            
            # è·å–è¿™ä¸ªæ ·æœ¬çš„æ—¶åºtokens
            sample_ts_embeds = ts_proj[ts_offset:ts_offset + n_ts]  # [n_ts, N_patches, H]
            ts_offset += n_ts
            
            # åˆå¹¶æ‰€æœ‰æ—¶åºçš„patches
            if n_ts > 0:
                ts_tokens = sample_ts_embeds.reshape(-1, H)  # [total_patches, H]
            else:
                ts_tokens = torch.empty(0, H, device=device, dtype=self.prompt_embeds.dtype)
            
            # æ„å»ºåºåˆ—: [Prompt] + [TS_tokens] + [CLS]
            seq_embeds = torch.cat([
                self.prompt_embeds,  # [prompt_len, H]
                ts_tokens,          # [total_patches, H]
                self.cls_embed.unsqueeze(0)  # [1, H]
            ], dim=0)
            
            # è®¡ç®—CLSä½ç½®
            cls_pos = self.prompt_len + ts_tokens.size(0)
            cls_positions.append(cls_pos)
            
            # Attention mask (å…¨1)
            seq_mask = torch.ones(seq_embeds.size(0), device=device, dtype=torch.long)
            
            all_seq_embeds.append(seq_embeds)
            all_seq_masks.append(seq_mask)
        
        # 3. Padåˆ°ç»Ÿä¸€é•¿åº¦
        inputs_embeds = pad_sequence(all_seq_embeds, batch_first=True)  # [B, max_len, H]
        attention_mask = pad_sequence(all_seq_masks, batch_first=True)  # [B, max_len]
        cls_positions = torch.tensor(cls_positions, device=device, dtype=torch.long)  # [B]
        
        return inputs_embeds, attention_mask, cls_positions
    
    def forward_prototype(
        self,
        batch: List[Dict[str, any]],
        return_hidden: bool = False
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Prototypeå‰å‘ä¼ æ’­
        
        Args:
            batch: æ‰¹æ¬¡æ•°æ®ï¼Œæ¯ä¸ªæ ·æœ¬éœ€åŒ…å«:
                - time_series: List[Tensor] æ—¶é—´åºåˆ—æ•°æ®
                - label_index: int ç±»åˆ«ç´¢å¼•
            return_hidden: æ˜¯å¦è¿”å›éšå‘é‡
        
        Returns:
            loss: äº¤å‰ç†µæŸå¤±
            logits: [B, num_classes]
            (å¯é€‰) hidden: [B, H] CLSéšå‘é‡
        """
        # 1. æ„å»ºè¾“å…¥
        inputs_embeds, attention_mask, cls_positions = self._build_prototype_input_embeds(batch)
        
        # 2. LLMå‰å‘ä¼ æ’­
        outputs = self.llm(
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask,
            output_hidden_states=True,
            return_dict=True,
        )
        
        # 3. æå–CLSä½ç½®çš„éšå‘é‡
        # outputs.hidden_states[-1]: [B, seq_len, H] (æœ€åä¸€å±‚çš„éšçŠ¶æ€)
        last_hidden = outputs.hidden_states[-1]
        
        B = len(batch)
        cls_hidden = torch.zeros(B, self.hidden_size, device=self.device, dtype=last_hidden.dtype)
        for i in range(B):
            cls_hidden[i] = last_hidden[i, cls_positions[i], :]
        
        # 4. Prototypeåˆ†ç±»
        logits = self.cls_head(cls_hidden)  # [B, num_classes]
        
        # 5. è®¡ç®—æŸå¤±
        labels = torch.tensor(
            [sample["label_index"] for sample in batch],
            device=self.device,
            dtype=torch.long
        )
        loss = F.cross_entropy(logits, labels)
        
        if return_hidden:
            return loss, logits, cls_hidden
        return loss, logits
    
    def forward(self, batch: List[Dict[str, any]]) -> torch.Tensor:
        """
        DDPå…¼å®¹çš„forwardæ–¹æ³•
        """
        loss, _ = self.forward_prototype(batch)
        return loss
    
    @torch.no_grad()
    def predict(self, batch: List[Dict[str, any]]) -> torch.Tensor:
        """
        é¢„æµ‹ç±»åˆ«
        
        Returns:
            predictions: [B] é¢„æµ‹çš„ç±»åˆ«ç´¢å¼•
        """
        self.eval()
        _, logits = self.forward_prototype(batch)
        return logits.argmax(dim=-1)
    
    def get_trainable_parameters_for_stage(self, stage: int) -> Dict[str, List[torch.nn.Parameter]]:
        """
        è·å–æŒ‡å®šé˜¶æ®µçš„å¯è®­ç»ƒå‚æ•°åˆ†ç»„
        
        Args:
            stage: 0 æˆ– 1
        
        Returns:
            å‚æ•°ç»„å­—å…¸ï¼Œkeyä¸ºç»„åï¼Œvalueä¸ºå‚æ•°åˆ—è¡¨
        """
        param_groups = {}
        
        if stage == 0:
            # Stage 0: åªè®­ç»ƒ prompt + cls + cls_head
            param_groups["prompt_cls"] = [self.prompt_embeds, self.cls_embed]
            param_groups["cls_head"] = list(self.cls_head.parameters())
        
        elif stage == 1:
            # Stage 1: è®­ç»ƒæ›´å¤šç»„ä»¶
            param_groups["encoder"] = list(self.encoder.parameters())
            param_groups["projector"] = list(self.projector.parameters())
            param_groups["prompt_cls"] = [self.prompt_embeds, self.cls_embed]
            param_groups["cls_head"] = list(self.cls_head.parameters())
            
            if self.lora_enabled:
                param_groups["lora"] = self.get_lora_parameters()
        
        return param_groups
    
    def store_to_file(self, path: str):
        """ä¿å­˜æ¨¡å‹åˆ°æ–‡ä»¶"""
        checkpoint = {
            "encoder_state": self.encoder.state_dict(),
            "projector_state": self.projector.state_dict(),
            "prompt_embeds": self.prompt_embeds.data,
            "cls_embed": self.cls_embed.data,
            "cls_head_state": self.cls_head.state_dict(),
            "prompt_len": self.prompt_len,
            "num_classes": self.num_classes,
        }
        
        # LoRAçŠ¶æ€
        self.save_lora_state_to_checkpoint(checkpoint)
        
        torch.save(checkpoint, path)
        print(f"ğŸ’¾ Saved OpenTSLMPrototype to: {path}")
    
    def load_from_file(self, path: str):
        """ä»æ–‡ä»¶åŠ è½½æ¨¡å‹"""
        ckpt = torch.load(path, map_location=self.device, weights_only=False)
        
        self.encoder.load_state_dict(ckpt["encoder_state"])
        self.projector.load_state_dict(ckpt["projector_state"])
        self.prompt_embeds.data = ckpt["prompt_embeds"].to(self.device)
        self.cls_embed.data = ckpt["cls_embed"].to(self.device)
        self.cls_head.load_state_dict(ckpt["cls_head_state"])
        
        # LoRAçŠ¶æ€
        self.load_lora_state_from_checkpoint(ckpt, allow_missing=True)
        
        print(f"ğŸ“¥ Loaded OpenTSLMPrototype from: {path}")
