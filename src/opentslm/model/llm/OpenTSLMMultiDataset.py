# SPDX-FileCopyrightText: 2025 Stanford University, ETH Zurich, and the project authors
# SPDX-License-Identifier: MIT

"""
OpenTSLMMultiDataset: å¤šæ•°æ®é›†ç»Ÿä¸€Prototypeåˆ†ç±»æ¨¡å‹

æ ¸å¿ƒæ¶æ„:
- è¾“å…¥åºåˆ—: [DS_PROMPT_{ds_id}] + [TS_TOKENS] + [CLS]
- æ¯æ•°æ®é›†ç‹¬ç«‹çš„ Prompt (from PromptBank) + Prototype (from PrototypeBank)
- å…±äº«ä¸»å¹²: Encoder + Projector + LLM (with LoRA)

ä¸ OpenTSLMPrototype çš„åŒºåˆ«:
1. ä½¿ç”¨ PromptBank æ›¿ä»£å•ä»½ prompt_embeds
2. ä½¿ç”¨ PrototypeBank æ›¿ä»£å•ä¸ª PrototypeClassificationHead
3. forward æ¥æ”¶åŒä¸€ä¸ª ds_id çš„ batch
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Dict, Tuple, Optional
from torch.nn.utils.rnn import pad_sequence

from .OpenTSLMSP import OpenTSLMSP
from .prototype_banks import PromptBank, PrototypeBank
from opentslm.time_series_datasets.multi_dataset import MultiDatasetRegistry


class OpenTSLMMultiDataset(OpenTSLMSP):
    """
    å¤šæ•°æ®é›†ç»Ÿä¸€Prototypeåˆ†ç±»æ¨¡å‹
    
    è¾“å…¥åºåˆ—ç»“æ„:
        [DS_PROMPT (prompt_len tokens)] + [TS_tokens] + [CLS (1 token)]
    
    æ¯ä¸ªæ•°æ®é›†æœ‰ç‹¬ç«‹çš„:
        - Prompt: ä» PromptBank è·å–
        - Prototype + Temperature: ä» PrototypeBank è·å–
    
    Args:
        registry: MultiDatasetRegistry æ•°æ®é›†æ³¨å†Œè¡¨
        llm_id: LLMæ¨¡å‹ID
        device: è®¾å¤‡
        encoder_type: ç¼–ç å™¨ç±»å‹
        prompt_len: æ¯ä¸ªæ•°æ®é›†çš„prompté•¿åº¦
        init_temperature: Prototypeæ¸©åº¦åˆå§‹å€¼
        **kwargs: å…¶ä»–ä¼ é€’ç»™OpenTSLMSPçš„å‚æ•°
    """
    
    def __init__(
        self,
        registry: MultiDatasetRegistry,
        llm_id: str = "meta-llama/Llama-3.2-1B",
        device: str = "cuda",
        encoder_type: str = "transformer_cnn",
        prompt_len: int = 10,
        init_temperature: float = 1.0,
        **kwargs
    ):
        super().__init__(
            llm_id=llm_id,
            device=device,
            encoder_type=encoder_type,
            **kwargs
        )
        
        self.registry = registry
        self.hidden_size = self.llm.config.hidden_size
        self.prompt_len = prompt_len
        self.num_datasets = registry.get_total_datasets()
        
        # è·å–LLM dtypeå’Œembeddingç»Ÿè®¡ä¿¡æ¯
        llm_dtype = next(self.llm.parameters()).dtype
        with torch.no_grad():
            llm_embeddings = self.llm.get_input_embeddings().weight
            emb_mean = llm_embeddings.mean(dim=0)
            emb_std = llm_embeddings.std(dim=0)
        
        # PromptBank: æ¯æ•°æ®é›†ç‹¬ç«‹çš„prompt
        self.prompt_bank = PromptBank(
            num_datasets=self.num_datasets,
            prompt_len=prompt_len,
            hidden_size=self.hidden_size,
            init_mean=emb_mean,
            init_std=emb_std,
            dtype=llm_dtype,
            device=device,
        )
        
        # PrototypeBank: æ¯æ•°æ®é›†ç‹¬ç«‹çš„prototype + temperature
        class_counts = registry.get_class_counts()
        self.prototype_bank = PrototypeBank(
            class_counts=class_counts,
            hidden_size=self.hidden_size,
            init_temperature=init_temperature,
            init_mean=emb_mean,
            init_std=emb_std,
            dtype=llm_dtype,
            device=device,
        )
        
        # å…±äº«çš„ CLS token
        cls_init = emb_mean + torch.randn(self.hidden_size, device=device, dtype=llm_dtype) * emb_std * 0.1
        self.cls_embed = nn.Parameter(cls_init)
        
        # å…±äº«çš„ CLS æŠ•å½±å±‚ (MLP with residual)
        self.cls_projector = nn.Sequential(
            nn.Linear(self.hidden_size, self.hidden_size, dtype=llm_dtype),
            nn.GELU(),
            nn.Linear(self.hidden_size, self.hidden_size, dtype=llm_dtype)
        ).to(device)
        
        # è¿‘ä¼¼æ’ç­‰åˆå§‹åŒ–
        with torch.no_grad():
            nn.init.eye_(self.cls_projector[0].weight)
            nn.init.zeros_(self.cls_projector[0].bias)
            nn.init.zeros_(self.cls_projector[2].weight)
            nn.init.zeros_(self.cls_projector[2].bias)
        
        print(f"âœ… OpenTSLMMultiDataset initialized: {self.num_datasets} datasets")
    
    def freeze_backbone(self):
        """
        Stage 0: å†»ç»“ä¸»å¹²ç½‘ç»œ
        åªè®­ç»ƒ PromptBank + PrototypeBank + cls_embed + cls_projector
        """
        # å†»ç»“ encoder
        for param in self.encoder.parameters():
            param.requires_grad = False
        
        # å†»ç»“ projector
        for param in self.projector.parameters():
            param.requires_grad = False
        
        # å†»ç»“ LLMï¼ˆåŒ…æ‹¬LoRAå¦‚æœæœ‰ï¼‰
        for param in self.llm.parameters():
            param.requires_grad = False
        
        # ç¡®ä¿å¯å­¦ä¹ ç»„ä»¶è§£å†»
        for param in self.prompt_bank.parameters():
            param.requires_grad = True
        for param in self.prototype_bank.parameters():
            param.requires_grad = True
        self.cls_embed.requires_grad = True
        for param in self.cls_projector.parameters():
            param.requires_grad = True
        
        print("ğŸ§Š Stage 0: Backbone frozen (encoder + projector + LLM)")
        print("   è®­ç»ƒå‚æ•°: PromptBank, PrototypeBank, cls_embed, cls_projector")
    
    def unfreeze_for_stage1(self, unfreeze_encoder: bool = True):
        """
        Stage 1: è§£å†»ç»„ä»¶è¿›è¡Œè”åˆè®­ç»ƒ
        
        Args:
            unfreeze_encoder: æ˜¯å¦è§£å†»encoderï¼ˆé»˜è®¤Trueï¼‰
        """
        # è§£å†» encoderï¼ˆå¯é€‰ï¼‰
        if unfreeze_encoder:
            for param in self.encoder.parameters():
                param.requires_grad = True
            print("ğŸ”“ Encoder å·²è§£å†»")
        
        # è§£å†» projector
        for param in self.projector.parameters():
            param.requires_grad = True
        print("ğŸ”“ Projector å·²è§£å†»")
        
        # å¦‚æœå¯ç”¨äº†LoRAï¼ŒLoRAå‚æ•°ä¼šè‡ªåŠ¨å¯è®­ç»ƒ
        if self.lora_enabled:
            lora_params = self.get_lora_parameters()
            print(f"ğŸ”“ LoRA å‚æ•°: {len(lora_params)} ä¸ª")
        
        print("âœ… Stage 1: è”åˆè®­ç»ƒæ¨¡å¼")
    
    def _build_multi_dataset_input_embeds(
        self,
        batch: List[Dict[str, any]],
        ds_id: int,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        æ„å»ºå¤šæ•°æ®é›†Prototypeæ¨¡å‹çš„è¾“å…¥embedding
        
        è¾“å…¥æ ¼å¼: [DS_PROMPT_{ds_id}] + [TS_tokens] + [CLS]
        
        Args:
            batch: æ‰¹æ¬¡æ•°æ®ï¼Œæ‰€æœ‰æ ·æœ¬å¿…é¡»æ¥è‡ªåŒä¸€ä¸ª ds_id
            ds_id: æ•°æ®é›†ID
        
        Returns:
            inputs_embeds: [B, seq_len, H]
            attention_mask: [B, seq_len]
            cls_positions: [B] CLS tokenåœ¨æ¯ä¸ªæ ·æœ¬ä¸­çš„ä½ç½®
        """
        device = self.device
        B = len(batch)
        H = self.hidden_size
        
        # 1. è·å–è¯¥æ•°æ®é›†çš„prompt embeddings
        ds_prompt = self.prompt_bank.get(ds_id)  # [prompt_len, H]
        
        # 2. å¤„ç†æ—¶é—´åºåˆ—
        ts_list = []
        for sample in batch:
            for ts in sample["time_series"]:
                if ts.dim() == 1:
                    ts = ts.unsqueeze(-1)
                ts_list.append(ts)
        
        # Padæ—¶é—´åºåˆ—å¹¶ç¼–ç 
        if ts_list:
            ts_padded = pad_sequence(ts_list, batch_first=True).to(device, non_blocking=True)
            T_max = ts_padded.size(1)
            rem = T_max % self.patch_size
            if rem:
                pad_len = self.patch_size - rem
                pad = ts_padded.new_zeros(ts_padded.size(0), pad_len, ts_padded.size(2))
                ts_padded = torch.cat([ts_padded, pad], dim=1)
            
            # Encode and project
            ts_enc = self.encoder(ts_padded.squeeze(-1))  # [B, N_patches, embed_dim]
            ts_proj = self.projector(ts_enc).to(ds_prompt.dtype)  # [B, N_patches, H]
        else:
            ts_proj = torch.empty(B, 0, H, device=device, dtype=ds_prompt.dtype)
        
        # 3. æ„å»ºæ¯ä¸ªæ ·æœ¬çš„åºåˆ—
        all_seq_embeds = []
        all_seq_masks = []
        cls_positions = []
        
        ts_offset = 0
        for i, sample in enumerate(batch):
            n_ts = len(sample["time_series"])
            
            # è·å–è¿™ä¸ªæ ·æœ¬çš„æ—¶åºtokens
            sample_ts_embeds = ts_proj[ts_offset:ts_offset + n_ts]  # [n_ts, N_patches, H]
            ts_offset += n_ts
            
            # åˆå¹¶æ‰€æœ‰æ—¶åºçš„patches
            if n_ts > 0:
                ts_tokens = sample_ts_embeds.reshape(-1, H)  # [total_patches, H]
            else:
                ts_tokens = torch.empty(0, H, device=device, dtype=ds_prompt.dtype)
            
            # æ„å»ºåºåˆ—: [DS_PROMPT] + [TS_tokens] + [CLS]
            seq_embeds = torch.cat([
                ds_prompt,                      # [prompt_len, H]
                ts_tokens,                      # [total_patches, H]
                self.cls_embed.unsqueeze(0)     # [1, H]
            ], dim=0)
            
            # è®¡ç®—CLSä½ç½®
            cls_pos = self.prompt_len + ts_tokens.size(0)
            cls_positions.append(cls_pos)
            
            # Attention mask (å…¨1)
            seq_mask = torch.ones(seq_embeds.size(0), device=device, dtype=torch.long)
            
            all_seq_embeds.append(seq_embeds)
            all_seq_masks.append(seq_mask)
        
        # 4. Padåˆ°ç»Ÿä¸€é•¿åº¦
        inputs_embeds = pad_sequence(all_seq_embeds, batch_first=True)  # [B, max_len, H]
        attention_mask = pad_sequence(all_seq_masks, batch_first=True)  # [B, max_len]
        cls_positions = torch.tensor(cls_positions, device=device, dtype=torch.long)  # [B]
        
        return inputs_embeds, attention_mask, cls_positions
    
    def forward_prototype(
        self,
        batch: List[Dict[str, any]],
        return_hidden: bool = False
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Prototypeå‰å‘ä¼ æ’­
        
        è¦æ±‚: batch ä¸­æ‰€æœ‰æ ·æœ¬å¿…é¡»æ¥è‡ªåŒä¸€ä¸ª ds_id
        
        Args:
            batch: æ‰¹æ¬¡æ•°æ®ï¼Œæ¯ä¸ªæ ·æœ¬éœ€åŒ…å«:
                - time_series: List[Tensor] æ—¶é—´åºåˆ—æ•°æ®
                - label_index: int ç±»åˆ«ç´¢å¼• (è¯¥æ•°æ®é›†å†…éƒ¨ç´¢å¼•)
                - ds_id: int æ•°æ®é›†ID
            return_hidden: æ˜¯å¦è¿”å›éšå‘é‡
        
        Returns:
            loss: äº¤å‰ç†µæŸå¤±
            logits: [B, num_classes_ds]
            (å¯é€‰) hidden: [B, H] CLSéšå‘é‡
        """
        # éªŒè¯åŒä¸€ ds_id
        ds_ids = set(sample["ds_id"] for sample in batch)
        assert len(ds_ids) == 1, f"Batch must contain samples from single dataset, got {ds_ids}"
        ds_id = batch[0]["ds_id"]
        
        # 1. æ„å»ºè¾“å…¥
        inputs_embeds, attention_mask, cls_positions = self._build_multi_dataset_input_embeds(batch, ds_id)
        
        # 2. LLMå‰å‘ä¼ æ’­
        outputs = self.llm(
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask,
            output_hidden_states=True,
            return_dict=True,
        )
        
        # 3. æå–CLSä½ç½®çš„éšå‘é‡
        last_hidden = outputs.hidden_states[-1]
        
        B = len(batch)
        cls_hidden = torch.zeros(B, self.hidden_size, device=self.device, dtype=last_hidden.dtype)
        for i in range(B):
            cls_hidden[i] = last_hidden[i, cls_positions[i], :]
        
        # 4. æŠ•å½±CLSéšå‘é‡ï¼ˆæ®‹å·®è¿æ¥ï¼‰
        cls_projected = cls_hidden + self.cls_projector(cls_hidden)
        
        # 5. ä½¿ç”¨è¯¥æ•°æ®é›†çš„Prototypeè®¡ç®—logits
        logits = self.prototype_bank.logits(ds_id, cls_projected)  # [B, num_classes_ds]
        
        # 6. è®¡ç®—æŸå¤±
        labels = torch.tensor(
            [sample["label_index"] for sample in batch],
            device=self.device,
            dtype=torch.long
        )
        loss = F.cross_entropy(logits, labels)
        
        if return_hidden:
            return loss, logits, cls_hidden
        return loss, logits
    
    def forward(self, batch: List[Dict[str, any]]) -> torch.Tensor:
        """DDPå…¼å®¹çš„forwardæ–¹æ³•"""
        loss, _ = self.forward_prototype(batch)
        return loss
    
    @torch.no_grad()
    def predict(self, batch: List[Dict[str, any]]) -> torch.Tensor:
        """
        é¢„æµ‹ç±»åˆ«
        
        Returns:
            predictions: [B] é¢„æµ‹çš„ç±»åˆ«ç´¢å¼•
        """
        self.eval()
        _, logits = self.forward_prototype(batch)
        return logits.argmax(dim=-1)
    
    def get_trainable_parameters_for_stage(self, stage: int) -> Dict[str, List[torch.nn.Parameter]]:
        """
        è·å–æŒ‡å®šé˜¶æ®µçš„å¯è®­ç»ƒå‚æ•°åˆ†ç»„
        
        Args:
            stage: 0 æˆ– 1
        
        Returns:
            å‚æ•°ç»„å­—å…¸ï¼Œkeyä¸ºç»„åï¼Œvalueä¸ºå‚æ•°åˆ—è¡¨
        """
        param_groups = {}
        
        if stage == 0:
            # Stage 0: åªè®­ç»ƒ banks + cls
            param_groups["prompt_bank"] = list(self.prompt_bank.parameters())
            param_groups["prototype_bank"] = list(self.prototype_bank.parameters())
            param_groups["cls"] = [self.cls_embed] + list(self.cls_projector.parameters())
        
        elif stage == 1:
            # Stage 1: è®­ç»ƒæ›´å¤šç»„ä»¶
            param_groups["encoder"] = list(self.encoder.parameters())
            param_groups["projector"] = list(self.projector.parameters())
            param_groups["prompt_bank"] = list(self.prompt_bank.parameters())
            param_groups["prototype_bank"] = list(self.prototype_bank.parameters())
            param_groups["cls"] = [self.cls_embed] + list(self.cls_projector.parameters())
            
            if self.lora_enabled:
                param_groups["lora"] = self.get_lora_parameters()
        
        return param_groups
    
    def store_to_file(self, path: str):
        """ä¿å­˜æ¨¡å‹åˆ°æ–‡ä»¶"""
        checkpoint = {
            "encoder_state": self.encoder.state_dict(),
            "projector_state": self.projector.state_dict(),
            "prompt_bank_state": self.prompt_bank.state_dict(),
            "prototype_bank_state": self.prototype_bank.state_dict(),
            "cls_embed": self.cls_embed.data,
            "cls_projector_state": self.cls_projector.state_dict(),
            "prompt_len": self.prompt_len,
            "num_datasets": self.num_datasets,
        }
        
        # LoRAçŠ¶æ€
        self.save_lora_state_to_checkpoint(checkpoint)
        
        torch.save(checkpoint, path)
        print(f"ğŸ’¾ Saved OpenTSLMMultiDataset to: {path}")
    
    def load_from_file(self, path: str):
        """ä»æ–‡ä»¶åŠ è½½æ¨¡å‹"""
        ckpt = torch.load(path, map_location=self.device, weights_only=False)
        
        self.encoder.load_state_dict(ckpt["encoder_state"])
        self.projector.load_state_dict(ckpt["projector_state"])
        
        if "prompt_bank_state" in ckpt:
            self.prompt_bank.load_state_dict(ckpt["prompt_bank_state"])
        if "prototype_bank_state" in ckpt:
            self.prototype_bank.load_state_dict(ckpt["prototype_bank_state"])
        
        self.cls_embed.data = ckpt["cls_embed"].to(self.device)
        if "cls_projector_state" in ckpt:
            self.cls_projector.load_state_dict(ckpt["cls_projector_state"])
        
        # LoRAçŠ¶æ€
        self.load_lora_state_from_checkpoint(ckpt, allow_missing=True)
        
        print(f"ğŸ“¥ Loaded OpenTSLMMultiDataset from: {path}")
