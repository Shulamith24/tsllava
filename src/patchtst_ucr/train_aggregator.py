#!/usr/bin/env python3
# SPDX-FileCopyrightText: 2025 Stanford University, ETH Zurich, and the project authors
# SPDX-License-Identifier: MIT

"""
PatchTST + Transformer èšåˆå¤´åœ¨ UCR æ•°æ®é›†ä¸Šçš„åˆ†ç±»

ä½¿ç”¨ PatchTST backbone æå– patch ç‰¹å¾ï¼Œç„¶åé€šè¿‡ Transformer èšåˆå¤´è¿›è¡Œåˆ†ç±»
ç‹¬ç«‹æ¨¡å—ï¼Œä¸ä¾èµ– opentslm åŒ…

ä½¿ç”¨æ–¹æ³•ï¼š
    python -m patchtst_ucr.train_aggregator --dataset Adiac --epochs 50

    # è‡ªå®šä¹‰èšåˆå¤´é…ç½®
    python -m patchtst_ucr.train_aggregator --dataset Adiac --aggregator_layers 2

    # å†»ç»“ backbone
    python -m patchtst_ucr.train_aggregator --dataset Adiac --freeze_backbone
"""

import os
import sys
import json
import argparse
import datetime
from pathlib import Path
from typing import List, Dict, Any

import torch
from torch.optim import AdamW
from torch.utils.data import DataLoader
from tqdm.auto import tqdm
from transformers import get_cosine_schedule_with_warmup

# æ·»åŠ  src ç›®å½•åˆ°è·¯å¾„ï¼ˆæ”¯æŒç›´æ¥è¿è¡Œï¼‰
script_dir = Path(__file__).parent
src_dir = script_dir.parent
if str(src_dir) not in sys.path:
    sys.path.insert(0, str(src_dir))

from patchtst_ucr.model import PatchTSTWithAggregator
from patchtst_ucr.ucr_dataset import UCRDatasetForPatchTST, get_dataset_info


def parse_args():
    parser = argparse.ArgumentParser(description="PatchTST + Aggregator UCR åˆ†ç±»")

    # æ•°æ®ç›¸å…³
    parser.add_argument("--dataset", type=str, default="Adiac", help="UCRæ•°æ®é›†åç§°")
    parser.add_argument("--data_path", type=str, default="./data", help="UCRæ•°æ®æ ¹ç›®å½•")
    
    # PatchTST æ¨¡å‹é…ç½®
    parser.add_argument("--context_length", type=int, default=None, 
                       help="ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆNoneåˆ™è‡ªåŠ¨è®¾ç½®ä¸ºæ•°æ®é›†æœ€å¤§é•¿åº¦ï¼‰")
    parser.add_argument("--patch_length", type=int, default=16, help="Patch é•¿åº¦")
    parser.add_argument("--stride", type=int, default=8, help="Patch æ­¥é•¿")
    parser.add_argument("--d_model", type=int, default=128, help="PatchTST æ¨¡å‹ç»´åº¦")
    parser.add_argument("--num_attention_heads", type=int, default=8, help="PatchTST Attention heads")
    parser.add_argument("--num_hidden_layers", type=int, default=3, help="PatchTST Transformer å±‚æ•°")
    parser.add_argument("--ffn_dim", type=int, default=512, help="PatchTST FFN ç»´åº¦")
    parser.add_argument("--dropout", type=float, default=0.1, help="Dropout")
    
    # èšåˆå¤´é…ç½®ï¼ˆæ ¸å¿ƒå‚æ•°ï¼‰
    parser.add_argument("--aggregator_layers", type=int, default=1, help="èšåˆå¤´ Transformer å±‚æ•°")
    parser.add_argument("--aggregator_hidden_size", type=int, default=None, 
                       help="èšåˆå¤´ hidden sizeï¼ˆNoneåˆ™ä¸d_modelç›¸åŒï¼‰")
    parser.add_argument("--aggregator_num_heads", type=int, default=8, help="èšåˆå¤´ attention heads")
    parser.add_argument("--aggregator_ffn_dim", type=int, default=None, 
                       help="èšåˆå¤´ FFN ç»´åº¦ï¼ˆNoneåˆ™è‡ªåŠ¨è®¡ç®—ï¼‰")
    
    # å†»ç»“é€‰é¡¹
    parser.add_argument("--freeze_backbone", action="store_true", help="å†»ç»“ PatchTST backbone")
    
    # è®­ç»ƒç›¸å…³
    parser.add_argument("--epochs", type=int, default=50, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=32, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--lr", type=float, default=1e-3, help="å­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=1e-4, help="æƒé‡è¡°å‡")
    parser.add_argument("--warmup_ratio", type=float, default=0.1, help="é¢„çƒ­æ¯”ä¾‹")
    parser.add_argument("--grad_clip", type=float, default=1.0, help="æ¢¯åº¦è£å‰ª")
    
    # ä¿å­˜ç›¸å…³
    parser.add_argument("--save_dir", type=str, default="results/patchtst_aggregator", help="ç»“æœä¿å­˜ç›®å½•")
    
    # å…¶ä»–
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--device", type=str, default="cuda", help="è®¾å¤‡")
    parser.add_argument("--eval_every", type=int, default=5, help="æ¯Nè½®è¯„ä¼°ä¸€æ¬¡")
    parser.add_argument("--early_stop", type=int, default=15, help="æ—©åœè€å¿ƒå€¼")
    parser.add_argument("--eval_batch_size", type=int, default=64, help="è¯„ä¼°æ‰¹æ¬¡å¤§å°")
    
    return parser.parse_args()


def set_seed(seed: int):
    """è®¾ç½®éšæœºç§å­"""
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    import random
    import numpy as np
    random.seed(seed)
    np.random.seed(seed)


def prepare_batch(
    batch: List[Dict],
    context_length: int,
    device: str,
):
    """
    å°† UCR æ‰¹æ¬¡è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼
    
    Returns:
        past_values: [B, context_length, 1]
        labels: [B]
    """
    past_values_list = []
    labels = []
    
    for sample in batch:
        ts = sample["time_series"][0]
        
        if not isinstance(ts, torch.Tensor):
            ts = torch.tensor(ts, dtype=torch.float32)
        
        if len(ts) < context_length:
            padded = torch.zeros(context_length, device=device)
            padded[:len(ts)] = ts.to(device)
        else:
            padded = ts[:context_length].to(device)
        
        past_values_list.append(padded.unsqueeze(-1))
        labels.append(sample["int_label"])
    
    past_values = torch.stack(past_values_list, dim=0)
    labels = torch.tensor(labels, device=device, dtype=torch.long)
    
    return past_values, labels


def create_data_loaders(args, num_classes: int, context_length: int):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    train_dataset = UCRDatasetForPatchTST(
        dataset_name=args.dataset,
        split="train",
        raw_data_path=args.data_path,
    )
    
    val_dataset = UCRDatasetForPatchTST(
        dataset_name=args.dataset,
        split="validation",
        raw_data_path=args.data_path,
    )
    
    test_dataset = UCRDatasetForPatchTST(
        dataset_name=args.dataset,
        split="test",
        raw_data_path=args.data_path,
    )
    
    def collate_fn(batch):
        return batch
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        collate_fn=collate_fn,
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.eval_batch_size,
        shuffle=False,
        collate_fn=collate_fn,
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=args.eval_batch_size,
        shuffle=False,
        collate_fn=collate_fn,
    )
    
    return train_loader, val_loader, test_loader


def train_one_epoch(
    model,
    train_loader: DataLoader,
    optimizer,
    scheduler,
    context_length: int,
    grad_clip: float,
    device: str,
    epoch: int,
    num_epochs: int,
) -> float:
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0.0
    num_batches = 0
    
    pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{num_epochs}")
    for batch in pbar:
        past_values, labels = prepare_batch(batch, context_length, device)
        
        outputs = model(past_values=past_values, labels=labels)
        loss = outputs["loss"]
        
        optimizer.zero_grad()
        loss.backward()
        
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)
        
        optimizer.step()
        scheduler.step()
        
        total_loss += loss.item()
        num_batches += 1
        
        pbar.set_postfix({
            "loss": f"{loss.item():.4f}",
            "lr": f"{scheduler.get_last_lr()[0]:.2e}"
        })
    
    return total_loss / max(num_batches, 1)


@torch.no_grad()
def evaluate(
    model,
    data_loader: DataLoader,
    context_length: int,
    device: str,
    desc: str = "Evaluating",
) -> Dict[str, Any]:
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    
    all_predictions = []
    all_labels = []
    total_loss = 0.0
    num_batches = 0
    
    for batch in tqdm(data_loader, desc=desc):
        past_values, labels = prepare_batch(batch, context_length, device)
        
        outputs = model(past_values=past_values, labels=labels)
        
        total_loss += outputs["loss"].item()
        num_batches += 1
        
        predictions = torch.argmax(outputs["logits"], dim=-1)
        
        all_predictions.extend(predictions.cpu().tolist())
        all_labels.extend(labels.cpu().tolist())
    
    avg_loss = total_loss / max(num_batches, 1)
    correct = sum(p == l for p, l in zip(all_predictions, all_labels))
    accuracy = correct / len(all_labels) if all_labels else 0.0
    
    return {
        "loss": avg_loss,
        "accuracy": accuracy,
        "predictions": all_predictions,
        "labels": all_labels,
    }


def main():
    args = parse_args()
    
    print("=" * 60)
    print("PatchTST + Transformer èšåˆå¤´ UCR åˆ†ç±»")
    print("=" * 60)
    print(f"æ—¶é—´: {datetime.datetime.now()}")
    print(f"æ•°æ®é›†: {args.dataset}")
    print(f"èšåˆå¤´å±‚æ•°: {args.aggregator_layers}")
    print(f"å†»ç»“ backbone: {args.freeze_backbone}")
    print("=" * 60)
    
    set_seed(args.seed)
    
    device = args.device if torch.cuda.is_available() else "cpu"
    print(f"\nä½¿ç”¨è®¾å¤‡: {device}")
    
    print("\nğŸ“‚ åˆ†ææ•°æ®é›†...")
    num_classes, max_length = get_dataset_info(args.dataset, args.data_path)
    
    if args.context_length is None:
        context_length = ((max_length - 1) // args.patch_length + 1) * args.patch_length
    else:
        context_length = args.context_length
    
    print(f"   ç±»åˆ«æ•°: {num_classes}")
    print(f"   æœ€å¤§é•¿åº¦: {max_length}")
    print(f"   Context length: {context_length}")
    
    num_patches = (context_length - args.patch_length) // args.stride + 1
    print(f"   é¢„æœŸ patch æ•°: {num_patches}")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    agg_str = f"L{args.aggregator_layers}"
    if args.aggregator_hidden_size:
        agg_str += f"_H{args.aggregator_hidden_size}"
    if args.freeze_backbone:
        agg_str += "_frozen"
    
    save_dir = os.path.join(
        args.save_dir, 
        args.dataset, 
        f"P{args.patch_length}_S{args.stride}_{agg_str}"
    )
    os.makedirs(save_dir, exist_ok=True)
    with open(os.path.join(save_dir, "config.json"), "w") as f:
        json.dump(vars(args), f, indent=2)
    
    print("\nğŸ”§ åˆ›å»ºæ¨¡å‹...")
    model = PatchTSTWithAggregator(
        num_classes=num_classes,
        context_length=context_length,
        patch_length=args.patch_length,
        stride=args.stride,
        d_model=args.d_model,
        num_attention_heads=args.num_attention_heads,
        num_hidden_layers=args.num_hidden_layers,
        ffn_dim=args.ffn_dim,
        dropout=args.dropout,
        aggregator_layers=args.aggregator_layers,
        aggregator_hidden_size=args.aggregator_hidden_size,
        aggregator_num_heads=args.aggregator_num_heads,
        aggregator_ffn_dim=args.aggregator_ffn_dim,
        device=device,
    ).to(device)
    
    if args.freeze_backbone:
        model.freeze_backbone()
    
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    print("\nğŸ“‚ åŠ è½½æ•°æ®...")
    train_loader, val_loader, test_loader = create_data_loaders(
        args, num_classes, context_length
    )
    
    print(f"   Train batches: {len(train_loader)}")
    print(f"   Val batches: {len(val_loader)}")
    print(f"   Test batches: {len(test_loader)}")
    
    print("\nâš™ï¸  åˆ›å»ºä¼˜åŒ–å™¨...")
    optimizer = AdamW(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=args.lr,
        weight_decay=args.weight_decay,
    )
    
    total_steps = args.epochs * len(train_loader)
    warmup_steps = int(args.warmup_ratio * total_steps)
    
    scheduler = get_cosine_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_steps,
    )
    
    print(f"   Total steps: {total_steps}")
    print(f"   Warmup steps: {warmup_steps}")
    
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    best_val_acc = 0.0
    patience_counter = 0
    loss_history = []
    
    try:
        for epoch in range(1, args.epochs + 1):
            train_loss = train_one_epoch(
                model, train_loader, optimizer, scheduler,
                context_length, args.grad_clip, device,
                epoch, args.epochs
            )
            
            if epoch % args.eval_every == 0 or epoch == args.epochs:
                print(f"\nğŸ“Š Epoch {epoch} è¯„ä¼°...")
                
                val_results = evaluate(
                    model, val_loader, context_length, device, "Validating"
                )
                val_loss = val_results["loss"]
                val_acc = val_results["accuracy"]
                
                print(f"   Train Loss: {train_loss:.4f}")
                print(f"   Val Loss: {val_loss:.4f}")
                print(f"   Val Accuracy: {val_acc:.4f}")
                
                print("   Sample predictions (first 5):")
                for i in range(min(5, len(val_results["predictions"]))):
                    pred = val_results["predictions"][i]
                    label = val_results["labels"][i]
                    print(f"     Pred: {pred} | Label: {label} | {'âœ“' if pred == label else 'âœ—'}")
                
                if val_acc > best_val_acc:
                    best_val_acc = val_acc
                    patience_counter = 0
                    
                    checkpoint = {
                        "model_state": model.state_dict(),
                        "optimizer_state": optimizer.state_dict(),
                        "scheduler_state": scheduler.state_dict(),
                        "epoch": epoch,
                        "val_loss": val_loss,
                        "val_acc": val_acc,
                        "config": model.get_config(),
                        "args": vars(args),
                    }
                    torch.save(checkpoint, os.path.join(save_dir, "best_model.pt"))
                    print(f"   ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹")
                else:
                    patience_counter += 1
                    print(f"   (æ— æ”¹è¿›, patience: {patience_counter}/{args.early_stop})")
                
                loss_history.append({
                    "epoch": epoch,
                    "train_loss": train_loss,
                    "val_loss": val_loss,
                    "val_acc": val_acc,
                })
                with open(os.path.join(save_dir, "loss_history.json"), "w") as f:
                    json.dump(loss_history, f, indent=2)
            else:
                print(f"Epoch {epoch}: Train Loss = {train_loss:.4f}")
            
            if patience_counter >= args.early_stop:
                print(f"\nâ¹ï¸  æ—©åœ! éªŒè¯å‡†ç¡®ç‡ {args.early_stop} è½®æœªæ”¹è¿›")
                break
        
        print("\n" + "=" * 60)
        print("ğŸ“‹ æœ€ç»ˆæµ‹è¯•è¯„ä¼°...")
        
        best_ckpt = torch.load(
            os.path.join(save_dir, "best_model.pt"),
            map_location=device,
            weights_only=False
        )
        model.load_state_dict(best_ckpt["model_state"])
        
        test_results = evaluate(
            model, test_loader, context_length, device, "Testing"
        )
        
        print(f"\nâœ… æµ‹è¯•ç»“æœ:")
        print(f"   Test Loss: {test_results['loss']:.4f}")
        print(f"   Test Accuracy: {test_results['accuracy']:.4f}")
        
        final_results = {
            "dataset": args.dataset,
            "num_classes": num_classes,
            "context_length": context_length,
            "aggregator_layers": args.aggregator_layers,
            "aggregator_hidden_size": args.aggregator_hidden_size or args.d_model,
            "freeze_backbone": args.freeze_backbone,
            "total_params": model.count_parameters(),
            "best_val_acc": best_val_acc,
            "test_loss": test_results["loss"],
            "test_accuracy": test_results["accuracy"],
            "epochs_trained": epoch,
        }
        
        with open(os.path.join(save_dir, "final_results.json"), "w") as f:
            json.dump(final_results, f, indent=2)
        
        with open(os.path.join(save_dir, "test_predictions.json"), "w") as f:
            json.dump({
                "predictions": test_results["predictions"],
                "labels": test_results["labels"],
            }, f, indent=2)
        
        print("=" * 60)
        print(f"ç»“æœä¿å­˜åˆ°: {save_dir}")
        print("=" * 60)
    
    except KeyboardInterrupt:
        print("\nâš ï¸  è®­ç»ƒè¢«ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
