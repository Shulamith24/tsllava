# SPDX-FileCopyrightText: 2025 Stanford University, ETH Zurich, and the project authors
# SPDX-License-Identifier: MIT

"""
PatchTSTWithAggregator: PatchTST Backbone + Transformer èšåˆå¤´

æ ¸å¿ƒè®¾è®¡ï¼š
- ä½¿ç”¨ HuggingFace PatchTSTModel (use_cls_token=False) æå– patch çº§ç‰¹å¾
- æ·»åŠ å¯å­¦ä¹  [ANS] token åˆ°åºåˆ—æœ«å°¾
- ä½¿ç”¨ SmallTransformerAggregator è¿›è¡Œç‰¹å¾èšåˆ
- æå– [ANS] ä½ç½®çš„ hidden state è¿›è¡Œåˆ†ç±»
"""

import torch
import torch.nn as nn
from typing import Optional, Dict, Any, Literal
from transformers import PatchTSTConfig, PatchTSTModel

from .aggregator import SmallTransformerAggregator
from .projector import MLPProjector, LinearProjector


class PatchTSTWithAggregator(nn.Module):
    """
    PatchTST Backbone + SmallTransformerAggregator åˆ†ç±»å™¨
    
    Args:
        num_classes: åˆ†ç±»ç±»åˆ«æ•°
        context_length: è¾“å…¥åºåˆ—é•¿åº¦
        patch_length: Patch é•¿åº¦
        stride: Patch æ­¥é•¿
        d_model: PatchTST æ¨¡å‹ç»´åº¦
        num_attention_heads: PatchTST attention heads
        num_hidden_layers: PatchTST Transformer å±‚æ•°
        ffn_dim: PatchTST FFN ç»´åº¦
        dropout: Dropout æ¦‚ç‡
        aggregator_layers: èšåˆå¤´ Transformer å±‚æ•°
        aggregator_hidden_size: èšåˆå¤´ hidden size (None åˆ™ä¸ d_model ç›¸åŒ)
        aggregator_num_heads: èšåˆå¤´ attention heads
        aggregator_ffn_dim: èšåˆå¤´ FFN ç»´åº¦ (None åˆ™è‡ªåŠ¨è®¡ç®—)
    """

    def __init__(
        self,
        num_classes: int,
        context_length: int,
        patch_length: int = 16,
        stride: int = 8,
        d_model: int = 128,
        num_attention_heads: int = 8,
        num_hidden_layers: int = 3,
        ffn_dim: int = 512,
        dropout: float = 0.1,
        # èšåˆå¤´å‚æ•°
        aggregator_layers: int = 1,
        aggregator_hidden_size: Optional[int] = None,
        aggregator_num_heads: int = 8,
        aggregator_ffn_dim: Optional[int] = None,
        # æŠ•å½±å±‚å‚æ•°
        projector_type: Literal["mlp", "linear", "none"] = "mlp",
        projector_dropout: float = 0.1,
        device: str = "cuda",
    ):
        super().__init__()
        
        self.num_classes = num_classes
        self.context_length = context_length
        self.d_model = d_model
        self.device = device
        self.projector_type = projector_type
        
        # 1) PatchTST Backbone (use_cls_token=False)
        patchtst_config = PatchTSTConfig(
            num_input_channels=1,  # å•å˜é‡æ—¶é—´åºåˆ—
            context_length=context_length,
            patch_length=patch_length,
            stride=stride,
            d_model=d_model,
            num_attention_heads=num_attention_heads,
            num_hidden_layers=num_hidden_layers,
            ffn_dim=ffn_dim,
            dropout=dropout,
            use_cls_token=False,  # ä¸ä½¿ç”¨ CLS tokenï¼Œè¾“å‡ºçº¯ patch ç‰¹å¾
        )
        
        self.backbone = PatchTSTModel(config=patchtst_config)
        
        # è®¡ç®— patch æ•°é‡
        self.num_patches = (context_length - patch_length) // stride + 1
        
        # 2) èšåˆå¤´é…ç½®
        self.aggregator_hidden_size = aggregator_hidden_size or d_model
        self.aggregator_ffn_dim = aggregator_ffn_dim or (self.aggregator_hidden_size * 4)
        
        # 3) æŠ•å½±å±‚ï¼ˆæ ¹æ®ç±»å‹é€‰æ‹©ï¼‰
        if projector_type == "none":
            # æ— æŠ•å½±å±‚ï¼Œå¼ºåˆ¶aggregatorç»´åº¦ä¸d_modelç›¸åŒ
            if aggregator_hidden_size is not None and aggregator_hidden_size != d_model:
                print(f"âš ï¸  projector_type='none' æ—¶ï¼Œaggregator_hidden_sizeè¢«å¼ºåˆ¶è®¾ä¸º{d_model}")
            self.aggregator_hidden_size = d_model
            self.projector = None
        elif self.aggregator_hidden_size != d_model:
            # éœ€è¦æŠ•å½±å±‚
            if projector_type == "mlp":
                self.projector = MLPProjector(d_model, self.aggregator_hidden_size, dropout=projector_dropout)
            elif projector_type == "linear":
                self.projector = LinearProjector(d_model, self.aggregator_hidden_size)
            else:
                raise ValueError(f"Unknown projector_type: {projector_type}")
        else:
            # ç»´åº¦ç›¸åŒï¼Œä¸éœ€è¦æŠ•å½±
            self.projector = None
        
        # 4) èšåˆå¤´
        self.aggregator = SmallTransformerAggregator(
            num_layers=aggregator_layers,
            hidden_size=self.aggregator_hidden_size,
            num_heads=aggregator_num_heads,
            ffn_dim=self.aggregator_ffn_dim,
            dropout=dropout,
        )
        
        # 4) å¯å­¦ä¹  [ANS] token
        self.ans_token = nn.Parameter(
            torch.randn(1, 1, self.aggregator_hidden_size) * 0.02
        )
        
        # 5) åˆ†ç±»å¤´
        self.classifier_head = nn.Linear(self.aggregator_hidden_size, num_classes)
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        self._print_model_info()
    
    def _print_model_info(self):
        """æ‰“å°æ¨¡å‹ä¿¡æ¯"""
        backbone_params = sum(p.numel() for p in self.backbone.parameters())
        aggregator_params = self.aggregator.count_parameters()
        total_params = sum(p.numel() for p in self.parameters())
        
        print(f"\n{'='*60}")
        print(f"PatchTSTWithAggregator æ¨¡å‹ä¿¡æ¯")
        print(f"{'='*60}")
        print(f"PatchTST Backbone:")
        print(f"  - context_length: {self.context_length}")
        print(f"  - num_patches: {self.num_patches}")
        print(f"  - d_model: {self.d_model}")
        print(f"  - å‚æ•°é‡: {backbone_params:,}")
        print(f"Aggregator:")
        print(f"  - å±‚æ•°: {self.aggregator.num_layers}")
        print(f"  - hidden_size: {self.aggregator_hidden_size}")
        print(f"  - å‚æ•°é‡: {aggregator_params:,}")
        print(f"æ€»å‚æ•°é‡: {total_params:,}")
        print(f"{'='*60}\n")
    
    def freeze_backbone(self):
        """å†»ç»“ PatchTST backbone å‚æ•°"""
        for param in self.backbone.parameters():
            param.requires_grad = False
        print("ğŸ§Š PatchTST backbone å·²å†»ç»“")
    
    def unfreeze_backbone(self):
        """è§£å†» PatchTST backbone å‚æ•°"""
        for param in self.backbone.parameters():
            param.requires_grad = True
        print("ğŸ”¥ PatchTST backbone å·²è§£å†»")
    
    def forward(
        self,
        past_values: torch.Tensor,
        labels: Optional[torch.Tensor] = None,
    ) -> Dict[str, Any]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            past_values: [B, context_length, 1] è¾“å…¥æ—¶é—´åºåˆ—
            labels: [B] å¯é€‰çš„åˆ†ç±»æ ‡ç­¾
            
        Returns:
            åŒ…å« loss (å¦‚æœæä¾› labels) å’Œ logits çš„å­—å…¸
        """
        B = past_values.size(0)
        device = past_values.device
        
        # 1) PatchTST backbone æå–ç‰¹å¾
        backbone_output = self.backbone(past_values=past_values)
        # PatchTST è¾“å‡º: [B, num_channels, num_patches, d_model]
        # å¯¹äºå•å˜é‡ (num_channels=1)ï¼Œsqueeze æ‰ channel ç»´åº¦
        patch_embeddings = backbone_output.last_hidden_state  # [B, 1, num_patches, d_model]
        if patch_embeddings.dim() == 4:
            patch_embeddings = patch_embeddings.squeeze(1)  # [B, num_patches, d_model]
        
        # 2) æŠ•å½±åˆ°èšåˆå¤´ç»´åº¦ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.projector is not None:
            patch_embeddings = self.projector(patch_embeddings)  # [B, num_patches, aggregator_hidden_size]
        
        # 3) æ·»åŠ  [ANS] token
        ans_tokens = self.ans_token.expand(B, -1, -1).to(device)  # [B, 1, aggregator_hidden_size]
        sequence = torch.cat([patch_embeddings, ans_tokens], dim=1)  # [B, num_patches+1, H]
        
        # 4) èšåˆå¤´å¤„ç†
        hidden_states = self.aggregator(sequence)  # [B, num_patches+1, H]
        
        # 5) æå– [ANS] ä½ç½®çš„ hidden stateï¼ˆæœ€åä¸€ä¸ªä½ç½®ï¼‰
        ans_hidden = hidden_states[:, -1, :]  # [B, H]
        
        # 6) åˆ†ç±»
        logits = self.classifier_head(ans_hidden)  # [B, num_classes]
        
        # 7) è®¡ç®—æŸå¤±ï¼ˆå¦‚æœæä¾›æ ‡ç­¾ï¼‰
        loss = None
        if labels is not None:
            loss = nn.functional.cross_entropy(logits, labels)
        
        return {
            "loss": loss,
            "logits": logits,
            "ans_hidden": ans_hidden,
        }
    
    def predict(self, past_values: torch.Tensor) -> torch.Tensor:
        """é¢„æµ‹ç±»åˆ«"""
        with torch.no_grad():
            outputs = self.forward(past_values)
            predictions = torch.argmax(outputs["logits"], dim=-1)
        return predictions
    
    def count_parameters(self) -> int:
        """è®¡ç®—æ€»å‚æ•°é‡"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)
    
    def get_config(self) -> Dict[str, Any]:
        """è¿”å›é…ç½®"""
        return {
            "num_classes": self.num_classes,
            "context_length": self.context_length,
            "num_patches": self.num_patches,
            "d_model": self.d_model,
            "aggregator_layers": self.aggregator.num_layers,
            "aggregator_hidden_size": self.aggregator_hidden_size,
            "total_params": self.count_parameters(),
        }
