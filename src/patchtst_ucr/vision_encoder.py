# SPDX-FileCopyrightText: 2025 Stanford University, ETH Zurich, and the project authors
# SPDX-License-Identifier: MIT

"""
VisionEncoder: æ—¶åºå›¾åƒåŒ– + ViT ç¼–ç å™¨

æ ¸å¿ƒè®¾è®¡ï¼š
- å°†æ—¶é—´åºåˆ—è½¬æ¢ä¸º 2D å›¾åƒï¼ˆå‚è€ƒ TiViT æ–¹æ³•ï¼‰
- ä½¿ç”¨é¢„è®­ç»ƒ ViT æå– patch-level ç‰¹å¾
- æ”¯æŒå¤šç§ ViT æ¨¡å‹ï¼ˆdinov2, clip, siglip, maeï¼‰
"""

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import einops
from typing import Optional, Tuple, Literal
from torchvision.transforms import Resize
import torchvision.transforms as T


def get_vit_model(model_name: str, device: str = "cuda"):
    """
    åŠ è½½é¢„è®­ç»ƒ ViT æ¨¡å‹
    
    æ”¯æŒçš„æ¨¡å‹ï¼š
    - facebook/dinov2-base, facebook/dinov2-small, facebook/dinov2-large
    - openai/clip-vit-base-patch16, openai/clip-vit-large-patch14
    - google/siglip-base-patch16-224
    - facebook/vit-mae-base
    
    Returns:
        processor: å›¾åƒå¤„ç†å™¨ï¼ˆç”¨äºæ ‡å‡†åŒ–ï¼‰
        vit: ViT æ¨¡å‹
        hidden_dim: è¾“å‡ºéšè—ç»´åº¦
        num_patches: patch æ•°é‡ï¼ˆä¸å« CLS tokenï¼‰
    """
    model_name_lower = model_name.lower()
    
    if "dinov2" in model_name_lower:
        from transformers import AutoImageProcessor, AutoModel
        processor = AutoImageProcessor.from_pretrained(model_name)
        vit = AutoModel.from_pretrained(model_name)
        hidden_dim = vit.config.hidden_size
        # DINOv2: 224x224 å›¾åƒï¼Œ14x14 patch â†’ 256 patchesï¼ˆä¸å« CLSï¼‰
        num_patches = (224 // vit.config.patch_size) ** 2
        
    elif "clip" in model_name_lower:
        from transformers import CLIPProcessor, CLIPModel
        processor = CLIPProcessor.from_pretrained(model_name)
        model = CLIPModel.from_pretrained(model_name)
        vit = model.vision_model
        hidden_dim = vit.config.hidden_size
        num_patches = (vit.config.image_size // vit.config.patch_size) ** 2
        
    elif "siglip" in model_name_lower:
        from transformers import AutoProcessor, AutoModel
        processor = AutoProcessor.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name)
        vit = model.vision_model
        hidden_dim = vit.config.hidden_size
        num_patches = (vit.config.image_size // vit.config.patch_size) ** 2
        
    elif "mae" in model_name_lower:
        from transformers import AutoImageProcessor, ViTMAEForPreTraining
        processor = AutoImageProcessor.from_pretrained(model_name)
        model = ViTMAEForPreTraining.from_pretrained(model_name)
        vit = model.vit
        hidden_dim = vit.config.hidden_size
        num_patches = (224 // vit.config.patch_size) ** 2
        
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")
    
    return processor, vit, hidden_dim, num_patches


class VisionEncoder(nn.Module):
    """
    æ—¶åºå›¾åƒåŒ– + ViT ç¼–ç å™¨
    
    Args:
        model_name: é¢„è®­ç»ƒ ViT æ¨¡å‹åç§°
        layer_idx: æå–ç‰¹å¾çš„å±‚ç´¢å¼•ï¼ˆ-1 è¡¨ç¤ºæœ€åä¸€å±‚ï¼‰
        ts_patch_size: æ—¶åºåˆ‡ç‰‡çš„ patch å¤§å°
        ts_stride: æ—¶åºåˆ‡ç‰‡çš„æ­¥é•¿æ¯”ä¾‹ï¼ˆ0-1 ä¹‹é—´ï¼‰
        image_size: è¾“å‡ºå›¾åƒå¤§å°ï¼ˆé»˜è®¤ 224ï¼‰
        return_cls_token: æ˜¯å¦è¿”å› CLS tokenï¼ˆç”¨äºæŸäº›ä¸‹æ¸¸ä»»åŠ¡ï¼‰
    """
    
    def __init__(
        self,
        model_name: str = "facebook/dinov2-base",
        layer_idx: int = -1,
        ts_patch_size: int = 16,
        ts_stride: float = 0.5,
        image_size: int = 224,
        return_cls_token: bool = False,
        device: str = "cuda",
    ):
        super().__init__()
        
        self.model_name = model_name
        self.layer_idx = layer_idx
        self.ts_patch_size = ts_patch_size
        self.ts_stride = ts_stride
        self.image_size = image_size
        self.return_cls_token = return_cls_token
        self.device = device
        
        # åŠ è½½ ViT æ¨¡å‹
        self.processor, self.vit, self.hidden_dim, self.num_vit_patches = get_vit_model(
            model_name, device
        )
        
        # ç”¨äºå°† tensor è½¬ä¸º PIL å›¾åƒï¼ˆæŸäº› processor éœ€è¦ï¼‰
        self.to_pil = T.ToPILImage()
        
        # æˆªæ–­å±‚ï¼ˆå¦‚æœéœ€è¦ï¼‰
        self._truncate_layers()
        
        print(f"\n{'='*50}")
        print(f"VisionEncoder åˆå§‹åŒ–å®Œæˆ")
        print(f"  æ¨¡å‹: {model_name}")
        print(f"  éšè—ç»´åº¦: {self.hidden_dim}")
        print(f"  ViT patches: {self.num_vit_patches}")
        print(f"  æ—¶åº patch size: {ts_patch_size}")
        print(f"  æ—¶åº stride: {ts_stride}")
        print(f"{'='*50}\n")
    
    def _truncate_layers(self):
        """æˆªæ–­ ViT å±‚ï¼ˆç”¨äºæå–ä¸­é—´å±‚ç‰¹å¾ï¼‰"""
        if self.layer_idx is not None and self.layer_idx != -1:
            if hasattr(self.vit, 'encoder'):
                if hasattr(self.vit.encoder, 'layers'):
                    self.vit.encoder.layers = self.vit.encoder.layers[:self.layer_idx]
                elif hasattr(self.vit.encoder, 'layer'):
                    self.vit.encoder.layer = self.vit.encoder.layer[:self.layer_idx]
    
    def ts2image(
        self,
        x: torch.Tensor,
        patch_size: Optional[int] = None,
        stride: Optional[float] = None,
    ) -> torch.Tensor:
        """
        æ—¶é—´åºåˆ—è½¬å›¾åƒï¼ˆå‚è€ƒ TiViT å®ç°ï¼‰
        
        Args:
            x: [B, T, D] æ—¶é—´åºåˆ—ï¼ŒD é€šå¸¸ä¸º 1ï¼ˆå•å˜é‡ï¼‰
            patch_size: å¯é€‰è¦†ç›– patch å¤§å°
            stride: å¯é€‰è¦†ç›– stride
            
        Returns:
            [B, 3, image_size, image_size] RGB å›¾åƒ
        """
        patch_size = patch_size or self.ts_patch_size
        stride = stride or self.ts_stride
        
        # 1) é²æ£’å½’ä¸€åŒ–ï¼ˆä¸­ä½æ•° + IQRï¼‰
        median = x.median(1, keepdim=True)[0]
        q_tensor = torch.tensor([0.75, 0.25], device=x.device, dtype=x.dtype)
        q75, q25 = torch.quantile(x, q_tensor, dim=1, keepdim=True)
        x = x - median
        iqr = q75 - q25
        x = x / (iqr + 1e-5)
        
        # 2) é‡æ’ç»´åº¦: [B, T, D] -> [B, D, T]
        x = einops.rearrange(x, "b t d -> b d t")
        T_len = x.shape[-1]
        
        # 3) æ—¶åºåˆ‡ç‰‡
        if stride == 1:  # æ— é‡å 
            pad_left = 0
            if T_len % patch_size != 0:
                pad_left = patch_size - T_len % patch_size
            x_pad = F.pad(x, (pad_left, 0), mode="replicate")
            x_2d = einops.rearrange(x_pad, "b d (p f) -> (b d) 1 f p", f=patch_size)
        elif 0 < stride < 1:  # é‡å åˆ‡ç‰‡
            stride_len = max(1, int(patch_size * stride))
            remainder = (T_len - patch_size) % stride_len
            pad_left = stride_len - remainder if remainder != 0 else 0
            x_pad = F.pad(x, (pad_left, 0), mode="replicate")
            x_2d = x_pad.unfold(dimension=2, size=patch_size, step=stride_len)
            # [B, D, num_patches, patch_size] -> [B*D, 1, num_patches, patch_size]
            x_2d = einops.rearrange(x_2d, "b d n p -> (b d) 1 n p")
        else:
            raise ValueError(f"stride åº”åœ¨ (0, 1] èŒƒå›´å†…ï¼Œå½“å‰å€¼: {stride}")
        
        # 4) å¯¹æ¯”åº¦è°ƒæ•´
        min_vals = x_2d.min(dim=-1, keepdim=True)[0].min(dim=-2, keepdim=True)[0]
        max_vals = x_2d.max(dim=-1, keepdim=True)[0].max(dim=-2, keepdim=True)[0]
        x_2d = (x_2d - min_vals) / (max_vals - min_vals + 1e-5)
        x_2d = torch.pow(x_2d, 0.8)  # gamma æ ¡æ­£
        
        # 5) ç¼©æ”¾åˆ° ViT è¾“å…¥åˆ†è¾¨ç‡
        x_resized = Resize(
            (self.image_size, self.image_size), 
            interpolation=T.InterpolationMode.NEAREST,
            antialias=False
        )(x_2d)
        
        # 6) ç°åº¦è½¬ RGBï¼ˆå¤åˆ¶é€šé“ï¼‰
        image_input = einops.repeat(x_resized, "b 1 h w -> b c h w", c=3)
        
        return image_input
    
    def forward_vit(self, images: torch.Tensor) -> torch.Tensor:
        """
        ViT å‰å‘ä¼ æ’­ï¼Œè·å– patch-level ç‰¹å¾
        
        Args:
            images: [B, 3, H, W] RGB å›¾åƒ
            
        Returns:
            [B, num_patches, hidden_dim] patch ç‰¹å¾ï¼ˆä¸å« CLS tokenï¼‰
        """
        device = images.device
        
        # ä½¿ç”¨ processor è¿›è¡Œæ ‡å‡†åŒ–
        # æ³¨æ„ï¼šæŸäº› processor éœ€è¦ PIL å›¾åƒ
        if hasattr(self.processor, 'image_processor'):
            # CLIP/SigLIP é£æ ¼
            images_list = [self.to_pil(img.cpu()) for img in images]
            inputs = self.processor(images=images_list, return_tensors="pt")
            pixel_values = inputs["pixel_values"].to(device)
        else:
            # DINOv2/MAE é£æ ¼
            images_list = [self.to_pil(img.cpu()) for img in images]
            inputs = self.processor(images=images_list, return_tensors="pt")
            pixel_values = inputs["pixel_values"].to(device)
        
        # ViT å‰å‘ä¼ æ’­
        outputs = self.vit(
            pixel_values=pixel_values,
            output_hidden_states=(self.layer_idx is None),
        )
        
        # è·å– hidden states
        hidden_states = outputs.last_hidden_state  # [B, 1+num_patches, hidden_dim]
        
        # è¿”å› patch ç‰¹å¾ï¼ˆå¯é€‰æ˜¯å¦åŒ…å« CLS tokenï¼‰
        if self.return_cls_token:
            return hidden_states  # [B, 1+num_patches, hidden_dim]
        else:
            return hidden_states[:, 1:, :]  # [B, num_patches, hidden_dim]ï¼Œå»é™¤ CLS
    
    def forward(
        self,
        past_values: torch.Tensor,
    ) -> torch.Tensor:
        """
        å®Œæ•´å‰å‘ä¼ æ’­ï¼šæ—¶åº â†’ å›¾åƒ â†’ patch ç‰¹å¾
        
        Args:
            past_values: [B, T, 1] æˆ– [B, T, D] æ—¶é—´åºåˆ—
            
        Returns:
            [B, num_patches, hidden_dim] patch-level ç‰¹å¾
        """
        # 1) æ—¶åºå›¾åƒåŒ–
        images = self.ts2image(past_values)  # [B*D, 3, H, W]
        
        # 2) ViT ç¼–ç 
        patch_features = self.forward_vit(images)  # [B*D, num_patches, hidden_dim]
        
        # å¦‚æœæ˜¯å¤šå˜é‡ï¼Œéœ€è¦åˆå¹¶é€šé“ç»´åº¦
        B = past_values.size(0)
        D = past_values.size(-1)
        if D > 1:
            # [B*D, num_patches, H] -> [B, D*num_patches, H]
            patch_features = einops.rearrange(
                patch_features, "(b d) n h -> b (d n) h", b=B, d=D
            )
        
        return patch_features
    
    def get_output_dim(self) -> int:
        """è¿”å›è¾“å‡ºç‰¹å¾ç»´åº¦"""
        return self.hidden_dim
    
    def get_num_patches(self) -> int:
        """è¿”å› patch æ•°é‡"""
        return self.num_vit_patches
    
    def freeze(self):
        """å†»ç»“ ViT å‚æ•°"""
        for param in self.vit.parameters():
            param.requires_grad = False
        print("ğŸ§Š VisionEncoder (ViT) å·²å†»ç»“")
    
    def unfreeze(self):
        """è§£å†» ViT å‚æ•°"""
        for param in self.vit.parameters():
            param.requires_grad = True
        print("ğŸ”¥ VisionEncoder (ViT) å·²è§£å†»")
    
    def count_parameters(self) -> int:
        """è®¡ç®—å‚æ•°é‡"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)
