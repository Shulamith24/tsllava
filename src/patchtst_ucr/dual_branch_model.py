# SPDX-FileCopyrightText: 2025 Stanford University, ETH Zurich, and the project authors
# SPDX-License-Identifier: MIT

"""
PatchTSTWithVisionBranch: åŒåˆ†æ”¯æ—¶åºåˆ†ç±»æ¨¡å‹

æ ¸å¿ƒè®¾è®¡ï¼š
- æ—¶åºåˆ†æ”¯ï¼šPatchTST backbone æå– patch ç‰¹å¾
- å›¾åƒåˆ†æ”¯ï¼šæ—¶åºå›¾åƒåŒ– + ViT ç¼–ç å™¨æå– patch ç‰¹å¾
- èåˆï¼šæ‹¼æ¥ä¸¤ä¸ªåˆ†æ”¯çš„ patch åºåˆ— + [ANS] token
- èšåˆï¼šTransformer Aggregator è¿›è¡Œç‰¹å¾èšåˆå’Œåˆ†ç±»
"""

import torch
import torch.nn as nn
from typing import Optional, Dict, Any, Literal
from transformers import PatchTSTConfig, PatchTSTModel

from .aggregator import SmallTransformerAggregator
from .projector import MLPProjector, LinearProjector
from .vision_encoder import VisionEncoder


class PatchTSTWithVisionBranch(nn.Module):
    """
    åŒåˆ†æ”¯æ—¶åºåˆ†ç±»æ¨¡å‹ï¼šPatchTST + VisionEncoder + Aggregator
    
    Args:
        num_classes: åˆ†ç±»ç±»åˆ«æ•°
        context_length: è¾“å…¥åºåˆ—é•¿åº¦
        
        # PatchTST æ—¶åºåˆ†æ”¯å‚æ•°
        patch_length: Patch é•¿åº¦
        stride: Patch æ­¥é•¿
        d_model: PatchTST æ¨¡å‹ç»´åº¦
        num_attention_heads: PatchTST attention heads
        num_hidden_layers: PatchTST Transformer å±‚æ•°
        ffn_dim: PatchTST FFN ç»´åº¦
        dropout: Dropout æ¦‚ç‡
        
        # Vision åˆ†æ”¯å‚æ•°
        vit_model_name: ViT æ¨¡å‹åç§°
        vit_layer_idx: ViT ç‰¹å¾æå–å±‚ç´¢å¼•
        vit_patch_size: æ—¶åºå›¾åƒåŒ– patch å¤§å°
        vit_stride: æ—¶åºå›¾åƒåŒ–æ­¥é•¿æ¯”ä¾‹
        
        # Aggregator å‚æ•°
        aggregator_layers: èšåˆå¤´ Transformer å±‚æ•°
        aggregator_hidden_size: èšåˆå¤´ hidden size
        aggregator_num_heads: èšåˆå¤´ attention heads
        aggregator_ffn_dim: èšåˆå¤´ FFN ç»´åº¦
        
        # æŠ•å½±å±‚å‚æ•°
        projector_type: æŠ•å½±å±‚ç±»å‹
        projector_dropout: æŠ•å½±å±‚ dropout
        
        # åˆ†æ”¯æ§åˆ¶
        branch_mode: åˆ†æ”¯æ¨¡å¼ ("both", "ts_only", "vision_only")
        freeze_ts_backbone: æ˜¯å¦å†»ç»“ PatchTST backbone
        freeze_vision_backbone: æ˜¯å¦å†»ç»“ Vision backbone
    """

    def __init__(
        self,
        num_classes: int,
        context_length: int,
        # PatchTST æ—¶åºåˆ†æ”¯å‚æ•°
        patch_length: int = 16,
        stride: int = 8,
        d_model: int = 128,
        num_attention_heads: int = 8,
        num_hidden_layers: int = 3,
        ffn_dim: int = 512,
        dropout: float = 0.1,
        # Vision åˆ†æ”¯å‚æ•°
        vit_model_name: str = "facebook/dinov2-base",
        vit_layer_idx: int = -1,
        vit_patch_size: int = 16,
        vit_stride: float = 0.5,
        # Aggregator å‚æ•°
        aggregator_layers: int = 1,
        aggregator_hidden_size: Optional[int] = None,
        aggregator_num_heads: int = 8,
        aggregator_ffn_dim: Optional[int] = None,
        # æŠ•å½±å±‚å‚æ•°
        projector_type: Literal["mlp", "linear", "none"] = "mlp",
        projector_dropout: float = 0.1,
        # åˆ†æ”¯æ§åˆ¶
        branch_mode: Literal["both", "ts_only", "vision_only"] = "both",
        freeze_ts_backbone: bool = False,
        freeze_vision_backbone: bool = True,
        device: str = "cuda",
    ):
        super().__init__()
        
        self.num_classes = num_classes
        self.context_length = context_length
        self.d_model = d_model
        self.device = device
        self.branch_mode = branch_mode
        self.projector_type = projector_type
        
        # ============ 1) æ—¶åºåˆ†æ”¯ï¼šPatchTST Backbone ============
        patchtst_config = PatchTSTConfig(
            num_input_channels=1,
            context_length=context_length,
            patch_length=patch_length,
            stride=stride,
            d_model=d_model,
            num_attention_heads=num_attention_heads,
            num_hidden_layers=num_hidden_layers,
            ffn_dim=ffn_dim,
            dropout=dropout,
            use_cls_token=False,
        )
        
        self.ts_backbone = PatchTSTModel(config=patchtst_config)
        self.ts_num_patches = (context_length - patch_length) // stride + 1
        
        # ============ 2) å›¾åƒåˆ†æ”¯ï¼šVisionEncoder ============
        if branch_mode in ["both", "vision_only"]:
            self.vision_encoder = VisionEncoder(
                model_name=vit_model_name,
                layer_idx=vit_layer_idx,
                ts_patch_size=vit_patch_size,
                ts_stride=vit_stride,
                device=device,
            )
            self.vision_hidden_dim = self.vision_encoder.get_output_dim()
            self.vision_num_patches = self.vision_encoder.get_num_patches()
        else:
            self.vision_encoder = None
            self.vision_hidden_dim = 0
            self.vision_num_patches = 0
        
        # ============ 3) Aggregator é…ç½® ============
        self.aggregator_hidden_size = aggregator_hidden_size or d_model
        self.aggregator_ffn_dim = aggregator_ffn_dim or (self.aggregator_hidden_size * 4)
        
        # ============ 4) æŠ•å½±å±‚ ============
        # æ—¶åºåˆ†æ”¯æŠ•å½±å±‚
        if branch_mode in ["both", "ts_only"]:
            if projector_type == "none":
                if self.aggregator_hidden_size != d_model:
                    print(f"âš ï¸  projector_type='none' æ—¶ï¼Œaggregator_hidden_size è¢«å¼ºåˆ¶è®¾ä¸º {d_model}")
                    self.aggregator_hidden_size = d_model
                self.ts_projector = None
            elif self.aggregator_hidden_size != d_model:
                if projector_type == "mlp":
                    self.ts_projector = MLPProjector(d_model, self.aggregator_hidden_size, dropout=projector_dropout)
                else:
                    self.ts_projector = LinearProjector(d_model, self.aggregator_hidden_size)
            else:
                self.ts_projector = None
        else:
            self.ts_projector = None
        
        # å›¾åƒåˆ†æ”¯æŠ•å½±å±‚
        if branch_mode in ["both", "vision_only"]:
            if self.vision_hidden_dim != self.aggregator_hidden_size:
                if projector_type == "mlp":
                    self.vision_projector = MLPProjector(
                        self.vision_hidden_dim, self.aggregator_hidden_size, dropout=projector_dropout
                    )
                else:
                    self.vision_projector = LinearProjector(
                        self.vision_hidden_dim, self.aggregator_hidden_size
                    )
            else:
                self.vision_projector = None
        else:
            self.vision_projector = None
        
        # ============ 5) è®¡ç®—æ€» patch æ•°é‡ ============
        if branch_mode == "both":
            self.total_patches = self.ts_num_patches + self.vision_num_patches
        elif branch_mode == "ts_only":
            self.total_patches = self.ts_num_patches
        else:  # vision_only
            self.total_patches = self.vision_num_patches
        
        # ============ 6) Aggregator ============
        self.aggregator = SmallTransformerAggregator(
            num_layers=aggregator_layers,
            hidden_size=self.aggregator_hidden_size,
            num_heads=aggregator_num_heads,
            ffn_dim=self.aggregator_ffn_dim,
            dropout=dropout,
        )
        
        # ============ 7) [ANS] Token ============
        self.ans_token = nn.Parameter(
            torch.randn(1, 1, self.aggregator_hidden_size) * 0.02
        )
        
        # ============ 8) åˆ†ç±»å¤´ ============
        self.classifier_head = nn.Linear(self.aggregator_hidden_size, num_classes)
        
        # ============ 9) å†»ç»“æ§åˆ¶ ============
        if freeze_ts_backbone and branch_mode in ["both", "ts_only"]:
            self.freeze_ts_backbone()
        
        if freeze_vision_backbone and branch_mode in ["both", "vision_only"]:
            self.freeze_vision_backbone()
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        self._print_model_info()
    
    def _print_model_info(self):
        """æ‰“å°æ¨¡å‹ä¿¡æ¯"""
        ts_params = sum(p.numel() for p in self.ts_backbone.parameters()) if self.branch_mode != "vision_only" else 0
        vision_params = self.vision_encoder.count_parameters() if self.vision_encoder else 0
        aggregator_params = self.aggregator.count_parameters()
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        print(f"\n{'='*60}")
        print(f"PatchTSTWithVisionBranch æ¨¡å‹ä¿¡æ¯")
        print(f"{'='*60}")
        print(f"åˆ†æ”¯æ¨¡å¼: {self.branch_mode}")
        if self.branch_mode in ["both", "ts_only"]:
            print(f"æ—¶åºåˆ†æ”¯ (PatchTST):")
            print(f"  - context_length: {self.context_length}")
            print(f"  - num_patches: {self.ts_num_patches}")
            print(f"  - d_model: {self.d_model}")
            print(f"  - å‚æ•°é‡: {ts_params:,}")
        if self.branch_mode in ["both", "vision_only"]:
            print(f"å›¾åƒåˆ†æ”¯ (VisionEncoder):")
            print(f"  - num_patches: {self.vision_num_patches}")
            print(f"  - hidden_dim: {self.vision_hidden_dim}")
            print(f"  - å‚æ•°é‡: {vision_params:,}")
        print(f"Aggregator:")
        print(f"  - å±‚æ•°: {self.aggregator.num_layers}")
        print(f"  - hidden_size: {self.aggregator_hidden_size}")
        print(f"  - total_patches (å« ANS): {self.total_patches + 1}")
        print(f"  - å‚æ•°é‡: {aggregator_params:,}")
        print(f"æ€»å‚æ•°é‡: {total_params:,}")
        print(f"å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,}")
        print(f"{'='*60}\n")
    
    def freeze_ts_backbone(self):
        """å†»ç»“ PatchTST backbone"""
        for param in self.ts_backbone.parameters():
            param.requires_grad = False
        print("ğŸ§Š PatchTST backbone å·²å†»ç»“")
    
    def unfreeze_ts_backbone(self):
        """è§£å†» PatchTST backbone"""
        for param in self.ts_backbone.parameters():
            param.requires_grad = True
        print("ğŸ”¥ PatchTST backbone å·²è§£å†»")
    
    def freeze_vision_backbone(self):
        """å†»ç»“ Vision backbone"""
        if self.vision_encoder:
            self.vision_encoder.freeze()
    
    def unfreeze_vision_backbone(self):
        """è§£å†» Vision backbone"""
        if self.vision_encoder:
            self.vision_encoder.unfreeze()
    
    def forward(
        self,
        past_values: torch.Tensor,
        labels: Optional[torch.Tensor] = None,
    ) -> Dict[str, Any]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            past_values: [B, context_length, 1] è¾“å…¥æ—¶é—´åºåˆ—
            labels: [B] å¯é€‰çš„åˆ†ç±»æ ‡ç­¾
            
        Returns:
            åŒ…å« loss (å¦‚æœæä¾› labels) å’Œ logits çš„å­—å…¸
        """
        B = past_values.size(0)
        device = past_values.device
        
        patch_sequences = []
        
        # 1) æ—¶åºåˆ†æ”¯
        if self.branch_mode in ["both", "ts_only"]:
            ts_output = self.ts_backbone(past_values=past_values)
            ts_embeddings = ts_output.last_hidden_state  # [B, 1, num_patches, d_model]
            if ts_embeddings.dim() == 4:
                ts_embeddings = ts_embeddings.squeeze(1)  # [B, num_patches, d_model]
            
            if self.ts_projector is not None:
                ts_embeddings = self.ts_projector(ts_embeddings)
            
            patch_sequences.append(ts_embeddings)
        
        # 2) å›¾åƒåˆ†æ”¯
        if self.branch_mode in ["both", "vision_only"]:
            vision_embeddings = self.vision_encoder(past_values)  # [B, num_patches, vision_dim]
            
            if self.vision_projector is not None:
                vision_embeddings = self.vision_projector(vision_embeddings)
            
            patch_sequences.append(vision_embeddings)
        
        # 3) æ‹¼æ¥ patch åºåˆ—
        combined = torch.cat(patch_sequences, dim=1)  # [B, total_patches, H]
        
        # 4) æ·»åŠ  [ANS] token
        ans_tokens = self.ans_token.expand(B, -1, -1).to(device)
        sequence = torch.cat([combined, ans_tokens], dim=1)  # [B, total_patches+1, H]
        
        # 5) Aggregator å¤„ç†
        hidden_states = self.aggregator(sequence)  # [B, total_patches+1, H]
        
        # 6) æå– [ANS] ä½ç½®çš„ hidden state
        ans_hidden = hidden_states[:, -1, :]  # [B, H]
        
        # 7) åˆ†ç±»
        logits = self.classifier_head(ans_hidden)  # [B, num_classes]
        
        # 8) è®¡ç®—æŸå¤±
        loss = None
        if labels is not None:
            loss = nn.functional.cross_entropy(logits, labels)
        
        return {
            "loss": loss,
            "logits": logits,
            "ans_hidden": ans_hidden,
        }
    
    def predict(self, past_values: torch.Tensor) -> torch.Tensor:
        """é¢„æµ‹ç±»åˆ«"""
        with torch.no_grad():
            outputs = self.forward(past_values)
            predictions = torch.argmax(outputs["logits"], dim=-1)
        return predictions
    
    def count_parameters(self) -> int:
        """è®¡ç®—å¯è®­ç»ƒå‚æ•°é‡"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)
    
    def get_config(self) -> Dict[str, Any]:
        """è¿”å›é…ç½®"""
        return {
            "num_classes": self.num_classes,
            "context_length": self.context_length,
            "branch_mode": self.branch_mode,
            "ts_num_patches": self.ts_num_patches if self.branch_mode != "vision_only" else 0,
            "vision_num_patches": self.vision_num_patches if self.branch_mode != "ts_only" else 0,
            "total_patches": self.total_patches,
            "d_model": self.d_model,
            "aggregator_layers": self.aggregator.num_layers,
            "aggregator_hidden_size": self.aggregator_hidden_size,
            "total_params": sum(p.numel() for p in self.parameters()),
            "trainable_params": self.count_parameters(),
        }
