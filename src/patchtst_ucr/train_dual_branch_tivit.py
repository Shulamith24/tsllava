#!/usr/bin/env python3
# SPDX-FileCopyrightText: 2025 Stanford University, ETH Zurich, and the project authors
# SPDX-License-Identifier: MIT

"""
PatchTST + VisionEncoder åŒåˆ†æ”¯æ—¶åºåˆ†ç±»è®­ç»ƒè„šæœ¬

ç‰¹æ€§ï¼š
- åŒåˆ†æ”¯èåˆï¼šPatchTST æ—¶åºç¼–ç  + TiViT é£æ ¼å›¾åƒç¼–ç 
- DDP åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
- æ˜¾å­˜ä¼˜åŒ–ï¼šFP16 æ··åˆç²¾åº¦ã€æ¢¯åº¦ç´¯ç§¯ã€æ¢¯åº¦æ£€æŸ¥ç‚¹

ä½¿ç”¨æ–¹æ³•ï¼š
    # å•å¡è®­ç»ƒ
    python -m src.patchtst_ucr.train_dual_branch_tivit --dataset Adiac --epochs 50

    # å¯ç”¨æ˜¾å­˜ä¼˜åŒ–
    python -m src.patchtst_ucr.train_dual_branch_tivit --dataset Adiac --fp16 --gradient_accumulation_steps 4

    # å¤šå¡ DDP è®­ç»ƒ
    torchrun --nproc_per_node=2 -m src.patchtst_ucr.train_dual_branch_tivit --dataset Adiac --use_ddp --fp16
"""

import os
import sys
import json
import argparse
import datetime
from pathlib import Path
from typing import List, Dict, Any

import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler
from torch.cuda.amp import GradScaler, autocast
from torch.optim import AdamW
from tqdm.auto import tqdm
from transformers import get_cosine_schedule_with_warmup

# æ·»åŠ  src ç›®å½•åˆ°è·¯å¾„
script_dir = Path(__file__).parent
src_dir = script_dir.parent
if str(src_dir) not in sys.path:
    sys.path.insert(0, str(src_dir))

from patchtst_ucr.dual_branch_model import PatchTSTWithVisionBranch
from patchtst_ucr.ucr_dataset import UCRDatasetForPatchTST, get_dataset_info


def parse_args():
    parser = argparse.ArgumentParser(description="PatchTST + VisionEncoder åŒåˆ†æ”¯åˆ†ç±»")

    # æ•°æ®ç›¸å…³
    parser.add_argument("--dataset", type=str, default="Adiac", help="UCRæ•°æ®é›†åç§°")
    parser.add_argument("--data_path", type=str, default="./data", help="UCRæ•°æ®æ ¹ç›®å½•")
    
    # PatchTST æ—¶åºåˆ†æ”¯é…ç½®
    parser.add_argument("--context_length", type=int, default=None, 
                       help="ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆNoneåˆ™è‡ªåŠ¨è®¾ç½®ä¸ºæ•°æ®é›†æœ€å¤§é•¿åº¦ï¼‰")
    parser.add_argument("--patch_length", type=int, default=16, help="Patch é•¿åº¦")
    parser.add_argument("--stride", type=int, default=8, help="Patch æ­¥é•¿")
    parser.add_argument("--d_model", type=int, default=128, help="PatchTST æ¨¡å‹ç»´åº¦")
    parser.add_argument("--num_attention_heads", type=int, default=8, help="PatchTST Attention heads")
    parser.add_argument("--num_hidden_layers", type=int, default=3, help="PatchTST Transformer å±‚æ•°")
    parser.add_argument("--ffn_dim", type=int, default=512, help="PatchTST FFN ç»´åº¦")
    parser.add_argument("--dropout", type=float, default=0.1, help="Dropout")
    
    # Vision åˆ†æ”¯é…ç½®ï¼ˆæ”¯æŒå¤šç§ ViT æ¨¡å‹ï¼‰
    parser.add_argument("--vit_model_name", type=str, default="facebook/dinov2-base",
                       help="ViTæ¨¡å‹åç§°ï¼Œæ”¯æŒ dinov2/clip/siglip/mae ç­‰")
    parser.add_argument("--vit_layer_idx", type=int, default=-1, help="ViT ç‰¹å¾æå–å±‚ç´¢å¼•")
    parser.add_argument("--vit_patch_size", type=int, default=16, help="æ—¶åºå›¾åƒåŒ– patch å¤§å°")
    parser.add_argument("--vit_stride", type=float, default=0.5, help="æ—¶åºå›¾åƒåŒ–æ­¥é•¿æ¯”ä¾‹")
    
    # åˆ†æ”¯æ§åˆ¶
    parser.add_argument("--branch_mode", type=str, default="both",
                       choices=["both", "ts_only", "vision_only"],
                       help="åˆ†æ”¯æ¨¡å¼: both(åŒåˆ†æ”¯), ts_only(ä»…æ—¶åº), vision_only(ä»…è§†è§‰)")
    
    # èšåˆå¤´é…ç½®
    parser.add_argument("--aggregator_layers", type=int, default=1, help="èšåˆå¤´ Transformer å±‚æ•°")
    parser.add_argument("--aggregator_hidden_size", type=int, default=None, 
                       help="èšåˆå¤´ hidden sizeï¼ˆNoneåˆ™ä¸d_modelç›¸åŒï¼‰")
    parser.add_argument("--aggregator_num_heads", type=int, default=8, help="èšåˆå¤´ attention heads")
    parser.add_argument("--aggregator_ffn_dim", type=int, default=None, 
                       help="èšåˆå¤´ FFN ç»´åº¦ï¼ˆNoneåˆ™è‡ªåŠ¨è®¡ç®—ï¼‰")
    
    # æŠ•å½±å±‚é…ç½®
    parser.add_argument("--projector_type", type=str, default="mlp", 
                       choices=["mlp", "linear", "none"],
                       help="æŠ•å½±å±‚ç±»å‹")
    parser.add_argument("--projector_dropout", type=float, default=0.1, 
                       help="MLPæŠ•å½±å±‚çš„Dropoutæ¦‚ç‡")
    
    # å†»ç»“é€‰é¡¹
    parser.add_argument("--freeze_ts_backbone", action="store_true", help="å†»ç»“ PatchTST backbone")
    parser.add_argument("--freeze_vision_backbone", action="store_true", default=True,
                       help="å†»ç»“ Vision backboneï¼ˆé»˜è®¤å¼€å¯ï¼‰")
    parser.add_argument("--no_freeze_vision_backbone", action="store_true",
                       help="ä¸å†»ç»“ Vision backbone")
    
    # è®­ç»ƒç›¸å…³
    parser.add_argument("--epochs", type=int, default=50, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=16, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--lr", type=float, default=1e-3, help="å­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=1e-4, help="æƒé‡è¡°å‡")
    parser.add_argument("--warmup_ratio", type=float, default=0.1, help="é¢„çƒ­æ¯”ä¾‹")
    parser.add_argument("--grad_clip", type=float, default=1.0, help="æ¢¯åº¦è£å‰ª")
    
    # DDP åˆ†å¸ƒå¼è®­ç»ƒ
    parser.add_argument("--use_ddp", action="store_true", help="å¯ç”¨ DDP åˆ†å¸ƒå¼è®­ç»ƒ")
    parser.add_argument("--local_rank", type=int, default=-1, help="DDP local rank")
    
    # æ˜¾å­˜ä¼˜åŒ–
    parser.add_argument("--fp16", action="store_true", help="å¯ç”¨ FP16 æ··åˆç²¾åº¦")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1,
                       help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    parser.add_argument("--gradient_checkpointing", action="store_true",
                       help="å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆèŠ‚çœæ˜¾å­˜ä½†é™ä½é€Ÿåº¦ï¼‰")
    
    # ä¿å­˜ç›¸å…³
    parser.add_argument("--save_dir", type=str, default="results/patchtst_dual_branch_tivit", 
                       help="ç»“æœä¿å­˜ç›®å½•")
    
    # å…¶ä»–
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--device", type=str, default="cuda", help="è®¾å¤‡")
    parser.add_argument("--eval_every", type=int, default=5, help="æ¯Nè½®è¯„ä¼°ä¸€æ¬¡")
    parser.add_argument("--early_stop", type=int, default=15, help="æ—©åœè€å¿ƒå€¼")
    parser.add_argument("--eval_batch_size", type=int, default=32, help="è¯„ä¼°æ‰¹æ¬¡å¤§å°")
    
    args = parser.parse_args()
    
    # å¤„ç†å†»ç»“é€‰é¡¹å†²çª
    if args.no_freeze_vision_backbone:
        args.freeze_vision_backbone = False
    
    return args


def set_seed(seed: int, rank: int = 0):
    """è®¾ç½®éšæœºç§å­"""
    seed = seed + rank
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    import random
    import numpy as np
    random.seed(seed)
    np.random.seed(seed)


def setup_ddp(args):
    """åˆå§‹åŒ– DDP"""
    if args.use_ddp:
        if args.local_rank == -1:
            args.local_rank = int(os.environ.get("LOCAL_RANK", 0))
        
        dist.init_process_group(backend="nccl")
        torch.cuda.set_device(args.local_rank)
        args.device = f"cuda:{args.local_rank}"
        args.world_size = dist.get_world_size()
        args.rank = dist.get_rank()
        
        if args.rank == 0:
            print(f"ğŸŒ DDP åˆå§‹åŒ–å®Œæˆï¼Œworld_size={args.world_size}")
    else:
        args.world_size = 1
        args.rank = 0


def cleanup_ddp(args):
    """æ¸…ç† DDP"""
    if args.use_ddp:
        dist.destroy_process_group()


def is_main_process(args):
    """åˆ¤æ–­æ˜¯å¦ä¸ºä¸»è¿›ç¨‹"""
    return args.rank == 0


def prepare_batch(
    batch: List[Dict],
    context_length: int,
    device: str,
):
    """å°† UCR æ‰¹æ¬¡è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥"""
    past_values_list = []
    labels = []
    
    for sample in batch:
        ts = sample["time_series"][0]
        
        if not isinstance(ts, torch.Tensor):
            ts = torch.tensor(ts, dtype=torch.float32)
        
        if len(ts) < context_length:
            padded = torch.zeros(context_length, device=device)
            padded[:len(ts)] = ts.to(device)
        else:
            padded = ts[:context_length].to(device)
        
        past_values_list.append(padded.unsqueeze(-1))
        labels.append(sample["int_label"])
    
    past_values = torch.stack(past_values_list, dim=0)
    labels = torch.tensor(labels, device=device, dtype=torch.long)
    
    return past_values, labels


def create_data_loaders(args, num_classes: int, context_length: int):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    train_dataset = UCRDatasetForPatchTST(
        dataset_name=args.dataset,
        split="train",
        raw_data_path=args.data_path,
    )
    
    val_dataset = UCRDatasetForPatchTST(
        dataset_name=args.dataset,
        split="validation",
        raw_data_path=args.data_path,
    )
    
    test_dataset = UCRDatasetForPatchTST(
        dataset_name=args.dataset,
        split="test",
        raw_data_path=args.data_path,
    )
    
    def collate_fn(batch):
        return batch
    
    # DDP é‡‡æ ·å™¨
    train_sampler = DistributedSampler(train_dataset, shuffle=True) if args.use_ddp else None
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=(train_sampler is None),
        sampler=train_sampler,
        collate_fn=collate_fn,
        num_workers=0,  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.eval_batch_size,
        shuffle=False,
        collate_fn=collate_fn,
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=args.eval_batch_size,
        shuffle=False,
        collate_fn=collate_fn,
    )
    
    return train_loader, val_loader, test_loader, train_sampler


def train_one_epoch(
    model,
    train_loader: DataLoader,
    optimizer,
    scheduler,
    context_length: int,
    grad_clip: float,
    device: str,
    epoch: int,
    num_epochs: int,
    args,
    scaler: GradScaler = None,
) -> float:
    """è®­ç»ƒä¸€ä¸ª epoch"""
    model.train()
    total_loss = 0.0
    num_batches = 0
    
    # DDP è®¾ç½® epochï¼ˆç”¨äº shuffleï¼‰
    if args.use_ddp and hasattr(train_loader, 'sampler'):
        train_loader.sampler.set_epoch(epoch)
    
    pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{num_epochs}", disable=not is_main_process(args))
    
    optimizer.zero_grad()
    
    for step, batch in enumerate(pbar):
        past_values, labels = prepare_batch(batch, context_length, device)
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        if args.fp16:
            with autocast():
                outputs = model(past_values=past_values, labels=labels)
                loss = outputs["loss"]
                loss = loss / args.gradient_accumulation_steps
            
            scaler.scale(loss).backward()
        else:
            outputs = model(past_values=past_values, labels=labels)
            loss = outputs["loss"]
            loss = loss / args.gradient_accumulation_steps
            loss.backward()
        
        # æ¢¯åº¦ç´¯ç§¯
        if (step + 1) % args.gradient_accumulation_steps == 0:
            if args.fp16:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)
                scaler.step(optimizer)
                scaler.update()
            else:
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)
                optimizer.step()
            
            scheduler.step()
            optimizer.zero_grad()
        
        total_loss += loss.item() * args.gradient_accumulation_steps
        num_batches += 1
        
        if is_main_process(args):
            pbar.set_postfix({
                "loss": f"{loss.item() * args.gradient_accumulation_steps:.4f}",
                "lr": f"{scheduler.get_last_lr()[0]:.2e}"
            })
    
    return total_loss / max(num_batches, 1)


@torch.no_grad()
def evaluate(
    model,
    data_loader: DataLoader,
    context_length: int,
    device: str,
    args,
    desc: str = "Evaluating",
) -> Dict[str, Any]:
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    
    all_predictions = []
    all_labels = []
    total_loss = 0.0
    num_batches = 0
    
    for batch in tqdm(data_loader, desc=desc, disable=not is_main_process(args)):
        past_values, labels = prepare_batch(batch, context_length, device)
        
        if args.fp16:
            with autocast():
                outputs = model(past_values=past_values, labels=labels)
        else:
            outputs = model(past_values=past_values, labels=labels)
        
        total_loss += outputs["loss"].item()
        num_batches += 1
        
        predictions = torch.argmax(outputs["logits"], dim=-1)
        
        all_predictions.extend(predictions.cpu().tolist())
        all_labels.extend(labels.cpu().tolist())
    
    avg_loss = total_loss / max(num_batches, 1)
    correct = sum(p == l for p, l in zip(all_predictions, all_labels))
    accuracy = correct / len(all_labels) if all_labels else 0.0
    
    return {
        "loss": avg_loss,
        "accuracy": accuracy,
        "predictions": all_predictions,
        "labels": all_labels,
    }


def main():
    args = parse_args()
    
    # åˆå§‹åŒ– DDP
    setup_ddp(args)
    
    if is_main_process(args):
        print("=" * 60)
        print("PatchTST + VisionEncoder åŒåˆ†æ”¯æ—¶åºåˆ†ç±»")
        print("=" * 60)
        print(f"æ—¶é—´: {datetime.datetime.now()}")
        print(f"æ•°æ®é›†: {args.dataset}")
        print(f"åˆ†æ”¯æ¨¡å¼: {args.branch_mode}")
        print(f"ViT æ¨¡å‹: {args.vit_model_name}")
        print(f"DDP: {args.use_ddp}, FP16: {args.fp16}")
        print(f"æ¢¯åº¦ç´¯ç§¯: {args.gradient_accumulation_steps}")
        print("=" * 60)
    
    set_seed(args.seed, args.rank)
    
    device = args.device if torch.cuda.is_available() else "cpu"
    if is_main_process(args):
        print(f"\nä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ†ææ•°æ®é›†
    if is_main_process(args):
        print("\nğŸ“‚ åˆ†ææ•°æ®é›†...")
    num_classes, max_length = get_dataset_info(args.dataset, args.data_path)
    
    if args.context_length is None:
        context_length = ((max_length - 1) // args.patch_length + 1) * args.patch_length
    else:
        context_length = args.context_length
    
    if is_main_process(args):
        print(f"   ç±»åˆ«æ•°: {num_classes}")
        print(f"   æœ€å¤§é•¿åº¦: {max_length}")
        print(f"   Context length: {context_length}")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    vit_short_name = args.vit_model_name.split("/")[-1].replace("-", "_")
    save_subdir = f"{args.branch_mode}_{vit_short_name}_L{args.aggregator_layers}"
    if args.freeze_ts_backbone:
        save_subdir += "_tsFrozen"
    if args.freeze_vision_backbone:
        save_subdir += "_vFrozen"
    
    save_dir = os.path.join(args.save_dir, args.dataset, save_subdir)
    
    if is_main_process(args):
        os.makedirs(save_dir, exist_ok=True)
        with open(os.path.join(save_dir, "config.json"), "w") as f:
            json.dump(vars(args), f, indent=2)
    
    # åˆ›å»ºæ¨¡å‹
    if is_main_process(args):
        print("\nğŸ”§ åˆ›å»ºæ¨¡å‹...")
    
    model = PatchTSTWithVisionBranch(
        num_classes=num_classes,
        context_length=context_length,
        patch_length=args.patch_length,
        stride=args.stride,
        d_model=args.d_model,
        num_attention_heads=args.num_attention_heads,
        num_hidden_layers=args.num_hidden_layers,
        ffn_dim=args.ffn_dim,
        dropout=args.dropout,
        vit_model_name=args.vit_model_name,
        vit_layer_idx=args.vit_layer_idx,
        vit_patch_size=args.vit_patch_size,
        vit_stride=args.vit_stride,
        aggregator_layers=args.aggregator_layers,
        aggregator_hidden_size=args.aggregator_hidden_size,
        aggregator_num_heads=args.aggregator_num_heads,
        aggregator_ffn_dim=args.aggregator_ffn_dim,
        projector_type=args.projector_type,
        projector_dropout=args.projector_dropout,
        branch_mode=args.branch_mode,
        freeze_ts_backbone=args.freeze_ts_backbone,
        freeze_vision_backbone=args.freeze_vision_backbone,
        device=device,
    ).to(device)
    
    # æ¢¯åº¦æ£€æŸ¥ç‚¹
    if args.gradient_checkpointing:
        if hasattr(model.ts_backbone, 'gradient_checkpointing_enable'):
            model.ts_backbone.gradient_checkpointing_enable()
        if is_main_process(args):
            print("ğŸ”„ æ¢¯åº¦æ£€æŸ¥ç‚¹å·²å¯ç”¨")
    
    # DDP åŒ…è£…
    if args.use_ddp:
        model = DDP(model, device_ids=[args.local_rank], find_unused_parameters=True)
    
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    if is_main_process(args):
        print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    if is_main_process(args):
        print("\nğŸ“‚ åŠ è½½æ•°æ®...")
    train_loader, val_loader, test_loader, train_sampler = create_data_loaders(
        args, num_classes, context_length
    )
    
    if is_main_process(args):
        print(f"   Train batches: {len(train_loader)}")
        print(f"   Val batches: {len(val_loader)}")
        print(f"   Test batches: {len(test_loader)}")
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    if is_main_process(args):
        print("\nâš™ï¸  åˆ›å»ºä¼˜åŒ–å™¨...")
    
    optimizer = AdamW(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=args.lr,
        weight_decay=args.weight_decay,
    )
    
    # è®¡ç®—æ€»æ­¥æ•°ï¼ˆè€ƒè™‘æ¢¯åº¦ç´¯ç§¯ï¼‰
    steps_per_epoch = len(train_loader) // args.gradient_accumulation_steps
    total_steps = args.epochs * steps_per_epoch
    warmup_steps = int(args.warmup_ratio * total_steps)
    
    scheduler = get_cosine_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_steps,
    )
    
    if is_main_process(args):
        print(f"   Total steps: {total_steps}")
        print(f"   Warmup steps: {warmup_steps}")
    
    # FP16 æ··åˆç²¾åº¦
    scaler = GradScaler() if args.fp16 else None
    if args.fp16 and is_main_process(args):
        print("âš¡ FP16 æ··åˆç²¾åº¦å·²å¯ç”¨")
    
    # è®­ç»ƒå¾ªç¯
    if is_main_process(args):
        print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    
    best_val_acc = 0.0
    patience_counter = 0
    loss_history = []
    
    try:
        for epoch in range(1, args.epochs + 1):
            train_loss = train_one_epoch(
                model, train_loader, optimizer, scheduler,
                context_length, args.grad_clip, device,
                epoch, args.epochs, args, scaler
            )
            
            if epoch % args.eval_every == 0 or epoch == args.epochs:
                if is_main_process(args):
                    print(f"\nğŸ“Š Epoch {epoch} è¯„ä¼°...")
                
                # è·å–åŸå§‹æ¨¡å‹ï¼ˆDDP åŒ…è£…ä¸‹ï¼‰
                eval_model = model.module if args.use_ddp else model
                
                val_results = evaluate(
                    eval_model, val_loader, context_length, device, args, "Validating"
                )
                val_loss = val_results["loss"]
                val_acc = val_results["accuracy"]
                
                if is_main_process(args):
                    print(f"   Train Loss: {train_loss:.4f}")
                    print(f"   Val Loss: {val_loss:.4f}")
                    print(f"   Val Accuracy: {val_acc:.4f}")
                    
                    if val_acc > best_val_acc:
                        best_val_acc = val_acc
                        patience_counter = 0
                        
                        checkpoint = {
                            "model_state": eval_model.state_dict(),
                            "optimizer_state": optimizer.state_dict(),
                            "scheduler_state": scheduler.state_dict(),
                            "epoch": epoch,
                            "val_loss": val_loss,
                            "val_acc": val_acc,
                            "config": eval_model.get_config(),
                            "args": vars(args),
                        }
                        torch.save(checkpoint, os.path.join(save_dir, "best_model.pt"))
                        print(f"   ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹")
                    else:
                        patience_counter += 1
                        print(f"   (æ— æ”¹è¿›, patience: {patience_counter}/{args.early_stop})")
                    
                    loss_history.append({
                        "epoch": epoch,
                        "train_loss": train_loss,
                        "val_loss": val_loss,
                        "val_acc": val_acc,
                    })
                    with open(os.path.join(save_dir, "loss_history.json"), "w") as f:
                        json.dump(loss_history, f, indent=2)
            else:
                if is_main_process(args):
                    print(f"Epoch {epoch}: Train Loss = {train_loss:.4f}")
            
            if patience_counter >= args.early_stop:
                if is_main_process(args):
                    print(f"\nâ¹ï¸  æ—©åœ! éªŒè¯å‡†ç¡®ç‡ {args.early_stop} è½®æœªæ”¹è¿›")
                break
        
        # æœ€ç»ˆæµ‹è¯•
        if is_main_process(args):
            print("\n" + "=" * 60)
            print("ğŸ“‹ æœ€ç»ˆæµ‹è¯•è¯„ä¼°...")
            
            eval_model = model.module if args.use_ddp else model
            
            # åŠ è½½æœ€ä½³æ¨¡å‹
            best_ckpt = torch.load(
                os.path.join(save_dir, "best_model.pt"),
                map_location=device,
                weights_only=False
            )
            eval_model.load_state_dict(best_ckpt["model_state"])
            
            test_results = evaluate(
                eval_model, test_loader, context_length, device, args, "Testing"
            )
            
            print(f"\nâœ… æµ‹è¯•ç»“æœ:")
            print(f"   Test Loss: {test_results['loss']:.4f}")
            print(f"   Test Accuracy: {test_results['accuracy']:.4f}")
            
            final_results = {
                "dataset": args.dataset,
                "num_classes": num_classes,
                "context_length": context_length,
                "branch_mode": args.branch_mode,
                "vit_model_name": args.vit_model_name,
                "aggregator_layers": args.aggregator_layers,
                "aggregator_hidden_size": args.aggregator_hidden_size or args.d_model,
                "freeze_ts_backbone": args.freeze_ts_backbone,
                "freeze_vision_backbone": args.freeze_vision_backbone,
                "total_params": sum(p.numel() for p in eval_model.parameters()),
                "trainable_params": eval_model.count_parameters(),
                "best_val_acc": best_val_acc,
                "test_loss": test_results["loss"],
                "test_accuracy": test_results["accuracy"],
                "epochs_trained": epoch,
            }
            
            with open(os.path.join(save_dir, "final_results.json"), "w") as f:
                json.dump(final_results, f, indent=2)
            
            with open(os.path.join(save_dir, "test_predictions.json"), "w") as f:
                json.dump({
                    "predictions": test_results["predictions"],
                    "labels": test_results["labels"],
                }, f, indent=2)
            
            print("=" * 60)
            print(f"ç»“æœä¿å­˜åˆ°: {save_dir}")
            print("=" * 60)
    
    except KeyboardInterrupt:
        if is_main_process(args):
            print("\nâš ï¸  è®­ç»ƒè¢«ä¸­æ–­")
    except Exception as e:
        if is_main_process(args):
            print(f"\nâŒ é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
        cleanup_ddp(args)
        return 1
    
    cleanup_ddp(args)
    return 0


if __name__ == "__main__":
    exit(main())
